{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanzhen4/feedback-prize-english-language-learning/blob/master/DeBERTa-pseudoLabel_1000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4487dd87",
      "metadata": {
        "id": "4487dd87",
        "papermill": {
          "duration": 0.006995,
          "end_time": "2022-08-31T22:31:20.199821",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.192826",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# About this notebook\n",
        "- Deberta-v3-base starter code\n",
        "- pip wheels is [here](https://www.kaggle.com/code/yasufuminakama/fb3-pip-wheels)\n",
        "- Inference notebook is [here](https://www.kaggle.com/yasufuminakama/fb3-deberta-v3-base-baseline-inference)\n",
        "\n",
        "If this notebook is helpful, feel free to upvote :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J5uY3toQO9es",
      "metadata": {
        "id": "J5uY3toQO9es"
      },
      "source": [
        "Package: pip install wandb transformers tokenizers iterative-stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9f89e5f8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:31:20.233612Z",
          "iopub.status.busy": "2022-08-31T22:31:20.233146Z",
          "iopub.status.idle": "2022-08-31T22:31:20.248183Z",
          "shell.execute_reply": "2022-08-31T22:31:20.247226Z"
        },
        "id": "9f89e5f8",
        "papermill": {
          "duration": 0.027843,
          "end_time": "2022-08-31T22:31:20.251266",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.223423",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = '/content/gdrive/MyDrive/Colab_Notebooks/Deberta/'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00af3d33",
      "metadata": {
        "id": "00af3d33",
        "papermill": {
          "duration": 0.006899,
          "end_time": "2022-08-31T22:31:20.215491",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.208592",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Directory settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "FvhPIWIZOA9T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvhPIWIZOA9T",
        "outputId": "32bd44d1-35a4-41dc-d22a-d13cf6760b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b42ab58",
      "metadata": {
        "id": "8b42ab58",
        "papermill": {
          "duration": 0.00523,
          "end_time": "2022-08-31T22:31:20.262025",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.256795",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5d62d96",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:31:20.275246Z",
          "iopub.status.busy": "2022-08-31T22:31:20.274550Z",
          "iopub.status.idle": "2022-08-31T22:31:20.282480Z",
          "shell.execute_reply": "2022-08-31T22:31:20.281625Z"
        },
        "id": "e5d62d96",
        "papermill": {
          "duration": 0.01687,
          "end_time": "2022-08-31T22:31:20.284537",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.267667",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    wandb=True\n",
        "    competition='FB3'\n",
        "    _wandb_kernel='nakama'\n",
        "    debug=False\n",
        "    apex=True\n",
        "    print_freq=20\n",
        "    num_workers=4\n",
        "    model=\"/content/gdrive/MyDrive/Colab_Notebooks/Deberta/microsoft_deberta-large\"\n",
        "    gradient_checkpointing=True\n",
        "    scheduler='cosine' # ['linear', 'cosine']\n",
        "    batch_scheduler=True\n",
        "    num_cycles=0.5\n",
        "    num_warmup_steps=0\n",
        "    epochs=5\n",
        "    encoder_lr=2e-6\n",
        "    decoder_lr=2e-6\n",
        "    min_lr=1e-6\n",
        "    eps=1e-6\n",
        "    betas=(0.9, 0.999)\n",
        "    batch_size=1\n",
        "    max_len=512\n",
        "    weight_decay=0.01\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    seed=42\n",
        "    n_fold=4\n",
        "    trn_fold=[0, 1, 2, 3]\n",
        "    train=True\n",
        "    \n",
        "if CFG.debug:\n",
        "    CFG.epochs = 2\n",
        "    CFG.trn_fold = [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4692bb04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T22:31:20.297610Z",
          "iopub.status.busy": "2022-08-31T22:31:20.296833Z",
          "iopub.status.idle": "2022-08-31T22:31:29.392500Z",
          "shell.execute_reply": "2022-08-31T22:31:29.391447Z"
        },
        "id": "4692bb04",
        "outputId": "0f1506b6-6be3-4cda-8ef0-bb5fa37d3e07",
        "papermill": {
          "duration": 9.105575,
          "end_time": "2022-08-31T22:31:29.395630",
          "exception": false,
          "start_time": "2022-08-31T22:31:20.290055",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
            "Get your W&B access token from here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221125_102556-2naro4d0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/anony-moose-448938/FB3-Public/runs/2naro4d0?apiKey=2691494e2d81554403eb49d43063353209b5c90c\" target=\"_blank\">/content/gdrive/MyDrive/Colab_Notebooks/Deberta/microsoft_deberta-large</a></strong> to <a href=\"https://wandb.ai/anony-moose-448938/FB3-Public?apiKey=2691494e2d81554403eb49d43063353209b5c90c\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====================================================\n",
        "# wandb\n",
        "# ====================================================\n",
        "if CFG.wandb:\n",
        "    \n",
        "    import wandb\n",
        "\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
        "        wandb.login(key=secret_value_0)\n",
        "        anony = None\n",
        "    except:\n",
        "        anony = \"must\"\n",
        "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
        "\n",
        "\n",
        "    def class2dict(f):\n",
        "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
        "\n",
        "    run = wandb.init(project='FB3-Public', \n",
        "                     name=CFG.model,\n",
        "                     config=class2dict(CFG),\n",
        "                     group=CFG.model,\n",
        "                     job_type=\"train\",\n",
        "                     anonymous=anony)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59ce0b2",
      "metadata": {
        "id": "b59ce0b2",
        "papermill": {
          "duration": 0.009322,
          "end_time": "2022-08-31T22:31:29.415506",
          "exception": false,
          "start_time": "2022-08-31T22:31:29.406184",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f513d712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T22:31:29.436467Z",
          "iopub.status.busy": "2022-08-31T22:31:29.436018Z",
          "iopub.status.idle": "2022-08-31T22:32:28.081444Z",
          "shell.execute_reply": "2022-08-31T22:32:28.080370Z"
        },
        "id": "f513d712",
        "outputId": "6f79af86-1401-4ef6-fe1f-b3a3ab39be20",
        "papermill": {
          "duration": 58.659308,
          "end_time": "2022-08-31T22:32:28.084435",
          "exception": false,
          "start_time": "2022-08-31T22:31:29.425127",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers.__version__: 4.24.0\n",
            "env: TOKENIZERS_PARALLELISM=true\n"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "import joblib\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "\n",
        "#os.system('pip install iterative-stratification==0.1.7')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "#os.system('pip uninstall -y transformers')\n",
        "#os.system('pip uninstall -y tokenizers')\n",
        "#os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
        "#os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
        "#import tokenizers\n",
        "import transformers\n",
        "#print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
        "print(f\"transformers.__version__: {transformers.__version__}\")\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "%env TOKENIZERS_PARALLELISM=true\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c521bb35",
      "metadata": {
        "id": "c521bb35",
        "papermill": {
          "duration": 0.015189,
          "end_time": "2022-08-31T22:32:28.113643",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.098454",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "37e2a622",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:28.142081Z",
          "iopub.status.busy": "2022-08-31T22:32:28.140999Z",
          "iopub.status.idle": "2022-08-31T22:32:28.163185Z",
          "shell.execute_reply": "2022-08-31T22:32:28.162199Z"
        },
        "id": "37e2a622",
        "papermill": {
          "duration": 0.039148,
          "end_time": "2022-08-31T22:32:28.165976",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.126828",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Utils\n",
        "# ====================================================\n",
        "def MCRMSE(y_trues, y_preds): #loss\n",
        "    scores = []\n",
        "    idxes = y_trues.shape[1]\n",
        "    for i in range(idxes):\n",
        "        y_true = y_trues[:,i]\n",
        "        y_pred = y_preds[:,i]\n",
        "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
        "        scores.append(score)\n",
        "    mcrmse_score = np.mean(scores)\n",
        "    return mcrmse_score, scores #score: single prediction result; scores: list of all results\n",
        "\n",
        "\n",
        "def get_score(y_trues, y_preds):\n",
        "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
        "    return mcrmse_score, scores\n",
        "\n",
        "\n",
        "def get_logger(filename=OUTPUT_DIR+'train'): #print log \n",
        "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = get_logger()\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e95c84",
      "metadata": {
        "id": "94e95c84",
        "papermill": {
          "duration": 0.010764,
          "end_time": "2022-08-31T22:32:28.191852",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.181088",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "iYFCrZKIIXbV",
      "metadata": {
        "id": "iYFCrZKIIXbV"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Data Loading\n",
        "# ====================================================\n",
        "train = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/Deberta/train.csv')\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/Deberta/test.csv')\n",
        "submission = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/Deberta/sample_submission.csv')\n",
        "old_train = pd.read_csv(\"/content/gdrive/MyDrive/Colab_Notebooks/Deberta/old_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "db454733",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:28.216397Z",
          "iopub.status.busy": "2022-08-31T22:32:28.215550Z",
          "iopub.status.idle": "2022-08-31T22:32:28.504765Z",
          "shell.execute_reply": "2022-08-31T22:32:28.503570Z"
        },
        "id": "db454733",
        "outputId": "01d62497-be08-4b72-d20a-e14915cd286c",
        "papermill": {
          "duration": 0.304959,
          "end_time": "2022-08-31T22:32:28.508101",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.203142",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train.shape: (1000, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9b0b6728-12b4-4e20-b438-d6f6a2facfd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b0b6728-12b4-4e20-b438-d6f6a2facfd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b0b6728-12b4-4e20-b438-d6f6a2facfd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b0b6728-12b4-4e20-b438-d6f6a2facfd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.shape: (3, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-64d48955-5125-4d2c-aed8-2131d10f4938\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64d48955-5125-4d2c-aed8-2131d10f4938')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64d48955-5125-4d2c-aed8-2131d10f4938 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64d48955-5125-4d2c-aed8-2131d10f4938');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        text_id                                          full_text\n",
              "0  0000C359D63E  when a person has no experience on a job their...\n",
              "1  000BAD50D026  Do you think students would benefit from being...\n",
              "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.shape: (3, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2a2ea3eb-01a1-4425-a3e4-a520c9439bea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a2ea3eb-01a1-4425-a3e4-a520c9439bea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a2ea3eb-01a1-4425-a3e4-a520c9439bea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a2ea3eb-01a1-4425-a3e4-a520c9439bea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Test\n",
        "train = train.head(1000)\n",
        "print(f\"train.shape: {train.shape}\")\n",
        "display(train.head())\n",
        "print(f\"test.shape: {test.shape}\")\n",
        "display(test.head())\n",
        "print(f\"submission.shape: {submission.shape}\")\n",
        "display(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bAmBrAta0HA-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bAmBrAta0HA-",
        "outputId": "2dc57ffe-0298-4fb2-c699-9984f129c30d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-090d89bb-2251-4b8d-b465-fc493aa16e0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to an article by the Edgar Snyder Fi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The most common of these distractions is a cel...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reaction time is the measure of how quickly an...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The affects can be physical, emotional, or psy...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In conclusion, people shouldn't use cellphones...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It could lead to accidents and altercations. Y...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Many people happen to pass away due to a motor...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In conclusion, Drivers shouldn't drive while u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Other drivers take notice when drivers are on ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Driving with cell phones should be banned beca...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Statistically, 50 minutes of phone chatter whi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Cell phones were presented within the United S...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Drivers who are distracted by their phone whil...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Distracted driving is now considered to be the...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Reasons for this can range from work related t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The question is people more concerned more wit...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>A major cause of deaths in the US is motor veh...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>According to an article of \" Teen driver sourc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Each state in the US has their own laws to pro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>While physical damage to a vehicle or physical...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>According to the website big AZ BIG Media, the...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Although cell phones have not been around for ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Throughout the years, teenagers have been eage...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Utilizing a phone while driving takes many liv...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Once again, using a phone while driving create...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>The new law was put in to ensure more safety d...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>A phone should not ever be used while a person...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>There are just so many things that we are able...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>If you are at a red light and you don't have ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Now with these cars we don't need to worry abo...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>It's said that only 39 states put a ban for no...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>I think I could say the same thing for a passe...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>When using your phone while driving, you shoul...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>In today's day and age, technology has drastic...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>When you make the decision to text and drive, ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Regardless of the all the harm caused by texti...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>In 2018 half a million people were injured be...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>In an article \"Just how much does car insuranc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>There are 41 state that ban texting while driv...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>The use of cars and mobile technology has done...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Drivers often use their phones for GPS in this...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Although texting and driving is the cause of ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>According to Legalmatch \"Texting while driving...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>In Conclusion, Drivers should not be able to u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>You're driving in your car so someone who you ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>On busy roads, the amount of time that one tal...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>94% percent of drivers support a ban on textin...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>On one occasion, my mom was hit from behind be...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>My mom's car insurance increased an additional...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Everybody has their own opinion regarding cell...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>It doesn't matter where you are you turn your ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>There have been many different accidents beca...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>There is always pros and cons about things peo...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>If an emergency maneuver or emergency braking ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Ever since phones have entered into the world,...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>\"Sending or reading a text causes drivers, on ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>When people use their cell phones on the highw...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Why is this? It is because using a cell phone ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>even if the driver only looked at the phone ju...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>\"Drivers on the phone have worse reaction time...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>As the driver you not only risk your own life,...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>Cell phone use should be prohibited why you op...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Talking while driving isn't as bad because you...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>The National Safety Council reports that cellp...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>In society, money is tight when it comes to d...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Distractive driving effects all drivers equall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>It is important to know because the bad behavi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>The driver will also experience a physical dis...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>You make decisions to text and drive, you make...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>Reasoning on no devices while driving is part ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>When you choose to text and drive, you're thre...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>You get pulled over and get a ticket. But also...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Being able to use a device while operating a v...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>At least one out of every twenty drivers will ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Amongst accidents, an average of nine teens a...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>On Instamotor it discusses 5 best gadgets to c...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>Another useful gadget is called the \"Motorola ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>Texting while driving is one of the worrying t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Despite these consequences, people are still f...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>On the other hand, these news laws need to ban...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>I strongly agree that texting and driving is t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Phones play a key role in people keeping in to...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>For instance, accidents caused by cellphone us...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>In my final judgment, I suppose we can drive s...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>The reason why people are using their phones w...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>Is it really that important? A text is not mor...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>The number of fatal crashes caused by cell pho...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Teenagers are not as experienced drivers and t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>So even if you are using your phone and paying...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Studies have shown that while talking to someo...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Andrei Zakhareuski, from Driving Tests, procla...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Cells phones are very common place for everyon...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>In fact adults are put at a higher risk becaus...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Many people use their cell phones while drivin...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>Driving while operating your phone can end in ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>If not, put down the phone and wait until you ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Among those killed: 1,730 drivers, 605 passeng...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>This shows that the couple of seconds you take...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>\\nCell phones were introduced in the United St...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Researches show that using a headset can be as...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>People shouldn't fully depend on cell phones b...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>The new \"hands free\" law is a law where no one...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>\"Incredibly, they also spent less time watchi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>You will be distracted by telephone conversati...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>Although cell phones have not been around for ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>When driving, sometimes it is difficult to foc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>They include talking to passengers, lighting a...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>Many people like to use their phone while driv...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Texting and driving can cause about 5 times as...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>Just because a driver can get away with texti...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>\\nTherefore texting in driving in any state sh...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>The reason is that when that person is driving...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>The reason is that when drivers are driving, t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Many habitants of planet earth have and use th...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>The perception of a freedom is a more effectiv...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Therefore, the use of cell phones should still...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Sometimes it is difficult to focus on the road...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>They (cops) think they can just lie as well bu...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Electronic devices nowadays are in everyone's ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Now we have tools that can hold your phone in ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>As a driver myself I try not to use my phone w...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>The purpose of this essay is to give you some ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Passing laws that require hands free devices s...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Contrary to common belief there is no such thi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>It's crazy the number of people that have died...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>Of all cell phone related tasks, texting is by...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>In conclusion t's now illegal to hold your pho...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Nobody ever needs to use their phone while dri...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Each year there are approximately 1.6 million ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>As to all the things there are pros and cons, ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>You would imagine that being on a phone call w...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>The Internet has allowed for cell phones to ha...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>In conclusion, it is my opinion that drivers s...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>There thousands of reasons that you should not...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>The average cellphone while driving violation ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>Imagine for just one moment a teen has friends...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>Starting in the 1996, cell phones became very ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>If driving from Georgia to New York with a map...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>With bluetooth in a car, you wouldn't be break...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>As you can see driving with your phone is not ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>\"Economic losses from distracted driving coul...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>Finally, the laws for driving and being on you...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>I know phones shouldn't be allowed to be messe...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>In an article by teensafe. com it states that,...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Just imagine losing a loved one due to someone...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There is no doubt that with how advanced techn...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>This is so that if someone does happen to have...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>Using the bluetooth phone hookup feature that ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>In conclusion, I feel that in the future cell ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>Thousands of accidents happen in one year. Mos...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>Monetary fines these can range from as low as ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>For, Conclusion being on your phone while driv...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>Should drivers be allowed to use their phones ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>Nowadays, teens feel the need to always have t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>In 2015, 391,000 people were injured in motor ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>To conclude, drivers, no matter the age or exp...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Drivers that text while driving are likely to...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>For instance, \"mobile phone use while driving ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>The Hands Free Law is a proposed driving law t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Texting while driving is the cause of more tha...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>So to me I disagree that using your cell phone...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>To tell the truth, the use of cellphones while...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>When we think of cell phones we think of a dev...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>People tend to text and drive without realizin...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Even today neighborhoods are have kids getting...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>In conclusion, I think that we all should be a...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>As a result of this the \"Hands Free\" law which...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Although unfortunately there are still million...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>While police and some software developers are ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Hopefully after reading this if you haven't al...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>as 69 percent of U.S. drivers between the ages...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>In this generations words don't mean anything....</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>In Georgia, the texting and driving ticket is ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>Over 37,000 people die in accidents, and which...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>No one behind the wheel of a vehicle expects ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>Some people would agree that driving while dru...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>According to the insurance institute for highw...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>In summary, drivers should know that they are ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>A distraction can come from anything, the radi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>I think teens get in more accidents than adult...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Many accidents are caused because people try t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>People shouldn't be able to use their phones w...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>Accidents can cause incident people to die, ge...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>drivers need to be more cautious of the conseq...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>When people drive with cautions driving is so ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Driver that drive with a cellphone get distrac...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>Much like the strike of a cobra, cellphones se...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>Another major part of the problem is teenagers...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>In conclusion, cellphones' grasps on the world...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>The telephone companies have commercials that ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>Now that would be technology in the making! If...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>The law is supposed to punish those people hol...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>Receiving citations can have an effect on insu...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>If an individual is walking/ jogging, and a dr...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>This essay has informed you on why it is not s...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>Whether they are looking at the weather, SnapC...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Cell phones were introduced in the United Stat...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>As a result, most governments, including Austr...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Waking up from a wonderful dream, you look aro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Let's talk about how dangerous this is. Accor...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-090d89bb-2251-4b8d-b465-fc493aa16e0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-090d89bb-2251-4b8d-b465-fc493aa16e0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-090d89bb-2251-4b8d-b465-fc493aa16e0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0    According to an article by the Edgar Snyder Fi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "1    The most common of these distractions is a cel...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "2    Reaction time is the measure of how quickly an...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "3    The affects can be physical, emotional, or psy...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "4    In conclusion, people shouldn't use cellphones...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "5    It could lead to accidents and altercations. Y...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "6    Many people happen to pass away due to a motor...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "7    In conclusion, Drivers shouldn't drive while u...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "8    Other drivers take notice when drivers are on ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "9    Driving with cell phones should be banned beca...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "10   Statistically, 50 minutes of phone chatter whi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "11   Cell phones were presented within the United S...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "12   Drivers who are distracted by their phone whil...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "13   Distracted driving is now considered to be the...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "14   Reasons for this can range from work related t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "15   The question is people more concerned more wit...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "16   A major cause of deaths in the US is motor veh...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "17   According to an article of \" Teen driver sourc...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "18   Each state in the US has their own laws to pro...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "19   While physical damage to a vehicle or physical...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "20   According to the website big AZ BIG Media, the...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "21   Although cell phones have not been around for ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "22   Throughout the years, teenagers have been eage...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "23   Utilizing a phone while driving takes many liv...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "24   Once again, using a phone while driving create...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "25   The new law was put in to ensure more safety d...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "26   A phone should not ever be used while a person...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "27   There are just so many things that we are able...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "28    If you are at a red light and you don't have ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "29   Now with these cars we don't need to worry abo...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "30   It's said that only 39 states put a ban for no...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "31   I think I could say the same thing for a passe...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "32   When using your phone while driving, you shoul...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "33   In today's day and age, technology has drastic...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "34   When you make the decision to text and drive, ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "35   Regardless of the all the harm caused by texti...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "36    In 2018 half a million people were injured be...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "37   In an article \"Just how much does car insuranc...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "38   There are 41 state that ban texting while driv...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "39   The use of cars and mobile technology has done...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "40   Drivers often use their phones for GPS in this...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "41    Although texting and driving is the cause of ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "42   According to Legalmatch \"Texting while driving...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "43   In Conclusion, Drivers should not be able to u...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "44   You're driving in your car so someone who you ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "45   On busy roads, the amount of time that one tal...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "46   94% percent of drivers support a ban on textin...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "47   On one occasion, my mom was hit from behind be...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "48   My mom's car insurance increased an additional...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "49   Everybody has their own opinion regarding cell...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "50   It doesn't matter where you are you turn your ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "51    There have been many different accidents beca...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "52   There is always pros and cons about things peo...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "53   If an emergency maneuver or emergency braking ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "54   Ever since phones have entered into the world,...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "55   \"Sending or reading a text causes drivers, on ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "56   When people use their cell phones on the highw...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "57   Why is this? It is because using a cell phone ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "58   even if the driver only looked at the phone ju...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "59   \"Drivers on the phone have worse reaction time...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "60   As the driver you not only risk your own life,...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "61   Cell phone use should be prohibited why you op...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "62   Talking while driving isn't as bad because you...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "63   The National Safety Council reports that cellp...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "64    In society, money is tight when it comes to d...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "65   Distractive driving effects all drivers equall...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "66   It is important to know because the bad behavi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "67   The driver will also experience a physical dis...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "68   You make decisions to text and drive, you make...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "69   Reasoning on no devices while driving is part ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "70   When you choose to text and drive, you're thre...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "71   You get pulled over and get a ticket. But also...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "72   Being able to use a device while operating a v...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "73   At least one out of every twenty drivers will ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "74    Amongst accidents, an average of nine teens a...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "75   On Instamotor it discusses 5 best gadgets to c...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "76   Another useful gadget is called the \"Motorola ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "77   Texting while driving is one of the worrying t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "78   Despite these consequences, people are still f...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "79   On the other hand, these news laws need to ban...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "80   I strongly agree that texting and driving is t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "81   Phones play a key role in people keeping in to...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "82   For instance, accidents caused by cellphone us...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "83   In my final judgment, I suppose we can drive s...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "84   The reason why people are using their phones w...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "85   Is it really that important? A text is not mor...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "86   The number of fatal crashes caused by cell pho...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "87   Teenagers are not as experienced drivers and t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "88   So even if you are using your phone and paying...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "89   Studies have shown that while talking to someo...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "90   Andrei Zakhareuski, from Driving Tests, procla...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "91   Cells phones are very common place for everyon...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "92   In fact adults are put at a higher risk becaus...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "93   Many people use their cell phones while drivin...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "94   Driving while operating your phone can end in ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "95   If not, put down the phone and wait until you ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "96   Among those killed: 1,730 drivers, 605 passeng...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "97   This shows that the couple of seconds you take...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "98   \\nCell phones were introduced in the United St...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "99   Researches show that using a headset can be as...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "100  People shouldn't fully depend on cell phones b...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "101  The new \"hands free\" law is a law where no one...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "102   \"Incredibly, they also spent less time watchi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "103  You will be distracted by telephone conversati...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "104  Although cell phones have not been around for ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "105  When driving, sometimes it is difficult to foc...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "106  They include talking to passengers, lighting a...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "107  Many people like to use their phone while driv...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "108  Texting and driving can cause about 5 times as...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "109   Just because a driver can get away with texti...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "110  \\nTherefore texting in driving in any state sh...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "111  The reason is that when that person is driving...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "112  The reason is that when drivers are driving, t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "113  Many habitants of planet earth have and use th...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "114  The perception of a freedom is a more effectiv...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "115  Therefore, the use of cell phones should still...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "116  Sometimes it is difficult to focus on the road...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "117  They (cops) think they can just lie as well bu...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "118  Electronic devices nowadays are in everyone's ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "119  Now we have tools that can hold your phone in ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "120  As a driver myself I try not to use my phone w...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "121  The purpose of this essay is to give you some ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "122  Passing laws that require hands free devices s...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "123  Contrary to common belief there is no such thi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "124  It's crazy the number of people that have died...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "125  Of all cell phone related tasks, texting is by...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "126  In conclusion t's now illegal to hold your pho...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "127  Nobody ever needs to use their phone while dri...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "128  Each year there are approximately 1.6 million ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "129  As to all the things there are pros and cons, ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "130  You would imagine that being on a phone call w...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "131  The Internet has allowed for cell phones to ha...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "132  In conclusion, it is my opinion that drivers s...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "133  There thousands of reasons that you should not...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "134  The average cellphone while driving violation ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "135  Imagine for just one moment a teen has friends...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "136  Starting in the 1996, cell phones became very ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "137  If driving from Georgia to New York with a map...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "138  With bluetooth in a car, you wouldn't be break...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "139  As you can see driving with your phone is not ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "140   \"Economic losses from distracted driving coul...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "141  Finally, the laws for driving and being on you...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "142  I know phones shouldn't be allowed to be messe...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "143  In an article by teensafe. com it states that,...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "144  Just imagine losing a loved one due to someone...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "145  There is no doubt that with how advanced techn...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "146  This is so that if someone does happen to have...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "147  Using the bluetooth phone hookup feature that ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "148  In conclusion, I feel that in the future cell ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "149  Thousands of accidents happen in one year. Mos...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "150  Monetary fines these can range from as low as ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "151  For, Conclusion being on your phone while driv...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "152  Should drivers be allowed to use their phones ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "153  Nowadays, teens feel the need to always have t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "154  In 2015, 391,000 people were injured in motor ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "155  To conclude, drivers, no matter the age or exp...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "156   Drivers that text while driving are likely to...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "157  For instance, \"mobile phone use while driving ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "158  The Hands Free Law is a proposed driving law t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "159  Texting while driving is the cause of more tha...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "160  So to me I disagree that using your cell phone...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "161  To tell the truth, the use of cellphones while...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "162  When we think of cell phones we think of a dev...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "163  People tend to text and drive without realizin...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "164  Even today neighborhoods are have kids getting...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "165  In conclusion, I think that we all should be a...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "166  As a result of this the \"Hands Free\" law which...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "167  Although unfortunately there are still million...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "168  While police and some software developers are ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "169  Hopefully after reading this if you haven't al...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "170  as 69 percent of U.S. drivers between the ages...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "171  In this generations words don't mean anything....       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "172  In Georgia, the texting and driving ticket is ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "173  Over 37,000 people die in accidents, and which...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "174   No one behind the wheel of a vehicle expects ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "175  Some people would agree that driving while dru...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "176  According to the insurance institute for highw...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "177  In summary, drivers should know that they are ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "178  A distraction can come from anything, the radi...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "179  I think teens get in more accidents than adult...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "180  Many accidents are caused because people try t...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "181  People shouldn't be able to use their phones w...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "182  Accidents can cause incident people to die, ge...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "183  drivers need to be more cautious of the conseq...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "184  When people drive with cautions driving is so ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "185  Driver that drive with a cellphone get distrac...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "186  Much like the strike of a cobra, cellphones se...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "187  Another major part of the problem is teenagers...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "188  In conclusion, cellphones' grasps on the world...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "189  The telephone companies have commercials that ...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "190  Now that would be technology in the making! If...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "191  The law is supposed to punish those people hol...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "192  Receiving citations can have an effect on insu...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "193  If an individual is walking/ jogging, and a dr...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "194  This essay has informed you on why it is not s...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "195  Whether they are looking at the weather, SnapC...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "196  Cell phones were introduced in the United Stat...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "197  As a result, most governments, including Austr...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "198  Waking up from a wonderful dream, you look aro...       0.0     0.0         0.0          0.0      0.0          0.0\n",
              "199   Let's talk about how dangerous this is. Accor...       0.0     0.0         0.0          0.0      0.0          0.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "size = 200\n",
        "old_train = old_train[[\"discourse_text\"]].rename(columns={\"discourse_text\": \"full_text\"})\n",
        "old_train = old_train[old_train[\"full_text\"].str.len() >= 500].head(size).reset_index()\n",
        "old_train = old_train.drop(columns = ['index'])\n",
        "initial_values = np.zeros((len(old_train), 6))\n",
        "old_train[CFG.target_cols] = initial_values\n",
        "old_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae4a81b",
      "metadata": {
        "id": "dae4a81b",
        "papermill": {
          "duration": 0.009487,
          "end_time": "2022-08-31T22:32:28.527661",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.518174",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# CV split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "32200ebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:28.554307Z",
          "iopub.status.busy": "2022-08-31T22:32:28.553699Z",
          "iopub.status.idle": "2022-08-31T22:32:28.704323Z",
          "shell.execute_reply": "2022-08-31T22:32:28.703340Z"
        },
        "id": "32200ebe",
        "outputId": "1f5ada39-c01c-447f-b19d-73cab18f6c91",
        "papermill": {
          "duration": 0.164445,
          "end_time": "2022-08-31T22:32:28.706697",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.542252",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "fold\n",
              "0    250\n",
              "1    250\n",
              "2    250\n",
              "3    250\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====================================================\n",
        "# CV split\n",
        "# ====================================================\n",
        "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
        "    train.loc[val_index, 'fold'] = int(n)\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "display(train.groupby('fold').size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "-R33CkEsSpEu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R33CkEsSpEu",
        "outputId": "a888a7c5-356a-4816-e24a-30f7eb5caf12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a025258f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:28.724353Z",
          "iopub.status.busy": "2022-08-31T22:32:28.724052Z",
          "iopub.status.idle": "2022-08-31T22:32:28.729744Z",
          "shell.execute_reply": "2022-08-31T22:32:28.728851Z"
        },
        "id": "a025258f",
        "papermill": {
          "duration": 0.016964,
          "end_time": "2022-08-31T22:32:28.731888",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.714924",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "if CFG.debug:\n",
        "    display(train.groupby('fold').size())\n",
        "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
        "    display(train.groupby('fold').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2d766a",
      "metadata": {
        "id": "aa2d766a",
        "papermill": {
          "duration": 0.007793,
          "end_time": "2022-08-31T22:32:28.747828",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.740035",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2fa845d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:28.765839Z",
          "iopub.status.busy": "2022-08-31T22:32:28.765473Z",
          "iopub.status.idle": "2022-08-31T22:32:35.437418Z",
          "shell.execute_reply": "2022-08-31T22:32:35.436468Z"
        },
        "id": "d2fa845d",
        "papermill": {
          "duration": 6.683936,
          "end_time": "2022-08-31T22:32:35.439931",
          "exception": false,
          "start_time": "2022-08-31T22:32:28.755995",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# tokenizer\n",
        "# ====================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model) #get deberta tokenizer \n",
        "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
        "CFG.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df117510",
      "metadata": {
        "id": "df117510",
        "papermill": {
          "duration": 0.008087,
          "end_time": "2022-08-31T22:32:35.456694",
          "exception": false,
          "start_time": "2022-08-31T22:32:35.448607",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dac78bb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "1a4d3d7abd14464abdd90484b2efa3b1",
            "309ca4ae760a4938b5da41b61634dae8",
            "61791d4971284907a704a8e5df365dd1",
            "27a82cd579d647e9932df00e88180976",
            "a3991ef6c11045edbc87df951a2091eb",
            "69cb2d77b57d4d38960d61c06b49db75",
            "ac523221f7af49af978945c12395dae1",
            "2eea28252b524bf79272f13af0782bfd",
            "16f57f81db6c45228e19dd35c4b7940f",
            "65d8c52f341f4ee49edf3193038d1a32",
            "a4a6f40b653842dc9f1c8a75ad02b236"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:35.476349Z",
          "iopub.status.busy": "2022-08-31T22:32:35.475302Z",
          "iopub.status.idle": "2022-08-31T22:32:41.683449Z",
          "shell.execute_reply": "2022-08-31T22:32:41.681990Z"
        },
        "id": "dac78bb9",
        "outputId": "d9e2b30f-957a-4db7-9040-476f0a897e05",
        "papermill": {
          "duration": 6.221034,
          "end_time": "2022-08-31T22:32:41.686006",
          "exception": false,
          "start_time": "2022-08-31T22:32:35.464972",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a4d3d7abd14464abdd90484b2efa3b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
            "max_len: 4182\n",
            "INFO:__main__:max_len: 4182\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4182"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ====================================================\n",
        "# Define max_len\n",
        "# Find max token length after tokenizing each text \n",
        "# ====================================================\n",
        "lengths = []\n",
        "tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
        "for text in tk0:\n",
        "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
        "LOGGER.info(f\"max_len: {CFG.max_len}\")\n",
        "CFG.max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "14b26230",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:41.706635Z",
          "iopub.status.busy": "2022-08-31T22:32:41.704899Z",
          "iopub.status.idle": "2022-08-31T22:32:41.715654Z",
          "shell.execute_reply": "2022-08-31T22:32:41.714775Z"
        },
        "id": "14b26230",
        "papermill": {
          "duration": 0.022792,
          "end_time": "2022-08-31T22:32:41.717965",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.695173",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Dataset\n",
        "# ====================================================\n",
        "def prepare_input(cfg, text):\n",
        "    inputs = cfg.tokenizer.encode_plus(\n",
        "        text, \n",
        "        return_tensors=None, \n",
        "        add_special_tokens=True, \n",
        "        max_length=CFG.max_len,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
        "    return inputs #a list of tensors \n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, cfg, df):\n",
        "        self.cfg = cfg\n",
        "        self.texts = df['full_text'].values\n",
        "        self.labels = df[cfg.target_cols].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        inputs = prepare_input(self.cfg, self.texts[item])\n",
        "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
        "        return inputs, label\n",
        "    \n",
        "\n",
        "def collate(inputs):\n",
        "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = inputs[k][:,:mask_len]\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bda5663",
      "metadata": {
        "id": "2bda5663",
        "papermill": {
          "duration": 0.008337,
          "end_time": "2022-08-31T22:32:41.734920",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.726583",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a5f24525",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:41.754836Z",
          "iopub.status.busy": "2022-08-31T22:32:41.754449Z",
          "iopub.status.idle": "2022-08-31T22:32:41.773130Z",
          "shell.execute_reply": "2022-08-31T22:32:41.769556Z"
        },
        "id": "a5f24525",
        "papermill": {
          "duration": 0.031877,
          "end_time": "2022-08-31T22:32:41.776126",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.744249",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Model\n",
        "# ====================================================\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() \n",
        "        #last_hidden_state is a list of n tensors, n is sequence length\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        #padding: [PAD] make same length, for [PAD] attention_mask = 0\n",
        "        #last_hidden_state is output of the model\n",
        "        return mean_embeddings\n",
        "    \n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if config_path is None:\n",
        "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True) \n",
        "            #output of every layer during inference\n",
        "            self.config.hidden_dropout = 0.\n",
        "            self.config.hidden_dropout_prob = 0.\n",
        "            self.config.attention_dropout = 0.\n",
        "            self.config.attention_probs_dropout_prob = 0.\n",
        "            LOGGER.info(self.config)\n",
        "        else:\n",
        "            self.config = torch.load(config_path)\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
        "        else:\n",
        "            self.model = AutoModel(self.config)\n",
        "        if self.cfg.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "        self.pool = MeanPooling()\n",
        "        self.fc = nn.Linear(self.config.hidden_size, 6) #hidden_size is input size, 6 is output size \n",
        "        self._init_weights(self.fc)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs[0]\n",
        "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
        "        return feature\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature = self.feature(inputs)\n",
        "        output = self.fc(feature)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f90882e",
      "metadata": {
        "id": "3f90882e",
        "papermill": {
          "duration": 0.008836,
          "end_time": "2022-08-31T22:32:41.801081",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.792245",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4ffd8144",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:41.819862Z",
          "iopub.status.busy": "2022-08-31T22:32:41.819486Z",
          "iopub.status.idle": "2022-08-31T22:32:41.829800Z",
          "shell.execute_reply": "2022-08-31T22:32:41.827775Z"
        },
        "id": "4ffd8144",
        "papermill": {
          "duration": 0.023124,
          "end_time": "2022-08-31T22:32:41.832806",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.809682",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Loss\n",
        "# ====================================================\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean', eps=1e-9):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.reduction = reduction\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
        "        if self.reduction == 'none':\n",
        "            loss = loss\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c15011e",
      "metadata": {
        "id": "1c15011e",
        "papermill": {
          "duration": 0.008528,
          "end_time": "2022-08-31T22:32:41.852306",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.843778",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Helpler functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c11acdda",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:41.872384Z",
          "iopub.status.busy": "2022-08-31T22:32:41.872032Z",
          "iopub.status.idle": "2022-08-31T22:32:41.892510Z",
          "shell.execute_reply": "2022-08-31T22:32:41.891247Z"
        },
        "id": "c11acdda",
        "papermill": {
          "duration": 0.034604,
          "end_time": "2022-08-31T22:32:41.895844",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.861240",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
        "    losses = AverageMeter()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = collate(inputs)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
        "            y_preds = model(inputs)\n",
        "            loss = criterion(y_preds, labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        #optimize\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            if CFG.batch_scheduler:\n",
        "                scheduler.step()\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  'LR: {lr:.8f}  '\n",
        "                  .format(epoch+1, step, len(train_loader), \n",
        "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                          loss=losses,\n",
        "                          grad_norm=grad_norm,\n",
        "                          lr=scheduler.get_lr()[0]))\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
        "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    start = end = time.time()\n",
        "    for step, (inputs, labels) in enumerate(valid_loader):\n",
        "        inputs = collate(inputs)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(inputs) #predict\n",
        "            loss = criterion(y_preds, labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        preds.append(y_preds.to('cpu').numpy())\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(step, len(valid_loader),\n",
        "                          loss=losses,\n",
        "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
        "    predictions = np.concatenate(preds)\n",
        "    return losses.avg, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730d838d",
      "metadata": {
        "id": "730d838d",
        "papermill": {
          "duration": 0.008782,
          "end_time": "2022-08-31T22:32:41.916203",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.907421",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "qeIFn4hJywBR",
      "metadata": {
        "id": "qeIFn4hJywBR"
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "# ====================================================\n",
        "# train loop\n",
        "# ====================================================\n",
        "import math\n",
        "def train_loop(folds, fold): \n",
        "     \n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # loader\n",
        "    # ====================================================\n",
        "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
        "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
        "    valid_labels = valid_folds[CFG.target_cols].values\n",
        "    \n",
        "    train_dataset = TrainDataset(CFG, train_folds)\n",
        "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=CFG.batch_size * 2,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "    # ====================================================\n",
        "    # model & optimizer\n",
        "    # ====================================================\n",
        "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
        "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
        "    model.to(device)\n",
        "    \n",
        "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_parameters = [\n",
        "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
        "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
        "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
        "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        return optimizer_parameters\n",
        "\n",
        "    optimizer_parameters = get_optimizer_params(model,\n",
        "                                                encoder_lr=CFG.encoder_lr, \n",
        "                                                decoder_lr=CFG.decoder_lr,\n",
        "                                                weight_decay=CFG.weight_decay)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
        "    \n",
        "    # ====================================================\n",
        "    # scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
        "        if cfg.scheduler == 'linear':\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
        "            )\n",
        "        elif cfg.scheduler == 'cosine':\n",
        "            scheduler = get_cosine_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
        "            )\n",
        "        return scheduler\n",
        "    \n",
        "    # num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
        "    # scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
        "\n",
        "    pesudoLabel_data = old_train #dataframe\n",
        "    pesudoLabel_data_len = len(pesudoLabel_data)\n",
        "    num_sample = 5\n",
        "    sample_size = int(pesudoLabel_data_len / num_sample)\n",
        "\n",
        "    num_train_steps = int((len(train_folds) + math.ceil(sample_size * num_sample / 2)) / CFG.batch_size * CFG.epochs * num_sample + len(train_folds) \n",
        "                          / CFG.batch_size * CFG.epochs)\n",
        "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
        "\n",
        "    # ====================================================\n",
        "    # loop\n",
        "    # ====================================================\n",
        "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
        "    \n",
        "    best_score = np.inf\n",
        "    lr_scaler = 4\n",
        "    # 5 Epoch with origin data\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "         \n",
        "        # train\n",
        "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
        "        \n",
        "        # scoring\n",
        "        score, scores = get_score(valid_labels, predictions)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
        "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
        "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
        "                       f\"[fold{fold}] score\": score})\n",
        "        \n",
        "        if best_score > score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        'predictions': predictions},\n",
        "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
        "\n",
        "    #unlabled_data = old data\n",
        "    \n",
        "    for i in range(1, 6):\n",
        "        # new scheduler\n",
        "        # optimizer_parameters = get_optimizer_params(model,\n",
        "        #                                         encoder_lr=CFG.encoder_lr / lr_scaler, \n",
        "        #                                         decoder_lr=CFG.decoder_lr / lr_scaler,\n",
        "        #                                         weight_decay=CFG.weight_decay)\n",
        "        # optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr / lr_scaler, eps=CFG.eps, betas=CFG.betas)\n",
        "        # lr_scaler = lr_scaler * lr_scaler\n",
        "        \n",
        "        # num_train_steps = int((len(train_folds) + i * sample_size) / CFG.batch_size * CFG.epochs)\n",
        "        # scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
        "\n",
        "        pesudoLabel_data_use = pesudoLabel_data[:sample_size]\n",
        "        pesudoLabel_data = pesudoLabel_data[sample_size:]\n",
        "        #unlabled_data = unlabled_data - sampled_data\n",
        "        pesudo_dataset = TrainDataset(CFG, pesudoLabel_data_use)\n",
        "        pesudo_loader = DataLoader(pesudo_dataset,\n",
        "                              batch_size=CFG.batch_size * 2,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "        preds = np.array([[]])\n",
        "        start = end = time.time()\n",
        "        i = 0\n",
        "        for step, (inputs, labels) in enumerate(pesudo_loader):\n",
        "            inputs = collate(inputs)\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.to(device)\n",
        "            with torch.no_grad():\n",
        "                y_preds = model(inputs) #predict\n",
        "            if i != 0:\n",
        "              preds = np.concatenate((preds, y_preds.to('cpu').numpy()), axis = 0)\n",
        "            else:\n",
        "              i = 1\n",
        "              preds = y_preds.to('cpu').numpy()\n",
        "            end = time.time()\n",
        "        \n",
        "        print(np.array(preds).shape, pesudoLabel_data_use.columns)\n",
        "        pesudoLabel_data_use[CFG.target_cols] = preds\n",
        "        print(train_folds.columns)\n",
        "        print(pesudoLabel_data_use.columns)\n",
        "        frames = [train_folds, pesudoLabel_data_use]\n",
        "        train_folds = pd.concat(frames)\n",
        "        train_dataset = TrainDataset(CFG, train_folds)\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "\n",
        "        # 5 Epochs training with origin data + pseudo-labled data\n",
        "        for epoch in range(CFG.epochs):\n",
        "\n",
        "            \n",
        "                \n",
        "            start_time = time.time()\n",
        "\n",
        "            #predict for all sampled_data, generate labled data\n",
        "            #stack sampled_data on new data\n",
        "            #add to train_loader\n",
        "            \n",
        "            # train\n",
        "            avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "            # eval\n",
        "            avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
        "            \n",
        "            # scoring\n",
        "            score, scores = get_score(valid_labels, predictions)\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
        "            if CFG.wandb:\n",
        "                wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
        "                          f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
        "                          f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
        "                          f\"[fold{fold}] score\": score})\n",
        "            \n",
        "            if best_score > score:\n",
        "                best_score = score\n",
        "                LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "                torch.save({'model': model.state_dict(),\n",
        "                            'predictions': predictions},\n",
        "                            OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
        "            \n",
        "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
        "                             map_location=torch.device('cpu'))['predictions']\n",
        "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return valid_folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "BTIYgNBfy88q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BTIYgNBfy88q",
        "outputId": "be8378be-d8a9-44db-e1d3-7616faa22cf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "INFO:__main__:========== fold: 0 training ==========\n",
            "DebertaConfig {\n",
            "  \"_name_or_path\": \"/content/gdrive/MyDrive/Colab_Notebooks/Deberta/microsoft_deberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"DebertaModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:__main__:DebertaConfig {\n",
            "  \"_name_or_path\": \"/content/gdrive/MyDrive/Colab_Notebooks/Deberta/microsoft_deberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"DebertaModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/750] Elapsed 0m 4s (remain 57m 15s) Loss: 2.7481(2.7481) Grad: inf  LR: 0.00000200  \n",
            "Epoch: [1][20/750] Elapsed 0m 10s (remain 5m 54s) Loss: 1.8999(2.1047) Grad: 382233.0312  LR: 0.00000200  \n",
            "Epoch: [1][40/750] Elapsed 0m 15s (remain 4m 30s) Loss: 2.3224(1.8411) Grad: 456535.5000  LR: 0.00000200  \n",
            "Epoch: [1][60/750] Elapsed 0m 21s (remain 3m 58s) Loss: 0.2473(1.4236) Grad: 405036.4062  LR: 0.00000200  \n",
            "Epoch: [1][80/750] Elapsed 0m 26s (remain 3m 40s) Loss: 0.0941(1.1234) Grad: 234005.9844  LR: 0.00000200  \n",
            "Epoch: [1][100/750] Elapsed 0m 32s (remain 3m 26s) Loss: 0.9347(0.9415) Grad: 579065.8125  LR: 0.00000200  \n",
            "Epoch: [1][120/750] Elapsed 0m 38s (remain 3m 19s) Loss: 0.0800(0.8231) Grad: 230424.7031  LR: 0.00000200  \n",
            "Epoch: [1][140/750] Elapsed 0m 44s (remain 3m 10s) Loss: 0.1007(0.7336) Grad: 250333.1719  LR: 0.00000200  \n",
            "Epoch: [1][160/750] Elapsed 0m 50s (remain 3m 3s) Loss: 0.0546(0.6639) Grad: 176067.3750  LR: 0.00000200  \n",
            "Epoch: [1][180/750] Elapsed 0m 55s (remain 2m 54s) Loss: 0.1821(0.6184) Grad: 341948.5625  LR: 0.00000200  \n",
            "Epoch: [1][200/750] Elapsed 1m 1s (remain 2m 46s) Loss: 0.0834(0.5699) Grad: 235184.3281  LR: 0.00000200  \n",
            "Epoch: [1][220/750] Elapsed 1m 6s (remain 2m 39s) Loss: 0.3955(0.5379) Grad: 231577.7344  LR: 0.00000200  \n",
            "Epoch: [1][240/750] Elapsed 1m 12s (remain 2m 32s) Loss: 0.1666(0.5124) Grad: 176467.8594  LR: 0.00000200  \n",
            "Epoch: [1][260/750] Elapsed 1m 17s (remain 2m 25s) Loss: 0.0871(0.4861) Grad: 126003.5000  LR: 0.00000200  \n",
            "Epoch: [1][280/750] Elapsed 1m 23s (remain 2m 18s) Loss: 0.2331(0.4679) Grad: 199675.0781  LR: 0.00000200  \n",
            "Epoch: [1][300/750] Elapsed 1m 28s (remain 2m 11s) Loss: 0.3393(0.4495) Grad: 237870.8750  LR: 0.00000200  \n",
            "Epoch: [1][320/750] Elapsed 1m 33s (remain 2m 5s) Loss: 0.1054(0.4293) Grad: 150100.6250  LR: 0.00000200  \n",
            "Epoch: [1][340/750] Elapsed 1m 39s (remain 1m 59s) Loss: 0.0631(0.4118) Grad: 119935.6797  LR: 0.00000200  \n",
            "Epoch: [1][360/750] Elapsed 1m 44s (remain 1m 53s) Loss: 0.2013(0.3992) Grad: 106118.0938  LR: 0.00000200  \n",
            "Epoch: [1][380/750] Elapsed 1m 50s (remain 1m 47s) Loss: 0.2428(0.3890) Grad: 109186.5156  LR: 0.00000200  \n",
            "Epoch: [1][400/750] Elapsed 1m 55s (remain 1m 40s) Loss: 0.0332(0.3771) Grad: 39984.4688  LR: 0.00000200  \n",
            "Epoch: [1][420/750] Elapsed 2m 1s (remain 1m 34s) Loss: 0.1051(0.3690) Grad: 65652.3203  LR: 0.00000200  \n",
            "Epoch: [1][440/750] Elapsed 2m 6s (remain 1m 28s) Loss: 0.0937(0.3602) Grad: 65836.1172  LR: 0.00000200  \n",
            "Epoch: [1][460/750] Elapsed 2m 12s (remain 1m 22s) Loss: 0.1473(0.3506) Grad: 77929.0938  LR: 0.00000200  \n",
            "Epoch: [1][480/750] Elapsed 2m 17s (remain 1m 17s) Loss: 0.1339(0.3420) Grad: 120027.8672  LR: 0.00000200  \n",
            "Epoch: [1][500/750] Elapsed 2m 23s (remain 1m 11s) Loss: 0.4912(0.3359) Grad: 363932.2188  LR: 0.00000200  \n",
            "Epoch: [1][520/750] Elapsed 2m 29s (remain 1m 5s) Loss: 0.1782(0.3289) Grad: 84548.7344  LR: 0.00000200  \n",
            "Epoch: [1][540/750] Elapsed 2m 34s (remain 0m 59s) Loss: 0.1040(0.3219) Grad: 71854.4375  LR: 0.00000200  \n",
            "Epoch: [1][560/750] Elapsed 2m 40s (remain 0m 53s) Loss: 0.1194(0.3146) Grad: 83778.7734  LR: 0.00000200  \n",
            "Epoch: [1][580/750] Elapsed 2m 46s (remain 0m 48s) Loss: 0.0231(0.3090) Grad: 28009.7617  LR: 0.00000200  \n",
            "Epoch: [1][600/750] Elapsed 2m 51s (remain 0m 42s) Loss: 0.1393(0.3030) Grad: 82893.4062  LR: 0.00000200  \n",
            "Epoch: [1][620/750] Elapsed 2m 57s (remain 0m 36s) Loss: 0.0607(0.2973) Grad: 50413.5508  LR: 0.00000200  \n",
            "Epoch: [1][640/750] Elapsed 3m 2s (remain 0m 31s) Loss: 0.0637(0.2912) Grad: 48757.4883  LR: 0.00000200  \n",
            "Epoch: [1][660/750] Elapsed 3m 8s (remain 0m 25s) Loss: 1.1146(0.2875) Grad: 158976.0156  LR: 0.00000200  \n",
            "Epoch: [1][680/750] Elapsed 3m 13s (remain 0m 19s) Loss: 0.1096(0.2830) Grad: 66755.3203  LR: 0.00000200  \n",
            "Epoch: [1][700/750] Elapsed 3m 19s (remain 0m 13s) Loss: 0.0868(0.2776) Grad: 61027.5156  LR: 0.00000200  \n",
            "Epoch: [1][720/750] Elapsed 3m 24s (remain 0m 8s) Loss: 0.0711(0.2743) Grad: 64039.2148  LR: 0.00000200  \n",
            "Epoch: [1][740/750] Elapsed 3m 31s (remain 0m 2s) Loss: 0.0549(0.2705) Grad: 45688.2891  LR: 0.00000200  \n",
            "Epoch: [1][749/750] Elapsed 3m 34s (remain 0m 0s) Loss: 0.0817(0.2690) Grad: 52762.4141  LR: 0.00000200  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 32s) Loss: 0.0546(0.0546) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1120(0.1843) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.1559(0.1675) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.1029(0.1637) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.2304(0.1680) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0441(0.1684) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0977(0.1759) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2690  avg_val_loss: 0.1742  time: 231s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.2690  avg_val_loss: 0.1742  time: 231s\n",
            "Epoch 1 - Score: 0.5986  Scores: [0.641151946503038, 0.5158336806909397, 0.562247368306175, 0.6336507957886203, 0.620643569047055, 0.6182538218846939]\n",
            "INFO:__main__:Epoch 1 - Score: 0.5986  Scores: [0.641151946503038, 0.5158336806909397, 0.562247368306175, 0.6336507957886203, 0.620643569047055, 0.6182538218846939]\n",
            "Epoch 1 - Save Best Score: 0.5986 Model\n",
            "INFO:__main__:Epoch 1 - Save Best Score: 0.5986 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.1423(0.1742) \n",
            "Epoch: [2][0/750] Elapsed 0m 0s (remain 5m 31s) Loss: 0.0951(0.0951) Grad: inf  LR: 0.00000200  \n",
            "Epoch: [2][20/750] Elapsed 0m 5s (remain 3m 24s) Loss: 0.0512(0.1130) Grad: 159013.7344  LR: 0.00000200  \n",
            "Epoch: [2][40/750] Elapsed 0m 12s (remain 3m 30s) Loss: 0.1020(0.1009) Grad: 265847.5000  LR: 0.00000200  \n",
            "Epoch: [2][60/750] Elapsed 0m 18s (remain 3m 26s) Loss: 0.1301(0.0933) Grad: 257116.5312  LR: 0.00000199  \n",
            "Epoch: [2][80/750] Elapsed 0m 23s (remain 3m 16s) Loss: 0.3389(0.0974) Grad: 569955.7500  LR: 0.00000199  \n",
            "Epoch: [2][100/750] Elapsed 0m 29s (remain 3m 8s) Loss: 0.1435(0.0945) Grad: 319821.9375  LR: 0.00000199  \n",
            "Epoch: [2][120/750] Elapsed 0m 34s (remain 3m 1s) Loss: 0.1142(0.0919) Grad: 311512.4062  LR: 0.00000199  \n",
            "Epoch: [2][140/750] Elapsed 0m 40s (remain 2m 54s) Loss: 0.0650(0.1050) Grad: 187572.8281  LR: 0.00000199  \n",
            "Epoch: [2][160/750] Elapsed 0m 45s (remain 2m 48s) Loss: 0.0420(0.1055) Grad: 183787.8438  LR: 0.00000199  \n",
            "Epoch: [2][180/750] Elapsed 0m 51s (remain 2m 42s) Loss: 0.0869(0.1082) Grad: 223792.0312  LR: 0.00000199  \n",
            "Epoch: [2][200/750] Elapsed 0m 57s (remain 2m 35s) Loss: 0.1245(0.1100) Grad: 275236.2812  LR: 0.00000199  \n",
            "Epoch: [2][220/750] Elapsed 1m 2s (remain 2m 29s) Loss: 0.0565(0.1102) Grad: 186184.6562  LR: 0.00000199  \n",
            "Epoch: [2][240/750] Elapsed 1m 8s (remain 2m 23s) Loss: 0.0804(0.1080) Grad: 232404.1094  LR: 0.00000199  \n",
            "Epoch: [2][260/750] Elapsed 1m 13s (remain 2m 17s) Loss: 0.0406(0.1084) Grad: 160290.1250  LR: 0.00000199  \n",
            "Epoch: [2][280/750] Elapsed 1m 18s (remain 2m 11s) Loss: 0.1361(0.1061) Grad: 306071.4062  LR: 0.00000199  \n",
            "Epoch: [2][300/750] Elapsed 1m 24s (remain 2m 5s) Loss: 0.2293(0.1067) Grad: 513279.5625  LR: 0.00000199  \n",
            "Epoch: [2][320/750] Elapsed 1m 29s (remain 2m 0s) Loss: 0.0106(0.1065) Grad: 84984.9531  LR: 0.00000199  \n",
            "Epoch: [2][340/750] Elapsed 1m 35s (remain 1m 54s) Loss: 0.0803(0.1070) Grad: 234825.6094  LR: 0.00000199  \n",
            "Epoch: [2][360/750] Elapsed 1m 41s (remain 1m 49s) Loss: 0.1111(0.1059) Grad: 290087.1250  LR: 0.00000199  \n",
            "Epoch: [2][380/750] Elapsed 1m 47s (remain 1m 44s) Loss: 0.1221(0.1061) Grad: 298270.0312  LR: 0.00000199  \n",
            "Epoch: [2][400/750] Elapsed 1m 53s (remain 1m 38s) Loss: 0.0422(0.1069) Grad: 77431.9922  LR: 0.00000199  \n",
            "Epoch: [2][420/750] Elapsed 1m 58s (remain 1m 32s) Loss: 0.0548(0.1071) Grad: 114496.9062  LR: 0.00000199  \n",
            "Epoch: [2][440/750] Elapsed 2m 4s (remain 1m 27s) Loss: 0.2429(0.1069) Grad: 234594.2344  LR: 0.00000199  \n",
            "Epoch: [2][460/750] Elapsed 2m 10s (remain 1m 21s) Loss: 0.0580(0.1080) Grad: 110940.1016  LR: 0.00000199  \n",
            "Epoch: [2][480/750] Elapsed 2m 15s (remain 1m 15s) Loss: 0.0298(0.1084) Grad: 85271.8906  LR: 0.00000199  \n",
            "Epoch: [2][500/750] Elapsed 2m 21s (remain 1m 10s) Loss: 0.0441(0.1086) Grad: 80870.8594  LR: 0.00000199  \n",
            "Epoch: [2][520/750] Elapsed 2m 26s (remain 1m 4s) Loss: 0.0820(0.1085) Grad: 128297.8672  LR: 0.00000199  \n",
            "Epoch: [2][540/750] Elapsed 2m 32s (remain 0m 58s) Loss: 0.1646(0.1080) Grad: 197490.3125  LR: 0.00000199  \n",
            "Epoch: [2][560/750] Elapsed 2m 37s (remain 0m 53s) Loss: 0.0789(0.1084) Grad: 107938.8125  LR: 0.00000199  \n",
            "Epoch: [2][580/750] Elapsed 2m 43s (remain 0m 47s) Loss: 0.0510(0.1077) Grad: 104991.3047  LR: 0.00000199  \n",
            "Epoch: [2][600/750] Elapsed 2m 48s (remain 0m 41s) Loss: 0.0977(0.1084) Grad: 113480.4219  LR: 0.00000199  \n",
            "Epoch: [2][620/750] Elapsed 2m 54s (remain 0m 36s) Loss: 0.2085(0.1090) Grad: 214172.0156  LR: 0.00000199  \n",
            "Epoch: [2][640/750] Elapsed 3m 0s (remain 0m 30s) Loss: 0.0070(0.1084) Grad: 27235.4004  LR: 0.00000198  \n",
            "Epoch: [2][660/750] Elapsed 3m 5s (remain 0m 25s) Loss: 0.0431(0.1084) Grad: 88153.6250  LR: 0.00000198  \n",
            "Epoch: [2][680/750] Elapsed 3m 11s (remain 0m 19s) Loss: 0.2518(0.1086) Grad: 248131.4062  LR: 0.00000198  \n",
            "Epoch: [2][700/750] Elapsed 3m 17s (remain 0m 13s) Loss: 0.0237(0.1079) Grad: 56093.5352  LR: 0.00000198  \n",
            "Epoch: [2][720/750] Elapsed 3m 23s (remain 0m 8s) Loss: 0.1039(0.1079) Grad: 148366.9844  LR: 0.00000198  \n",
            "Epoch: [2][740/750] Elapsed 3m 28s (remain 0m 2s) Loss: 0.6362(0.1086) Grad: 321496.3438  LR: 0.00000198  \n",
            "Epoch: [2][749/750] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0643(0.1089) Grad: 100043.4219  LR: 0.00000198  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0419(0.0419) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0694(0.1175) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0407(0.1104) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0925(0.1107) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1034(0.1121) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1067(0.1103) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0628(0.1130) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1089  avg_val_loss: 0.1118  time: 227s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.1089  avg_val_loss: 0.1118  time: 227s\n",
            "Epoch 2 - Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n",
            "INFO:__main__:Epoch 2 - Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n",
            "Epoch 2 - Save Best Score: 0.4744 Model\n",
            "INFO:__main__:Epoch 2 - Save Best Score: 0.4744 Model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0760(0.1118) \n",
            "Epoch: [3][0/750] Elapsed 0m 0s (remain 6m 2s) Loss: 0.0488(0.0488) Grad: 339814.7500  LR: 0.00000198  \n",
            "Epoch: [3][20/750] Elapsed 0m 5s (remain 3m 27s) Loss: 0.0592(0.0868) Grad: 213397.0938  LR: 0.00000198  \n",
            "Epoch: [3][40/750] Elapsed 0m 11s (remain 3m 23s) Loss: 0.1139(0.1217) Grad: 265611.0625  LR: 0.00000198  \n",
            "Epoch: [3][60/750] Elapsed 0m 17s (remain 3m 16s) Loss: 0.0404(0.1110) Grad: 153278.4219  LR: 0.00000198  \n",
            "Epoch: [3][80/750] Elapsed 0m 23s (remain 3m 15s) Loss: 0.0869(0.1049) Grad: 258457.7031  LR: 0.00000198  \n",
            "Epoch: [3][100/750] Elapsed 0m 29s (remain 3m 7s) Loss: 0.0803(0.0997) Grad: 223013.5312  LR: 0.00000198  \n",
            "Epoch: [3][120/750] Elapsed 0m 34s (remain 3m 0s) Loss: 0.1515(0.1040) Grad: 323058.3438  LR: 0.00000198  \n",
            "Epoch: [3][140/750] Elapsed 0m 40s (remain 2m 53s) Loss: 0.0459(0.1000) Grad: 163850.4375  LR: 0.00000198  \n",
            "Epoch: [3][160/750] Elapsed 0m 45s (remain 2m 47s) Loss: 0.0543(0.0962) Grad: 183162.9844  LR: 0.00000198  \n",
            "Epoch: [3][180/750] Elapsed 0m 52s (remain 2m 44s) Loss: 0.1042(0.0974) Grad: 290926.4062  LR: 0.00000198  \n",
            "Epoch: [3][200/750] Elapsed 0m 58s (remain 2m 40s) Loss: 0.3291(0.0974) Grad: 1398012.3750  LR: 0.00000198  \n",
            "Epoch: [3][220/750] Elapsed 1m 4s (remain 2m 33s) Loss: 0.0547(0.0949) Grad: 184080.3125  LR: 0.00000198  \n",
            "Epoch: [3][240/750] Elapsed 1m 9s (remain 2m 26s) Loss: 0.1377(0.0938) Grad: 316680.0312  LR: 0.00000198  \n",
            "Epoch: [3][260/750] Elapsed 1m 15s (remain 2m 22s) Loss: 0.1324(0.0934) Grad: 478590.2812  LR: 0.00000198  \n",
            "Epoch: [3][280/750] Elapsed 1m 21s (remain 2m 15s) Loss: 0.0930(0.0948) Grad: 275752.6562  LR: 0.00000198  \n",
            "Epoch: [3][300/750] Elapsed 1m 26s (remain 2m 9s) Loss: 0.0333(0.0941) Grad: 169477.7344  LR: 0.00000197  \n",
            "Epoch: [3][320/750] Elapsed 1m 32s (remain 2m 3s) Loss: 0.0948(0.0941) Grad: 275728.2500  LR: 0.00000197  \n",
            "Epoch: [3][340/750] Elapsed 1m 38s (remain 1m 57s) Loss: 0.0298(0.0940) Grad: 128498.4219  LR: 0.00000197  \n",
            "Epoch: [3][360/750] Elapsed 1m 43s (remain 1m 51s) Loss: 0.0096(0.0931) Grad: 71630.5078  LR: 0.00000197  \n",
            "Epoch: [3][380/750] Elapsed 1m 49s (remain 1m 45s) Loss: 0.1323(0.0937) Grad: 351811.7188  LR: 0.00000197  \n",
            "Epoch: [3][400/750] Elapsed 1m 54s (remain 1m 39s) Loss: 0.1236(0.0932) Grad: 330144.0312  LR: 0.00000197  \n",
            "Epoch: [3][420/750] Elapsed 2m 0s (remain 1m 33s) Loss: 0.0137(0.0929) Grad: 95008.4609  LR: 0.00000197  \n",
            "Epoch: [3][440/750] Elapsed 2m 5s (remain 1m 28s) Loss: 0.1515(0.0942) Grad: 351381.2500  LR: 0.00000197  \n",
            "Epoch: [3][460/750] Elapsed 2m 11s (remain 1m 22s) Loss: 0.0264(0.0930) Grad: 129619.6719  LR: 0.00000197  \n",
            "Epoch: [3][480/750] Elapsed 2m 16s (remain 1m 16s) Loss: 0.1434(0.0933) Grad: 345556.5312  LR: 0.00000197  \n",
            "Epoch: [3][500/750] Elapsed 2m 22s (remain 1m 10s) Loss: 0.0664(0.0937) Grad: 568016.0000  LR: 0.00000197  \n",
            "Epoch: [3][520/750] Elapsed 2m 27s (remain 1m 4s) Loss: 0.1243(0.0937) Grad: 291030.3750  LR: 0.00000197  \n",
            "Epoch: [3][540/750] Elapsed 2m 32s (remain 0m 59s) Loss: 0.0155(0.0943) Grad: 98918.5156  LR: 0.00000197  \n",
            "Epoch: [3][560/750] Elapsed 2m 38s (remain 0m 53s) Loss: 0.0284(0.0939) Grad: 127905.9453  LR: 0.00000197  \n",
            "Epoch: [3][580/750] Elapsed 2m 44s (remain 0m 47s) Loss: 0.0253(0.0931) Grad: 121239.6953  LR: 0.00000197  \n",
            "Epoch: [3][600/750] Elapsed 2m 49s (remain 0m 42s) Loss: 0.0847(0.0929) Grad: 227906.1250  LR: 0.00000197  \n",
            "Epoch: [3][620/750] Elapsed 2m 55s (remain 0m 36s) Loss: 0.1082(0.0929) Grad: 277103.3125  LR: 0.00000196  \n",
            "Epoch: [3][640/750] Elapsed 3m 0s (remain 0m 30s) Loss: 0.0983(0.0933) Grad: 254832.9844  LR: 0.00000196  \n",
            "Epoch: [3][660/750] Elapsed 3m 6s (remain 0m 25s) Loss: 0.0538(0.0937) Grad: 218045.5469  LR: 0.00000196  \n",
            "Epoch: [3][680/750] Elapsed 3m 11s (remain 0m 19s) Loss: 0.1392(0.0947) Grad: 345293.8125  LR: 0.00000196  \n",
            "Epoch: [3][700/750] Elapsed 3m 17s (remain 0m 13s) Loss: 0.3213(0.0950) Grad: 470266.1562  LR: 0.00000196  \n",
            "Epoch: [3][720/750] Elapsed 3m 22s (remain 0m 8s) Loss: 0.1057(0.0955) Grad: 285295.1250  LR: 0.00000196  \n",
            "Epoch: [3][740/750] Elapsed 3m 28s (remain 0m 2s) Loss: 0.0827(0.0954) Grad: 214889.8594  LR: 0.00000196  \n",
            "Epoch: [3][749/750] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0325(0.0956) Grad: 127557.5469  LR: 0.00000196  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0614(0.0614) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1050(0.1189) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0321(0.1149) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0653(0.1142) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.0813(0.1158) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1099(0.1154) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0758(0.1176) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0956  avg_val_loss: 0.1165  time: 227s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0956  avg_val_loss: 0.1165  time: 227s\n",
            "Epoch 3 - Score: 0.4845  Scores: [0.5145388761737608, 0.48340827745687803, 0.4690414656230287, 0.47847139433682306, 0.4941474856748964, 0.46733711356318514]\n",
            "INFO:__main__:Epoch 3 - Score: 0.4845  Scores: [0.5145388761737608, 0.48340827745687803, 0.4690414656230287, 0.47847139433682306, 0.4941474856748964, 0.46733711356318514]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0677(0.1165) \n",
            "Epoch: [4][0/750] Elapsed 0m 0s (remain 6m 9s) Loss: 0.0284(0.0284) Grad: 311914.2812  LR: 0.00000196  \n",
            "Epoch: [4][20/750] Elapsed 0m 5s (remain 3m 25s) Loss: 0.0790(0.0706) Grad: 253537.5781  LR: 0.00000196  \n",
            "Epoch: [4][40/750] Elapsed 0m 11s (remain 3m 19s) Loss: 0.0679(0.0668) Grad: 186944.9375  LR: 0.00000196  \n",
            "Epoch: [4][60/750] Elapsed 0m 17s (remain 3m 12s) Loss: 0.0250(0.0658) Grad: 129742.4453  LR: 0.00000196  \n",
            "Epoch: [4][80/750] Elapsed 0m 22s (remain 3m 8s) Loss: 0.1086(0.0688) Grad: 345600.5312  LR: 0.00000196  \n",
            "Epoch: [4][100/750] Elapsed 0m 28s (remain 3m 2s) Loss: 0.0364(0.0727) Grad: 153483.2500  LR: 0.00000196  \n",
            "Epoch: [4][120/750] Elapsed 0m 33s (remain 2m 56s) Loss: 0.0182(0.0714) Grad: 113967.5703  LR: 0.00000196  \n",
            "Epoch: [4][140/750] Elapsed 0m 39s (remain 2m 49s) Loss: 0.0264(0.0716) Grad: 125036.5781  LR: 0.00000196  \n",
            "Epoch: [4][160/750] Elapsed 0m 44s (remain 2m 43s) Loss: 0.0376(0.0726) Grad: 167371.4688  LR: 0.00000195  \n",
            "Epoch: [4][180/750] Elapsed 0m 50s (remain 2m 37s) Loss: 0.0501(0.0751) Grad: 179964.9688  LR: 0.00000195  \n",
            "Epoch: [4][200/750] Elapsed 0m 55s (remain 2m 32s) Loss: 0.0717(0.0771) Grad: 227965.1719  LR: 0.00000195  \n",
            "Epoch: [4][220/750] Elapsed 1m 1s (remain 2m 26s) Loss: 0.0613(0.0796) Grad: 204395.1875  LR: 0.00000195  \n",
            "Epoch: [4][240/750] Elapsed 1m 7s (remain 2m 22s) Loss: 0.1020(0.0807) Grad: 255037.3906  LR: 0.00000195  \n",
            "Epoch: [4][260/750] Elapsed 1m 13s (remain 2m 16s) Loss: 0.0373(0.0810) Grad: 144052.2656  LR: 0.00000195  \n",
            "Epoch: [4][280/750] Elapsed 1m 18s (remain 2m 11s) Loss: 0.2007(0.0820) Grad: 429058.4688  LR: 0.00000195  \n",
            "Epoch: [4][300/750] Elapsed 1m 24s (remain 2m 6s) Loss: 0.2047(0.0833) Grad: 383478.2188  LR: 0.00000195  \n",
            "Epoch: [4][320/750] Elapsed 1m 30s (remain 2m 0s) Loss: 0.1063(0.0832) Grad: 266679.5625  LR: 0.00000195  \n",
            "Epoch: [4][340/750] Elapsed 1m 35s (remain 1m 54s) Loss: 0.1496(0.0830) Grad: 296934.8125  LR: 0.00000195  \n",
            "Epoch: [4][360/750] Elapsed 1m 41s (remain 1m 48s) Loss: 0.0408(0.0826) Grad: 170216.4688  LR: 0.00000195  \n",
            "Epoch: [4][380/750] Elapsed 1m 46s (remain 1m 43s) Loss: 0.0363(0.0819) Grad: 144171.6875  LR: 0.00000195  \n",
            "Epoch: [4][400/750] Elapsed 1m 52s (remain 1m 37s) Loss: 0.0803(0.0826) Grad: 250178.7812  LR: 0.00000195  \n",
            "Epoch: [4][420/750] Elapsed 1m 57s (remain 1m 32s) Loss: 0.0971(0.0830) Grad: 212680.7344  LR: 0.00000194  \n",
            "Epoch: [4][440/750] Elapsed 2m 3s (remain 1m 26s) Loss: 0.3104(0.0834) Grad: 422205.4375  LR: 0.00000194  \n",
            "Epoch: [4][460/750] Elapsed 2m 9s (remain 1m 21s) Loss: 0.3029(0.0846) Grad: 468059.5938  LR: 0.00000194  \n",
            "Epoch: [4][480/750] Elapsed 2m 14s (remain 1m 15s) Loss: 0.0502(0.0842) Grad: 177195.2969  LR: 0.00000194  \n",
            "Epoch: [4][500/750] Elapsed 2m 20s (remain 1m 9s) Loss: 0.0403(0.0852) Grad: 171021.4688  LR: 0.00000194  \n",
            "Epoch: [4][520/750] Elapsed 2m 25s (remain 1m 4s) Loss: 0.1021(0.0842) Grad: 331045.4062  LR: 0.00000194  \n",
            "Epoch: [4][540/750] Elapsed 2m 31s (remain 0m 58s) Loss: 0.0258(0.0844) Grad: 137248.3906  LR: 0.00000194  \n",
            "Epoch: [4][560/750] Elapsed 2m 37s (remain 0m 52s) Loss: 0.5328(0.0853) Grad: 579279.9375  LR: 0.00000194  \n",
            "Epoch: [4][580/750] Elapsed 2m 42s (remain 0m 47s) Loss: 0.0728(0.0855) Grad: 207132.2188  LR: 0.00000194  \n",
            "Epoch: [4][600/750] Elapsed 2m 48s (remain 0m 41s) Loss: 0.0870(0.0864) Grad: 230427.3906  LR: 0.00000194  \n",
            "Epoch: [4][620/750] Elapsed 2m 53s (remain 0m 36s) Loss: 0.0310(0.0869) Grad: 130170.2812  LR: 0.00000194  \n",
            "Epoch: [4][640/750] Elapsed 2m 59s (remain 0m 30s) Loss: 0.0183(0.0869) Grad: 106621.5859  LR: 0.00000193  \n",
            "Epoch: [4][660/750] Elapsed 3m 5s (remain 0m 25s) Loss: 0.0895(0.0870) Grad: 116904.9141  LR: 0.00000193  \n",
            "Epoch: [4][680/750] Elapsed 3m 11s (remain 0m 19s) Loss: 0.1548(0.0887) Grad: 177777.4062  LR: 0.00000193  \n",
            "Epoch: [4][700/750] Elapsed 3m 16s (remain 0m 13s) Loss: 0.0528(0.0890) Grad: 206914.1719  LR: 0.00000193  \n",
            "Epoch: [4][720/750] Elapsed 3m 22s (remain 0m 8s) Loss: 0.1332(0.0892) Grad: 176243.5000  LR: 0.00000193  \n",
            "Epoch: [4][740/750] Elapsed 3m 28s (remain 0m 2s) Loss: 0.0932(0.0891) Grad: 114856.9922  LR: 0.00000193  \n",
            "Epoch: [4][749/750] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0659(0.0893) Grad: 119423.0703  LR: 0.00000193  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0595(0.0595) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0816(0.1183) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0379(0.1174) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0989(0.1172) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1169(0.1189) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1016(0.1173) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0795(0.1196) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0893  avg_val_loss: 0.1187  time: 227s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0893  avg_val_loss: 0.1187  time: 227s\n",
            "Epoch 4 - Score: 0.4894  Scores: [0.5211263130209435, 0.49627608450169625, 0.4322608255041249, 0.5241217778015762, 0.4946244019826858, 0.46807324052124055]\n",
            "INFO:__main__:Epoch 4 - Score: 0.4894  Scores: [0.5211263130209435, 0.49627608450169625, 0.4322608255041249, 0.5241217778015762, 0.4946244019826858, 0.46807324052124055]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0838(0.1187) \n",
            "Epoch: [5][0/750] Elapsed 0m 0s (remain 5m 45s) Loss: 0.1034(0.1034) Grad: inf  LR: 0.00000193  \n",
            "Epoch: [5][20/750] Elapsed 0m 5s (remain 3m 26s) Loss: 0.0863(0.0703) Grad: 249097.4688  LR: 0.00000193  \n",
            "Epoch: [5][40/750] Elapsed 0m 11s (remain 3m 19s) Loss: 0.0151(0.0750) Grad: 89696.0703  LR: 0.00000193  \n",
            "Epoch: [5][60/750] Elapsed 0m 16s (remain 3m 11s) Loss: 0.0244(0.0729) Grad: 160769.7500  LR: 0.00000193  \n",
            "Epoch: [5][80/750] Elapsed 0m 23s (remain 3m 14s) Loss: 0.0942(0.0787) Grad: 542060.5625  LR: 0.00000193  \n",
            "Epoch: [5][100/750] Elapsed 0m 28s (remain 3m 6s) Loss: 0.2108(0.0820) Grad: 416533.6250  LR: 0.00000193  \n",
            "Epoch: [5][120/750] Elapsed 0m 34s (remain 2m 59s) Loss: 0.2816(0.0807) Grad: 562795.8750  LR: 0.00000192  \n",
            "Epoch: [5][140/750] Elapsed 0m 41s (remain 2m 57s) Loss: 0.0487(0.0822) Grad: 204493.3125  LR: 0.00000192  \n",
            "Epoch: [5][160/750] Elapsed 0m 46s (remain 2m 51s) Loss: 0.2706(0.0809) Grad: 490479.3750  LR: 0.00000192  \n",
            "Epoch: [5][180/750] Elapsed 0m 52s (remain 2m 44s) Loss: 0.0714(0.0788) Grad: 250192.4844  LR: 0.00000192  \n",
            "Epoch: [5][200/750] Elapsed 0m 57s (remain 2m 38s) Loss: 0.0478(0.0771) Grad: 175401.2656  LR: 0.00000192  \n",
            "Epoch: [5][220/750] Elapsed 1m 3s (remain 2m 31s) Loss: 0.0362(0.0769) Grad: 125996.3438  LR: 0.00000192  \n",
            "Epoch: [5][240/750] Elapsed 1m 9s (remain 2m 26s) Loss: 0.0366(0.0787) Grad: 153903.1875  LR: 0.00000192  \n",
            "Epoch: [5][260/750] Elapsed 1m 14s (remain 2m 19s) Loss: 0.0468(0.0789) Grad: 185208.5000  LR: 0.00000192  \n",
            "Epoch: [5][280/750] Elapsed 1m 20s (remain 2m 13s) Loss: 0.0314(0.0779) Grad: 123066.0000  LR: 0.00000192  \n",
            "Epoch: [5][300/750] Elapsed 1m 25s (remain 2m 7s) Loss: 0.0918(0.0775) Grad: 208582.3281  LR: 0.00000192  \n",
            "Epoch: [5][320/750] Elapsed 1m 31s (remain 2m 2s) Loss: 0.0338(0.0766) Grad: 149999.9844  LR: 0.00000191  \n",
            "Epoch: [5][340/750] Elapsed 1m 37s (remain 1m 56s) Loss: 0.0388(0.0759) Grad: 160111.5156  LR: 0.00000191  \n",
            "Epoch: [5][360/750] Elapsed 1m 42s (remain 1m 50s) Loss: 0.0366(0.0748) Grad: 165341.3438  LR: 0.00000191  \n",
            "Epoch: [5][380/750] Elapsed 1m 48s (remain 1m 45s) Loss: 0.0922(0.0739) Grad: 271580.0938  LR: 0.00000191  \n",
            "Epoch: [5][400/750] Elapsed 1m 54s (remain 1m 39s) Loss: 0.1308(0.0744) Grad: 350034.1562  LR: 0.00000191  \n",
            "Epoch: [5][420/750] Elapsed 1m 59s (remain 1m 33s) Loss: 0.0582(0.0736) Grad: 191155.3281  LR: 0.00000191  \n",
            "Epoch: [5][440/750] Elapsed 2m 5s (remain 1m 27s) Loss: 0.0263(0.0748) Grad: 113217.3281  LR: 0.00000191  \n",
            "Epoch: [5][460/750] Elapsed 2m 11s (remain 1m 22s) Loss: 0.0733(0.0750) Grad: 241231.2344  LR: 0.00000191  \n",
            "Epoch: [5][480/750] Elapsed 2m 16s (remain 1m 16s) Loss: 0.0752(0.0760) Grad: 241135.2344  LR: 0.00000191  \n",
            "Epoch: [5][500/750] Elapsed 2m 22s (remain 1m 10s) Loss: 0.0235(0.0765) Grad: 110037.8672  LR: 0.00000190  \n",
            "Epoch: [5][520/750] Elapsed 2m 27s (remain 1m 4s) Loss: 0.0219(0.0762) Grad: 135399.7812  LR: 0.00000190  \n",
            "Epoch: [5][540/750] Elapsed 2m 33s (remain 0m 59s) Loss: 0.0256(0.0767) Grad: 166689.1406  LR: 0.00000190  \n",
            "Epoch: [5][560/750] Elapsed 2m 38s (remain 0m 53s) Loss: 0.0428(0.0768) Grad: 171262.0312  LR: 0.00000190  \n",
            "Epoch: [5][580/750] Elapsed 2m 45s (remain 0m 48s) Loss: 0.0248(0.0784) Grad: 146459.4688  LR: 0.00000190  \n",
            "Epoch: [5][600/750] Elapsed 2m 50s (remain 0m 42s) Loss: 0.1531(0.0787) Grad: 347953.0312  LR: 0.00000190  \n",
            "Epoch: [5][620/750] Elapsed 2m 56s (remain 0m 36s) Loss: 0.1138(0.0785) Grad: 302184.8750  LR: 0.00000190  \n",
            "Epoch: [5][640/750] Elapsed 3m 1s (remain 0m 30s) Loss: 0.0526(0.0784) Grad: 173627.6250  LR: 0.00000190  \n",
            "Epoch: [5][660/750] Elapsed 3m 7s (remain 0m 25s) Loss: 0.2057(0.0791) Grad: 350134.7500  LR: 0.00000190  \n",
            "Epoch: [5][680/750] Elapsed 3m 12s (remain 0m 19s) Loss: 0.1133(0.0790) Grad: 277250.4062  LR: 0.00000189  \n",
            "Epoch: [5][700/750] Elapsed 3m 18s (remain 0m 13s) Loss: 0.0753(0.0787) Grad: 202204.8906  LR: 0.00000189  \n",
            "Epoch: [5][720/750] Elapsed 3m 23s (remain 0m 8s) Loss: 0.0637(0.0789) Grad: 200729.4844  LR: 0.00000189  \n",
            "Epoch: [5][740/750] Elapsed 3m 29s (remain 0m 2s) Loss: 0.0410(0.0788) Grad: 148844.3594  LR: 0.00000189  \n",
            "Epoch: [5][749/750] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0397(0.0789) Grad: 170100.4219  LR: 0.00000189  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0761(0.0761) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0983(0.1190) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0526(0.1127) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0616(0.1125) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1002(0.1148) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0980(0.1143) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0482(0.1163) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0789  avg_val_loss: 0.1150  time: 227s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0789  avg_val_loss: 0.1150  time: 227s\n",
            "Epoch 5 - Score: 0.4809  Scores: [0.49533342755907755, 0.48859671030723406, 0.43278750732394966, 0.49378452405505696, 0.5108711866744036, 0.46382926230809857]\n",
            "INFO:__main__:Epoch 5 - Score: 0.4809  Scores: [0.49533342755907755, 0.48859671030723406, 0.43278750732394966, 0.49378452405505696, 0.5108711866744036, 0.46382926230809857]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0796(0.1150) \n",
            "(40, 6) Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Index(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold'], dtype='object')\n",
            "Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Epoch: [1][0/790] Elapsed 0m 0s (remain 6m 15s) Loss: 0.0000(0.0000) Grad: 777.9039  LR: 0.00000189  \n",
            "Epoch: [1][20/790] Elapsed 0m 5s (remain 3m 37s) Loss: 0.0040(0.0650) Grad: 47626.6172  LR: 0.00000189  \n",
            "Epoch: [1][40/790] Elapsed 0m 11s (remain 3m 28s) Loss: 0.0131(0.0531) Grad: 93124.7734  LR: 0.00000189  \n",
            "Epoch: [1][60/790] Elapsed 0m 16s (remain 3m 21s) Loss: 0.0147(0.0496) Grad: 83264.1172  LR: 0.00000189  \n",
            "Epoch: [1][80/790] Elapsed 0m 22s (remain 3m 18s) Loss: 0.1001(0.0501) Grad: 331677.2812  LR: 0.00000189  \n",
            "Epoch: [1][100/790] Elapsed 0m 28s (remain 3m 12s) Loss: 0.1129(0.0513) Grad: 366817.7500  LR: 0.00000189  \n",
            "Epoch: [1][120/790] Elapsed 0m 34s (remain 3m 10s) Loss: 0.0223(0.0585) Grad: 113643.6250  LR: 0.00000188  \n",
            "Epoch: [1][140/790] Elapsed 0m 39s (remain 3m 3s) Loss: 0.0238(0.0587) Grad: 125238.9453  LR: 0.00000188  \n",
            "Epoch: [1][160/790] Elapsed 0m 45s (remain 2m 58s) Loss: 0.0835(0.0586) Grad: 297904.3438  LR: 0.00000188  \n",
            "Epoch: [1][180/790] Elapsed 0m 51s (remain 2m 52s) Loss: 0.0846(0.0566) Grad: 286988.5000  LR: 0.00000188  \n",
            "Epoch: [1][200/790] Elapsed 0m 56s (remain 2m 46s) Loss: 0.0521(0.0583) Grad: 174267.0625  LR: 0.00000188  \n",
            "Epoch: [1][220/790] Elapsed 1m 2s (remain 2m 40s) Loss: 0.0410(0.0595) Grad: 143728.3125  LR: 0.00000188  \n",
            "Epoch: [1][240/790] Elapsed 1m 7s (remain 2m 34s) Loss: 0.0095(0.0611) Grad: 117891.2266  LR: 0.00000188  \n",
            "Epoch: [1][260/790] Elapsed 1m 13s (remain 2m 28s) Loss: 0.0902(0.0612) Grad: 250418.1406  LR: 0.00000188  \n",
            "Epoch: [1][280/790] Elapsed 1m 19s (remain 2m 23s) Loss: 0.0527(0.0625) Grad: 282739.0312  LR: 0.00000187  \n",
            "Epoch: [1][300/790] Elapsed 1m 24s (remain 2m 17s) Loss: 0.0384(0.0642) Grad: 141837.8906  LR: 0.00000187  \n",
            "Epoch: [1][320/790] Elapsed 1m 30s (remain 2m 12s) Loss: 0.0279(0.0645) Grad: 120207.0625  LR: 0.00000187  \n",
            "Epoch: [1][340/790] Elapsed 1m 36s (remain 2m 6s) Loss: 0.0119(0.0638) Grad: 86936.8828  LR: 0.00000187  \n",
            "Epoch: [1][360/790] Elapsed 1m 41s (remain 2m 1s) Loss: 0.1632(0.0637) Grad: 406061.0000  LR: 0.00000187  \n",
            "Epoch: [1][380/790] Elapsed 1m 47s (remain 1m 55s) Loss: 0.0447(0.0649) Grad: 216870.0625  LR: 0.00000187  \n",
            "Epoch: [1][400/790] Elapsed 1m 53s (remain 1m 49s) Loss: 0.0406(0.0656) Grad: 160263.5156  LR: 0.00000187  \n",
            "Epoch: [1][420/790] Elapsed 1m 58s (remain 1m 43s) Loss: 0.1982(0.0660) Grad: 452249.0312  LR: 0.00000187  \n",
            "Epoch: [1][440/790] Elapsed 2m 3s (remain 1m 38s) Loss: 0.0456(0.0656) Grad: 153916.4375  LR: 0.00000186  \n",
            "Epoch: [1][460/790] Elapsed 2m 9s (remain 1m 32s) Loss: 0.1263(0.0651) Grad: 331855.8125  LR: 0.00000186  \n",
            "Epoch: [1][480/790] Elapsed 2m 15s (remain 1m 27s) Loss: 0.1079(0.0651) Grad: 362666.9688  LR: 0.00000186  \n",
            "Epoch: [1][500/790] Elapsed 2m 21s (remain 1m 21s) Loss: 0.0601(0.0654) Grad: 222808.0781  LR: 0.00000186  \n",
            "Epoch: [1][520/790] Elapsed 2m 26s (remain 1m 15s) Loss: 0.1645(0.0656) Grad: 245472.6094  LR: 0.00000186  \n",
            "Epoch: [1][540/790] Elapsed 2m 32s (remain 1m 10s) Loss: 0.0468(0.0649) Grad: 195262.7812  LR: 0.00000186  \n",
            "Epoch: [1][560/790] Elapsed 2m 37s (remain 1m 4s) Loss: 0.0235(0.0647) Grad: 121062.6172  LR: 0.00000186  \n",
            "Epoch: [1][580/790] Elapsed 2m 43s (remain 0m 58s) Loss: 0.1180(0.0640) Grad: 303322.8438  LR: 0.00000186  \n",
            "Epoch: [1][600/790] Elapsed 2m 48s (remain 0m 53s) Loss: 0.1020(0.0635) Grad: 287015.3438  LR: 0.00000185  \n",
            "Epoch: [1][620/790] Elapsed 2m 54s (remain 0m 47s) Loss: 0.0065(0.0634) Grad: 56912.0664  LR: 0.00000185  \n",
            "Epoch: [1][640/790] Elapsed 2m 59s (remain 0m 41s) Loss: 0.0510(0.0637) Grad: 208063.6094  LR: 0.00000185  \n",
            "Epoch: [1][660/790] Elapsed 3m 5s (remain 0m 36s) Loss: 0.0363(0.0642) Grad: 147207.4062  LR: 0.00000185  \n",
            "Epoch: [1][680/790] Elapsed 3m 10s (remain 0m 30s) Loss: 0.0283(0.0641) Grad: 148676.0156  LR: 0.00000185  \n",
            "Epoch: [1][700/790] Elapsed 3m 16s (remain 0m 24s) Loss: 0.0648(0.0642) Grad: 224352.3750  LR: 0.00000185  \n",
            "Epoch: [1][720/790] Elapsed 3m 22s (remain 0m 19s) Loss: 0.0023(0.0640) Grad: 39452.2461  LR: 0.00000185  \n",
            "Epoch: [1][740/790] Elapsed 3m 28s (remain 0m 13s) Loss: 0.0705(0.0641) Grad: 268160.3438  LR: 0.00000184  \n",
            "Epoch: [1][760/790] Elapsed 3m 34s (remain 0m 8s) Loss: 0.1323(0.0645) Grad: 328527.3125  LR: 0.00000184  \n",
            "Epoch: [1][780/790] Elapsed 3m 39s (remain 0m 2s) Loss: 0.0463(0.0643) Grad: 173989.1719  LR: 0.00000184  \n",
            "Epoch: [1][789/790] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0441(0.0643) Grad: 185446.1250  LR: 0.00000184  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0721(0.0721) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0655(0.1092) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0661(0.1075) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0553(0.1100) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1192(0.1140) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0577(0.1131) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0591(0.1163) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0643  avg_val_loss: 0.1155  time: 238s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0643  avg_val_loss: 0.1155  time: 238s\n",
            "Epoch 1 - Score: 0.4824  Scores: [0.5055299379672322, 0.4890132423002857, 0.42954788602164845, 0.4955284299631898, 0.49359445040153926, 0.48127658247419625]\n",
            "INFO:__main__:Epoch 1 - Score: 0.4824  Scores: [0.5055299379672322, 0.4890132423002857, 0.42954788602164845, 0.4955284299631898, 0.49359445040153926, 0.48127658247419625]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0974(0.1155) \n",
            "Epoch: [2][0/790] Elapsed 0m 0s (remain 6m 25s) Loss: 0.0333(0.0333) Grad: 444096.3750  LR: 0.00000184  \n",
            "Epoch: [2][20/790] Elapsed 0m 6s (remain 3m 54s) Loss: 0.0252(0.0460) Grad: 275286.2812  LR: 0.00000184  \n",
            "Epoch: [2][40/790] Elapsed 0m 11s (remain 3m 35s) Loss: 0.0300(0.0507) Grad: 144618.4219  LR: 0.00000184  \n",
            "Epoch: [2][60/790] Elapsed 0m 17s (remain 3m 26s) Loss: 0.0078(0.0526) Grad: 89117.1797  LR: 0.00000184  \n",
            "Epoch: [2][80/790] Elapsed 0m 22s (remain 3m 19s) Loss: 0.0612(0.0515) Grad: 260215.7031  LR: 0.00000184  \n",
            "Epoch: [2][100/790] Elapsed 0m 28s (remain 3m 12s) Loss: 0.0465(0.0509) Grad: 187924.7656  LR: 0.00000183  \n",
            "Epoch: [2][120/790] Elapsed 0m 34s (remain 3m 10s) Loss: 0.0796(0.0515) Grad: 210047.9375  LR: 0.00000183  \n",
            "Epoch: [2][140/790] Elapsed 0m 40s (remain 3m 4s) Loss: 0.0523(0.0499) Grad: 205354.4531  LR: 0.00000183  \n",
            "Epoch: [2][160/790] Elapsed 0m 45s (remain 2m 58s) Loss: 0.2782(0.0508) Grad: 608330.8750  LR: 0.00000183  \n",
            "Epoch: [2][180/790] Elapsed 0m 51s (remain 2m 53s) Loss: 0.0208(0.0526) Grad: 172915.8125  LR: 0.00000183  \n",
            "Epoch: [2][200/790] Elapsed 0m 57s (remain 2m 47s) Loss: 0.0184(0.0552) Grad: 129346.5312  LR: 0.00000183  \n",
            "Epoch: [2][220/790] Elapsed 1m 2s (remain 2m 41s) Loss: 0.0311(0.0540) Grad: 175572.6562  LR: 0.00000183  \n",
            "Epoch: [2][240/790] Elapsed 1m 8s (remain 2m 35s) Loss: 0.1480(0.0534) Grad: 255919.2812  LR: 0.00000182  \n",
            "Epoch: [2][260/790] Elapsed 1m 13s (remain 2m 29s) Loss: 0.0193(0.0529) Grad: 55682.6406  LR: 0.00000182  \n",
            "Epoch: [2][280/790] Elapsed 1m 19s (remain 2m 23s) Loss: 0.0775(0.0537) Grad: 93281.3750  LR: 0.00000182  \n",
            "Epoch: [2][300/790] Elapsed 1m 24s (remain 2m 17s) Loss: 0.0317(0.0533) Grad: 99683.7969  LR: 0.00000182  \n",
            "Epoch: [2][320/790] Elapsed 1m 31s (remain 2m 13s) Loss: 0.0061(0.0548) Grad: 32241.5781  LR: 0.00000182  \n",
            "Epoch: [2][340/790] Elapsed 1m 36s (remain 2m 7s) Loss: 0.0558(0.0545) Grad: 97736.7656  LR: 0.00000182  \n",
            "Epoch: [2][360/790] Elapsed 1m 42s (remain 2m 1s) Loss: 0.1500(0.0548) Grad: 155839.0469  LR: 0.00000182  \n",
            "Epoch: [2][380/790] Elapsed 1m 47s (remain 1m 55s) Loss: 0.0640(0.0552) Grad: 135299.6094  LR: 0.00000181  \n",
            "Epoch: [2][400/790] Elapsed 1m 53s (remain 1m 49s) Loss: 0.0564(0.0553) Grad: 85921.1562  LR: 0.00000181  \n",
            "Epoch: [2][420/790] Elapsed 1m 59s (remain 1m 45s) Loss: 0.0487(0.0565) Grad: 118588.6641  LR: 0.00000181  \n",
            "Epoch: [2][440/790] Elapsed 2m 5s (remain 1m 39s) Loss: 0.0408(0.0561) Grad: 76151.1797  LR: 0.00000181  \n",
            "Epoch: [2][460/790] Elapsed 2m 11s (remain 1m 33s) Loss: 0.0271(0.0558) Grad: 71665.5781  LR: 0.00000181  \n",
            "Epoch: [2][480/790] Elapsed 2m 16s (remain 1m 27s) Loss: 0.0353(0.0555) Grad: 68400.5625  LR: 0.00000181  \n",
            "Epoch: [2][500/790] Elapsed 2m 22s (remain 1m 21s) Loss: 0.0571(0.0562) Grad: 104216.9141  LR: 0.00000181  \n",
            "Epoch: [2][520/790] Elapsed 2m 27s (remain 1m 16s) Loss: 0.0143(0.0560) Grad: 66289.2969  LR: 0.00000180  \n",
            "Epoch: [2][540/790] Elapsed 2m 32s (remain 1m 10s) Loss: 0.0241(0.0557) Grad: 54486.6211  LR: 0.00000180  \n",
            "Epoch: [2][560/790] Elapsed 2m 38s (remain 1m 4s) Loss: 0.0293(0.0558) Grad: 93504.4453  LR: 0.00000180  \n",
            "Epoch: [2][580/790] Elapsed 2m 44s (remain 0m 59s) Loss: 0.0275(0.0556) Grad: 94212.8281  LR: 0.00000180  \n",
            "Epoch: [2][600/790] Elapsed 2m 49s (remain 0m 53s) Loss: 0.0057(0.0553) Grad: 25465.7559  LR: 0.00000180  \n",
            "Epoch: [2][620/790] Elapsed 2m 55s (remain 0m 47s) Loss: 0.1067(0.0554) Grad: 188342.9844  LR: 0.00000180  \n",
            "Epoch: [2][640/790] Elapsed 3m 0s (remain 0m 42s) Loss: 0.0634(0.0562) Grad: 104948.7500  LR: 0.00000180  \n",
            "Epoch: [2][660/790] Elapsed 3m 6s (remain 0m 36s) Loss: 0.1854(0.0567) Grad: 186063.7656  LR: 0.00000179  \n",
            "Epoch: [2][680/790] Elapsed 3m 11s (remain 0m 30s) Loss: 0.0638(0.0578) Grad: 104685.7188  LR: 0.00000179  \n",
            "Epoch: [2][700/790] Elapsed 3m 17s (remain 0m 25s) Loss: 0.0256(0.0582) Grad: 67623.8984  LR: 0.00000179  \n",
            "Epoch: [2][720/790] Elapsed 3m 23s (remain 0m 19s) Loss: 0.0357(0.0583) Grad: 82235.4609  LR: 0.00000179  \n",
            "Epoch: [2][740/790] Elapsed 3m 28s (remain 0m 13s) Loss: 0.0727(0.0587) Grad: 155364.7031  LR: 0.00000179  \n",
            "Epoch: [2][760/790] Elapsed 3m 34s (remain 0m 8s) Loss: 0.0810(0.0598) Grad: 122344.3828  LR: 0.00000179  \n",
            "Epoch: [2][780/790] Elapsed 3m 39s (remain 0m 2s) Loss: 0.0954(0.0596) Grad: 152412.5156  LR: 0.00000178  \n",
            "Epoch: [2][789/790] Elapsed 3m 42s (remain 0m 0s) Loss: 0.0300(0.0600) Grad: 72177.3516  LR: 0.00000178  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0596(0.0596) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0877(0.1145) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0670(0.1141) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0699(0.1127) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1229(0.1181) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0762(0.1157) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0627(0.1193) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0600  avg_val_loss: 0.1183  time: 238s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0600  avg_val_loss: 0.1183  time: 238s\n",
            "Epoch 2 - Score: 0.4887  Scores: [0.5044489082785619, 0.48967375602516483, 0.4543790238374466, 0.5022487331847074, 0.5048387574023848, 0.4768222589918503]\n",
            "INFO:__main__:Epoch 2 - Score: 0.4887  Scores: [0.5044489082785619, 0.48967375602516483, 0.4543790238374466, 0.5022487331847074, 0.5048387574023848, 0.4768222589918503]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0959(0.1183) \n",
            "Epoch: [3][0/790] Elapsed 0m 0s (remain 6m 43s) Loss: 0.0329(0.0329) Grad: 388958.4375  LR: 0.00000178  \n",
            "Epoch: [3][20/790] Elapsed 0m 5s (remain 3m 39s) Loss: 0.0399(0.0549) Grad: 479692.7500  LR: 0.00000178  \n",
            "Epoch: [3][40/790] Elapsed 0m 11s (remain 3m 29s) Loss: 0.0155(0.0506) Grad: 214984.4062  LR: 0.00000178  \n",
            "Epoch: [3][60/790] Elapsed 0m 16s (remain 3m 23s) Loss: 0.0075(0.0485) Grad: 139322.3125  LR: 0.00000178  \n",
            "Epoch: [3][80/790] Elapsed 0m 22s (remain 3m 17s) Loss: 0.0924(0.0471) Grad: 597614.6875  LR: 0.00000178  \n",
            "Epoch: [3][100/790] Elapsed 0m 28s (remain 3m 11s) Loss: 0.0319(0.0456) Grad: 158819.5625  LR: 0.00000178  \n",
            "Epoch: [3][120/790] Elapsed 0m 33s (remain 3m 4s) Loss: 0.0801(0.0450) Grad: 258804.9219  LR: 0.00000177  \n",
            "Epoch: [3][140/790] Elapsed 0m 39s (remain 3m 2s) Loss: 0.0275(0.0444) Grad: 140393.9062  LR: 0.00000177  \n",
            "Epoch: [3][160/790] Elapsed 0m 45s (remain 2m 56s) Loss: 0.0405(0.0443) Grad: 174233.6562  LR: 0.00000177  \n",
            "Epoch: [3][180/790] Elapsed 0m 50s (remain 2m 50s) Loss: 0.0352(0.0463) Grad: 156219.7344  LR: 0.00000177  \n",
            "Epoch: [3][200/790] Elapsed 0m 56s (remain 2m 44s) Loss: 0.0507(0.0473) Grad: 286152.8750  LR: 0.00000177  \n",
            "Epoch: [3][220/790] Elapsed 1m 1s (remain 2m 39s) Loss: 0.0148(0.0473) Grad: 126588.9062  LR: 0.00000177  \n",
            "Epoch: [3][240/790] Elapsed 1m 7s (remain 2m 33s) Loss: 0.0515(0.0462) Grad: 179394.3281  LR: 0.00000176  \n",
            "Epoch: [3][260/790] Elapsed 1m 13s (remain 2m 27s) Loss: 0.0133(0.0463) Grad: 93790.7500  LR: 0.00000176  \n",
            "Epoch: [3][280/790] Elapsed 1m 18s (remain 2m 22s) Loss: 0.1051(0.0459) Grad: 349960.6250  LR: 0.00000176  \n",
            "Epoch: [3][300/790] Elapsed 1m 24s (remain 2m 16s) Loss: 0.0903(0.0451) Grad: 237162.7031  LR: 0.00000176  \n",
            "Epoch: [3][320/790] Elapsed 1m 29s (remain 2m 10s) Loss: 0.0695(0.0444) Grad: 320252.5625  LR: 0.00000176  \n",
            "Epoch: [3][340/790] Elapsed 1m 35s (remain 2m 5s) Loss: 0.0419(0.0435) Grad: 226467.1562  LR: 0.00000176  \n",
            "Epoch: [3][360/790] Elapsed 1m 40s (remain 1m 59s) Loss: 0.0386(0.0436) Grad: 183535.2344  LR: 0.00000175  \n",
            "Epoch: [3][380/790] Elapsed 1m 46s (remain 1m 53s) Loss: 0.0187(0.0431) Grad: 106270.6328  LR: 0.00000175  \n",
            "Epoch: [3][400/790] Elapsed 1m 51s (remain 1m 48s) Loss: 0.0651(0.0433) Grad: 194292.7656  LR: 0.00000175  \n",
            "Epoch: [3][420/790] Elapsed 1m 58s (remain 1m 43s) Loss: 0.0211(0.0436) Grad: 105989.0078  LR: 0.00000175  \n",
            "Epoch: [3][440/790] Elapsed 2m 3s (remain 1m 37s) Loss: 0.0211(0.0440) Grad: 106612.3984  LR: 0.00000175  \n",
            "Epoch: [3][460/790] Elapsed 2m 9s (remain 1m 32s) Loss: 0.0489(0.0446) Grad: 253720.2031  LR: 0.00000175  \n",
            "Epoch: [3][480/790] Elapsed 2m 15s (remain 1m 27s) Loss: 0.0471(0.0457) Grad: 247720.3281  LR: 0.00000175  \n",
            "Epoch: [3][500/790] Elapsed 2m 20s (remain 1m 21s) Loss: 0.0374(0.0456) Grad: 175323.5938  LR: 0.00000174  \n",
            "Epoch: [3][520/790] Elapsed 2m 26s (remain 1m 15s) Loss: 0.1126(0.0456) Grad: 287817.4688  LR: 0.00000174  \n",
            "Epoch: [3][540/790] Elapsed 2m 31s (remain 1m 9s) Loss: 0.0761(0.0452) Grad: 209069.4531  LR: 0.00000174  \n",
            "Epoch: [3][560/790] Elapsed 2m 37s (remain 1m 4s) Loss: 0.0245(0.0450) Grad: 142991.5938  LR: 0.00000174  \n",
            "Epoch: [3][580/790] Elapsed 2m 42s (remain 0m 58s) Loss: 0.0236(0.0445) Grad: 122049.6953  LR: 0.00000174  \n",
            "Epoch: [3][600/790] Elapsed 2m 48s (remain 0m 52s) Loss: 0.0999(0.0443) Grad: 233720.9531  LR: 0.00000173  \n",
            "Epoch: [3][620/790] Elapsed 2m 53s (remain 0m 47s) Loss: 0.0352(0.0438) Grad: 173375.6094  LR: 0.00000173  \n",
            "Epoch: [3][640/790] Elapsed 2m 59s (remain 0m 41s) Loss: 0.0401(0.0437) Grad: 173797.2344  LR: 0.00000173  \n",
            "Epoch: [3][660/790] Elapsed 3m 4s (remain 0m 36s) Loss: 0.0305(0.0437) Grad: 177642.7500  LR: 0.00000173  \n",
            "Epoch: [3][680/790] Elapsed 3m 10s (remain 0m 30s) Loss: 0.0270(0.0438) Grad: 138281.2656  LR: 0.00000173  \n",
            "Epoch: [3][700/790] Elapsed 3m 16s (remain 0m 24s) Loss: 0.0740(0.0441) Grad: 214920.1406  LR: 0.00000173  \n",
            "Epoch: [3][720/790] Elapsed 3m 22s (remain 0m 19s) Loss: 0.0396(0.0439) Grad: 85759.3047  LR: 0.00000172  \n",
            "Epoch: [3][740/790] Elapsed 3m 28s (remain 0m 13s) Loss: 0.0278(0.0443) Grad: 69621.5469  LR: 0.00000172  \n",
            "Epoch: [3][760/790] Elapsed 3m 33s (remain 0m 8s) Loss: 0.1339(0.0452) Grad: 243910.5469  LR: 0.00000172  \n",
            "Epoch: [3][780/790] Elapsed 3m 39s (remain 0m 2s) Loss: 0.0734(0.0455) Grad: 114828.2188  LR: 0.00000172  \n",
            "Epoch: [3][789/790] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0087(0.0454) Grad: 32465.5527  LR: 0.00000172  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0755(0.0755) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1094(0.1189) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0561(0.1175) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0806(0.1180) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1454(0.1230) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0780(0.1227) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0756(0.1252) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0454  avg_val_loss: 0.1243  time: 238s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0454  avg_val_loss: 0.1243  time: 238s\n",
            "Epoch 3 - Score: 0.5010  Scores: [0.5306761334044718, 0.5216499142211736, 0.4370716391308852, 0.5129925990498712, 0.5263395871723923, 0.4771789924614918]\n",
            "INFO:__main__:Epoch 3 - Score: 0.5010  Scores: [0.5306761334044718, 0.5216499142211736, 0.4370716391308852, 0.5129925990498712, 0.5263395871723923, 0.4771789924614918]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0710(0.1243) \n",
            "Epoch: [4][0/790] Elapsed 0m 0s (remain 6m 37s) Loss: 0.0987(0.0987) Grad: 429921.4062  LR: 0.00000172  \n",
            "Epoch: [4][20/790] Elapsed 0m 5s (remain 3m 39s) Loss: 0.0440(0.0425) Grad: 388471.5000  LR: 0.00000172  \n",
            "Epoch: [4][40/790] Elapsed 0m 11s (remain 3m 31s) Loss: 0.0448(0.0396) Grad: 161091.8594  LR: 0.00000171  \n",
            "Epoch: [4][60/790] Elapsed 0m 17s (remain 3m 24s) Loss: 0.0139(0.0378) Grad: 97065.8516  LR: 0.00000171  \n",
            "Epoch: [4][80/790] Elapsed 0m 23s (remain 3m 22s) Loss: 0.0433(0.0370) Grad: 206411.9219  LR: 0.00000171  \n",
            "Epoch: [4][100/790] Elapsed 0m 28s (remain 3m 16s) Loss: 0.0426(0.0346) Grad: 184975.8750  LR: 0.00000171  \n",
            "Epoch: [4][120/790] Elapsed 0m 34s (remain 3m 9s) Loss: 0.0257(0.0341) Grad: 162448.2031  LR: 0.00000171  \n",
            "Epoch: [4][140/790] Elapsed 0m 39s (remain 3m 2s) Loss: 0.0099(0.0335) Grad: 75112.6484  LR: 0.00000171  \n",
            "Epoch: [4][160/790] Elapsed 0m 45s (remain 2m 56s) Loss: 0.0719(0.0329) Grad: 262287.3750  LR: 0.00000170  \n",
            "Epoch: [4][180/790] Elapsed 0m 50s (remain 2m 50s) Loss: 0.0375(0.0343) Grad: 175872.0000  LR: 0.00000170  \n",
            "Epoch: [4][200/790] Elapsed 0m 56s (remain 2m 44s) Loss: 0.0241(0.0348) Grad: 169879.1094  LR: 0.00000170  \n",
            "Epoch: [4][220/790] Elapsed 1m 1s (remain 2m 38s) Loss: 0.0395(0.0344) Grad: 200009.7188  LR: 0.00000170  \n",
            "Epoch: [4][240/790] Elapsed 1m 7s (remain 2m 32s) Loss: 0.0268(0.0341) Grad: 141564.8281  LR: 0.00000170  \n",
            "Epoch: [4][260/790] Elapsed 1m 12s (remain 2m 27s) Loss: 0.0073(0.0341) Grad: 68930.0469  LR: 0.00000170  \n",
            "Epoch: [4][280/790] Elapsed 1m 18s (remain 2m 21s) Loss: 0.0020(0.0347) Grad: 33754.4648  LR: 0.00000169  \n",
            "Epoch: [4][300/790] Elapsed 1m 23s (remain 2m 15s) Loss: 0.0070(0.0349) Grad: 68406.6875  LR: 0.00000169  \n",
            "Epoch: [4][320/790] Elapsed 1m 29s (remain 2m 10s) Loss: 0.0821(0.0354) Grad: 280221.4062  LR: 0.00000169  \n",
            "Epoch: [4][340/790] Elapsed 1m 34s (remain 2m 4s) Loss: 0.0190(0.0360) Grad: 113793.4453  LR: 0.00000169  \n",
            "Epoch: [4][360/790] Elapsed 1m 40s (remain 1m 59s) Loss: 0.0062(0.0357) Grad: 64588.1758  LR: 0.00000169  \n",
            "Epoch: [4][380/790] Elapsed 1m 46s (remain 1m 54s) Loss: 0.0159(0.0354) Grad: 125283.4297  LR: 0.00000168  \n",
            "Epoch: [4][400/790] Elapsed 1m 51s (remain 1m 48s) Loss: 0.0171(0.0350) Grad: 101705.0703  LR: 0.00000168  \n",
            "Epoch: [4][420/790] Elapsed 1m 57s (remain 1m 42s) Loss: 0.0339(0.0347) Grad: 171316.9531  LR: 0.00000168  \n",
            "Epoch: [4][440/790] Elapsed 2m 2s (remain 1m 37s) Loss: 0.0136(0.0345) Grad: 104640.3516  LR: 0.00000168  \n",
            "Epoch: [4][460/790] Elapsed 2m 8s (remain 1m 31s) Loss: 0.0625(0.0345) Grad: 234213.0781  LR: 0.00000168  \n",
            "Epoch: [4][480/790] Elapsed 2m 15s (remain 1m 26s) Loss: 0.0376(0.0346) Grad: 225615.9375  LR: 0.00000168  \n",
            "Epoch: [4][500/790] Elapsed 2m 20s (remain 1m 21s) Loss: 0.0157(0.0344) Grad: 112172.6875  LR: 0.00000167  \n",
            "Epoch: [4][520/790] Elapsed 2m 26s (remain 1m 15s) Loss: 0.0048(0.0345) Grad: 61662.2305  LR: 0.00000167  \n",
            "Epoch: [4][540/790] Elapsed 2m 31s (remain 1m 9s) Loss: 0.0052(0.0342) Grad: 72020.8438  LR: 0.00000167  \n",
            "Epoch: [4][560/790] Elapsed 2m 37s (remain 1m 4s) Loss: 0.0037(0.0343) Grad: 65222.9531  LR: 0.00000167  \n",
            "Epoch: [4][580/790] Elapsed 2m 42s (remain 0m 58s) Loss: 0.0622(0.0345) Grad: 237222.7969  LR: 0.00000167  \n",
            "Epoch: [4][600/790] Elapsed 2m 48s (remain 0m 52s) Loss: 0.0301(0.0343) Grad: 160618.4062  LR: 0.00000166  \n",
            "Epoch: [4][620/790] Elapsed 2m 53s (remain 0m 47s) Loss: 0.0275(0.0344) Grad: 169614.4844  LR: 0.00000166  \n",
            "Epoch: [4][640/790] Elapsed 2m 59s (remain 0m 41s) Loss: 0.0079(0.0345) Grad: 68084.4453  LR: 0.00000166  \n",
            "Epoch: [4][660/790] Elapsed 3m 5s (remain 0m 36s) Loss: 0.0185(0.0347) Grad: 131585.9688  LR: 0.00000166  \n",
            "Epoch: [4][680/790] Elapsed 3m 10s (remain 0m 30s) Loss: 0.0107(0.0345) Grad: 85800.9844  LR: 0.00000166  \n",
            "Epoch: [4][700/790] Elapsed 3m 16s (remain 0m 24s) Loss: 0.0164(0.0349) Grad: 122127.0156  LR: 0.00000165  \n",
            "Epoch: [4][720/790] Elapsed 3m 22s (remain 0m 19s) Loss: 0.0440(0.0346) Grad: 222182.7812  LR: 0.00000165  \n",
            "Epoch: [4][740/790] Elapsed 3m 27s (remain 0m 13s) Loss: 0.0415(0.0345) Grad: 203015.2344  LR: 0.00000165  \n",
            "Epoch: [4][760/790] Elapsed 3m 33s (remain 0m 8s) Loss: 0.0230(0.0346) Grad: 127450.3750  LR: 0.00000165  \n",
            "Epoch: [4][780/790] Elapsed 3m 39s (remain 0m 2s) Loss: 0.0514(0.0346) Grad: 239954.3594  LR: 0.00000165  \n",
            "Epoch: [4][789/790] Elapsed 3m 41s (remain 0m 0s) Loss: 0.0119(0.0346) Grad: 106365.7344  LR: 0.00000165  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0764(0.0764) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1065(0.1176) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0606(0.1179) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0616(0.1162) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1088(0.1200) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0792(0.1189) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0843(0.1214) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0346  avg_val_loss: 0.1211  time: 238s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0346  avg_val_loss: 0.1211  time: 238s\n",
            "Epoch 4 - Score: 0.4941  Scores: [0.5294398950830769, 0.49329629383425994, 0.43446605874976596, 0.5040222796128071, 0.5119832567323038, 0.49159624139041413]\n",
            "INFO:__main__:Epoch 4 - Score: 0.4941  Scores: [0.5294398950830769, 0.49329629383425994, 0.43446605874976596, 0.5040222796128071, 0.5119832567323038, 0.49159624139041413]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0907(0.1211) \n",
            "Epoch: [5][0/790] Elapsed 0m 0s (remain 6m 26s) Loss: 0.0028(0.0028) Grad: 124702.6016  LR: 0.00000165  \n",
            "Epoch: [5][20/790] Elapsed 0m 6s (remain 4m 1s) Loss: 0.0191(0.0225) Grad: 343755.5625  LR: 0.00000164  \n",
            "Epoch: [5][40/790] Elapsed 0m 12s (remain 3m 41s) Loss: 0.0187(0.0211) Grad: 292866.7500  LR: 0.00000164  \n",
            "Epoch: [5][60/790] Elapsed 0m 17s (remain 3m 30s) Loss: 0.0255(0.0230) Grad: 367315.5000  LR: 0.00000164  \n",
            "Epoch: [5][80/790] Elapsed 0m 23s (remain 3m 21s) Loss: 0.0142(0.0240) Grad: 216306.5625  LR: 0.00000164  \n",
            "Epoch: [5][100/790] Elapsed 0m 28s (remain 3m 15s) Loss: 0.0981(0.0243) Grad: 554746.6250  LR: 0.00000164  \n",
            "Epoch: [5][120/790] Elapsed 0m 34s (remain 3m 8s) Loss: 0.0346(0.0239) Grad: 258660.0156  LR: 0.00000163  \n",
            "Epoch: [5][140/790] Elapsed 0m 39s (remain 3m 2s) Loss: 0.0285(0.0245) Grad: 413021.0000  LR: 0.00000163  \n",
            "Epoch: [5][160/790] Elapsed 0m 45s (remain 2m 56s) Loss: 0.0124(0.0272) Grad: 228435.3750  LR: 0.00000163  \n",
            "Epoch: [5][180/790] Elapsed 0m 51s (remain 2m 51s) Loss: 0.0132(0.0259) Grad: 259451.3438  LR: 0.00000163  \n",
            "Epoch: [5][200/790] Elapsed 0m 56s (remain 2m 45s) Loss: 0.0093(0.0260) Grad: 206351.4375  LR: 0.00000163  \n",
            "Epoch: [5][220/790] Elapsed 1m 2s (remain 2m 39s) Loss: 0.0710(0.0261) Grad: 449549.3125  LR: 0.00000162  \n",
            "Epoch: [5][240/790] Elapsed 1m 7s (remain 2m 33s) Loss: 0.0092(0.0262) Grad: 254212.6406  LR: 0.00000162  \n",
            "Epoch: [5][260/790] Elapsed 1m 13s (remain 2m 28s) Loss: 0.0239(0.0256) Grad: 285265.8125  LR: 0.00000162  \n",
            "Epoch: [5][280/790] Elapsed 1m 19s (remain 2m 23s) Loss: 0.0154(0.0256) Grad: 184934.4062  LR: 0.00000162  \n",
            "Epoch: [5][300/790] Elapsed 1m 24s (remain 2m 17s) Loss: 0.0031(0.0252) Grad: 100325.8125  LR: 0.00000162  \n",
            "Epoch: [5][320/790] Elapsed 1m 30s (remain 2m 12s) Loss: 0.0187(0.0253) Grad: 274140.2188  LR: 0.00000161  \n",
            "Epoch: [5][340/790] Elapsed 1m 36s (remain 2m 6s) Loss: 0.0445(0.0252) Grad: 419081.3125  LR: 0.00000161  \n",
            "Epoch: [5][360/790] Elapsed 1m 41s (remain 2m 0s) Loss: 0.0523(0.0248) Grad: 602306.1875  LR: 0.00000161  \n",
            "Epoch: [5][380/790] Elapsed 1m 47s (remain 1m 55s) Loss: 0.0242(0.0245) Grad: 335861.0625  LR: 0.00000161  \n",
            "Epoch: [5][400/790] Elapsed 1m 52s (remain 1m 49s) Loss: 0.0565(0.0249) Grad: 441785.0312  LR: 0.00000161  \n",
            "Epoch: [5][420/790] Elapsed 1m 58s (remain 1m 43s) Loss: 0.1447(0.0251) Grad: 549279.1250  LR: 0.00000160  \n",
            "Epoch: [5][440/790] Elapsed 2m 3s (remain 1m 37s) Loss: 0.0290(0.0250) Grad: 208667.2344  LR: 0.00000160  \n",
            "Epoch: [5][460/790] Elapsed 2m 9s (remain 1m 32s) Loss: 0.1506(0.0254) Grad: 484325.6250  LR: 0.00000160  \n",
            "Epoch: [5][480/790] Elapsed 2m 14s (remain 1m 26s) Loss: 0.0309(0.0253) Grad: 179584.9219  LR: 0.00000160  \n",
            "Epoch: [5][500/790] Elapsed 2m 20s (remain 1m 20s) Loss: 0.0332(0.0254) Grad: 150144.8906  LR: 0.00000160  \n",
            "Epoch: [5][520/790] Elapsed 2m 25s (remain 1m 15s) Loss: 0.0239(0.0254) Grad: 229166.5312  LR: 0.00000159  \n",
            "Epoch: [5][540/790] Elapsed 2m 31s (remain 1m 9s) Loss: 0.0355(0.0252) Grad: 293399.5625  LR: 0.00000159  \n",
            "Epoch: [5][560/790] Elapsed 2m 37s (remain 1m 4s) Loss: 0.0125(0.0257) Grad: 130343.1719  LR: 0.00000159  \n",
            "Epoch: [5][580/790] Elapsed 2m 43s (remain 0m 58s) Loss: 0.0599(0.0257) Grad: 188381.4062  LR: 0.00000159  \n",
            "Epoch: [5][600/790] Elapsed 2m 48s (remain 0m 53s) Loss: 0.0089(0.0255) Grad: 118094.2812  LR: 0.00000159  \n",
            "Epoch: [5][620/790] Elapsed 2m 54s (remain 0m 47s) Loss: 0.0203(0.0253) Grad: 175693.5625  LR: 0.00000158  \n",
            "Epoch: [5][640/790] Elapsed 2m 59s (remain 0m 41s) Loss: 0.0159(0.0251) Grad: 107988.9375  LR: 0.00000158  \n",
            "Epoch: [5][660/790] Elapsed 3m 6s (remain 0m 36s) Loss: 0.0124(0.0252) Grad: 121302.3125  LR: 0.00000158  \n",
            "Epoch: [5][680/790] Elapsed 3m 11s (remain 0m 30s) Loss: 0.0906(0.0252) Grad: 247800.1406  LR: 0.00000158  \n",
            "Epoch: [5][700/790] Elapsed 3m 17s (remain 0m 25s) Loss: 0.0194(0.0255) Grad: 102747.8672  LR: 0.00000158  \n",
            "Epoch: [5][720/790] Elapsed 3m 23s (remain 0m 19s) Loss: 0.0016(0.0260) Grad: 34719.4805  LR: 0.00000157  \n",
            "Epoch: [5][740/790] Elapsed 3m 28s (remain 0m 13s) Loss: 0.0130(0.0259) Grad: 96650.3281  LR: 0.00000157  \n",
            "Epoch: [5][760/790] Elapsed 3m 34s (remain 0m 8s) Loss: 0.0103(0.0258) Grad: 93830.3906  LR: 0.00000157  \n",
            "Epoch: [5][780/790] Elapsed 3m 39s (remain 0m 2s) Loss: 0.0101(0.0258) Grad: 114602.2734  LR: 0.00000157  \n",
            "Epoch: [5][789/790] Elapsed 3m 42s (remain 0m 0s) Loss: 0.0217(0.0260) Grad: 192373.1875  LR: 0.00000157  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0763(0.0763) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0795(0.1181) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0438(0.1136) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0539(0.1127) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1100(0.1188) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0819(0.1188) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0515(0.1213) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0260  avg_val_loss: 0.1207  time: 238s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0260  avg_val_loss: 0.1207  time: 238s\n",
            "Epoch 5 - Score: 0.4946  Scores: [0.5247316175828755, 0.5088788268005908, 0.443385208370467, 0.5116406533589808, 0.5103610904327999, 0.46837681507373363]\n",
            "INFO:__main__:Epoch 5 - Score: 0.4946  Scores: [0.5247316175828755, 0.5088788268005908, 0.443385208370467, 0.5116406533589808, 0.5103610904327999, 0.46837681507373363]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0842(0.1207) \n",
            "(40, 6) Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Index(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold'], dtype='object')\n",
            "Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Epoch: [1][0/830] Elapsed 0m 0s (remain 6m 49s) Loss: 0.0147(0.0147) Grad: 241334.1875  LR: 0.00000157  \n",
            "Epoch: [1][20/830] Elapsed 0m 6s (remain 3m 51s) Loss: 0.0053(0.0137) Grad: 128185.0391  LR: 0.00000157  \n",
            "Epoch: [1][40/830] Elapsed 0m 11s (remain 3m 41s) Loss: 0.0395(0.0159) Grad: 364608.5312  LR: 0.00000156  \n",
            "Epoch: [1][60/830] Elapsed 0m 16s (remain 3m 33s) Loss: 0.0102(0.0172) Grad: 178865.3125  LR: 0.00000156  \n",
            "Epoch: [1][80/830] Elapsed 0m 22s (remain 3m 28s) Loss: 0.0171(0.0171) Grad: 355670.4688  LR: 0.00000156  \n",
            "Epoch: [1][100/830] Elapsed 0m 28s (remain 3m 28s) Loss: 0.0455(0.0181) Grad: 350496.9688  LR: 0.00000156  \n",
            "Epoch: [1][120/830] Elapsed 0m 34s (remain 3m 21s) Loss: 0.0239(0.0181) Grad: 306107.7188  LR: 0.00000155  \n",
            "Epoch: [1][140/830] Elapsed 0m 39s (remain 3m 14s) Loss: 0.0159(0.0172) Grad: 317681.8438  LR: 0.00000155  \n",
            "Epoch: [1][160/830] Elapsed 0m 46s (remain 3m 12s) Loss: 0.0065(0.0181) Grad: 184061.6562  LR: 0.00000155  \n",
            "Epoch: [1][180/830] Elapsed 0m 51s (remain 3m 5s) Loss: 0.0066(0.0178) Grad: 179866.7500  LR: 0.00000155  \n",
            "Epoch: [1][200/830] Elapsed 0m 57s (remain 2m 58s) Loss: 0.0031(0.0177) Grad: 107687.2266  LR: 0.00000155  \n",
            "Epoch: [1][220/830] Elapsed 1m 2s (remain 2m 52s) Loss: 0.0091(0.0177) Grad: 179190.6406  LR: 0.00000154  \n",
            "Epoch: [1][240/830] Elapsed 1m 8s (remain 2m 46s) Loss: 0.0235(0.0175) Grad: 302140.8438  LR: 0.00000154  \n",
            "Epoch: [1][260/830] Elapsed 1m 14s (remain 2m 42s) Loss: 0.0040(0.0177) Grad: 126937.8594  LR: 0.00000154  \n",
            "Epoch: [1][280/830] Elapsed 1m 20s (remain 2m 37s) Loss: 0.0381(0.0195) Grad: 348899.4688  LR: 0.00000154  \n",
            "Epoch: [1][300/830] Elapsed 1m 26s (remain 2m 31s) Loss: 0.0271(0.0196) Grad: 305224.3125  LR: 0.00000154  \n",
            "Epoch: [1][320/830] Elapsed 1m 31s (remain 2m 25s) Loss: 0.0325(0.0190) Grad: 290607.3750  LR: 0.00000153  \n",
            "Epoch: [1][340/830] Elapsed 1m 37s (remain 2m 19s) Loss: 0.0113(0.0189) Grad: 161037.5469  LR: 0.00000153  \n",
            "Epoch: [1][360/830] Elapsed 1m 42s (remain 2m 13s) Loss: 0.0216(0.0187) Grad: 318048.1250  LR: 0.00000153  \n",
            "Epoch: [1][380/830] Elapsed 1m 48s (remain 2m 7s) Loss: 0.0023(0.0188) Grad: 53687.8438  LR: 0.00000153  \n",
            "Epoch: [1][400/830] Elapsed 1m 53s (remain 2m 1s) Loss: 0.0040(0.0187) Grad: 76866.2422  LR: 0.00000153  \n",
            "Epoch: [1][420/830] Elapsed 1m 59s (remain 1m 56s) Loss: 0.0145(0.0187) Grad: 145453.6875  LR: 0.00000152  \n",
            "Epoch: [1][440/830] Elapsed 2m 5s (remain 1m 50s) Loss: 0.0014(0.0183) Grad: 44690.7695  LR: 0.00000152  \n",
            "Epoch: [1][460/830] Elapsed 2m 10s (remain 1m 44s) Loss: 0.0095(0.0183) Grad: 82422.9062  LR: 0.00000152  \n",
            "Epoch: [1][480/830] Elapsed 2m 16s (remain 1m 39s) Loss: 0.0046(0.0183) Grad: 69144.4062  LR: 0.00000152  \n",
            "Epoch: [1][500/830] Elapsed 2m 22s (remain 1m 33s) Loss: 0.0042(0.0184) Grad: 61367.1016  LR: 0.00000151  \n",
            "Epoch: [1][520/830] Elapsed 2m 27s (remain 1m 27s) Loss: 0.0144(0.0184) Grad: 97748.9297  LR: 0.00000151  \n",
            "Epoch: [1][540/830] Elapsed 2m 33s (remain 1m 21s) Loss: 0.0046(0.0182) Grad: 64722.4727  LR: 0.00000151  \n",
            "Epoch: [1][560/830] Elapsed 2m 38s (remain 1m 16s) Loss: 0.0160(0.0184) Grad: 103423.4844  LR: 0.00000151  \n",
            "Epoch: [1][580/830] Elapsed 2m 44s (remain 1m 10s) Loss: 0.0174(0.0183) Grad: 181067.9219  LR: 0.00000151  \n",
            "Epoch: [1][600/830] Elapsed 2m 49s (remain 1m 4s) Loss: 0.0136(0.0184) Grad: 92082.7188  LR: 0.00000150  \n",
            "Epoch: [1][620/830] Elapsed 2m 55s (remain 0m 58s) Loss: 0.0037(0.0185) Grad: 51065.8555  LR: 0.00000150  \n",
            "Epoch: [1][640/830] Elapsed 3m 0s (remain 0m 53s) Loss: 0.0063(0.0184) Grad: 94673.2578  LR: 0.00000150  \n",
            "Epoch: [1][660/830] Elapsed 3m 6s (remain 0m 47s) Loss: 0.0164(0.0190) Grad: 137474.7812  LR: 0.00000150  \n",
            "Epoch: [1][680/830] Elapsed 3m 11s (remain 0m 42s) Loss: 0.0116(0.0189) Grad: 85776.1484  LR: 0.00000149  \n",
            "Epoch: [1][700/830] Elapsed 3m 17s (remain 0m 36s) Loss: 0.0041(0.0188) Grad: 64032.0508  LR: 0.00000149  \n",
            "Epoch: [1][720/830] Elapsed 3m 22s (remain 0m 30s) Loss: 0.0155(0.0187) Grad: 121374.2188  LR: 0.00000149  \n",
            "Epoch: [1][740/830] Elapsed 3m 28s (remain 0m 25s) Loss: 0.0049(0.0189) Grad: 63624.3320  LR: 0.00000149  \n",
            "Epoch: [1][760/830] Elapsed 3m 33s (remain 0m 19s) Loss: 0.0235(0.0188) Grad: 164124.5312  LR: 0.00000149  \n",
            "Epoch: [1][780/830] Elapsed 3m 39s (remain 0m 13s) Loss: 0.0215(0.0189) Grad: 167945.8750  LR: 0.00000148  \n",
            "Epoch: [1][800/830] Elapsed 3m 44s (remain 0m 8s) Loss: 0.0063(0.0189) Grad: 81648.9453  LR: 0.00000148  \n",
            "Epoch: [1][820/830] Elapsed 3m 50s (remain 0m 2s) Loss: 0.0170(0.0191) Grad: 134900.2969  LR: 0.00000148  \n",
            "Epoch: [1][829/830] Elapsed 3m 52s (remain 0m 0s) Loss: 0.0715(0.0192) Grad: 291609.4375  LR: 0.00000148  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0818(0.0818) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0933(0.1257) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0468(0.1232) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0815(0.1229) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1163(0.1255) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1112(0.1256) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0819(0.1274) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0192  avg_val_loss: 0.1269  time: 249s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0192  avg_val_loss: 0.1269  time: 249s\n",
            "Epoch 1 - Score: 0.5061  Scores: [0.5508749649516262, 0.5347997549318488, 0.44172435832155965, 0.5206330164364796, 0.5141792996420594, 0.47425402594792826]\n",
            "INFO:__main__:Epoch 1 - Score: 0.5061  Scores: [0.5508749649516262, 0.5347997549318488, 0.44172435832155965, 0.5206330164364796, 0.5141792996420594, 0.47425402594792826]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0620(0.1269) \n",
            "Epoch: [2][0/830] Elapsed 0m 0s (remain 7m 1s) Loss: 0.0306(0.0306) Grad: 306974.0625  LR: 0.00000148  \n",
            "Epoch: [2][20/830] Elapsed 0m 5s (remain 3m 50s) Loss: 0.0612(0.0152) Grad: 464692.9688  LR: 0.00000148  \n",
            "Epoch: [2][40/830] Elapsed 0m 11s (remain 3m 40s) Loss: 0.0004(0.0131) Grad: 35287.2891  LR: 0.00000147  \n",
            "Epoch: [2][60/830] Elapsed 0m 17s (remain 3m 35s) Loss: 0.0061(0.0139) Grad: 214888.2188  LR: 0.00000147  \n",
            "Epoch: [2][80/830] Elapsed 0m 22s (remain 3m 28s) Loss: 0.0070(0.0138) Grad: 183790.5625  LR: 0.00000147  \n",
            "Epoch: [2][100/830] Elapsed 0m 27s (remain 3m 21s) Loss: 0.0235(0.0133) Grad: 403675.3438  LR: 0.00000147  \n",
            "Epoch: [2][120/830] Elapsed 0m 33s (remain 3m 17s) Loss: 0.0093(0.0144) Grad: 239001.1094  LR: 0.00000147  \n",
            "Epoch: [2][140/830] Elapsed 0m 39s (remain 3m 11s) Loss: 0.0065(0.0142) Grad: 184510.7344  LR: 0.00000146  \n",
            "Epoch: [2][160/830] Elapsed 0m 44s (remain 3m 6s) Loss: 0.0212(0.0143) Grad: 309594.7188  LR: 0.00000146  \n",
            "Epoch: [2][180/830] Elapsed 0m 50s (remain 3m 0s) Loss: 0.0198(0.0150) Grad: 365668.5625  LR: 0.00000146  \n",
            "Epoch: [2][200/830] Elapsed 0m 55s (remain 2m 54s) Loss: 0.0175(0.0150) Grad: 327894.8125  LR: 0.00000146  \n",
            "Epoch: [2][220/830] Elapsed 1m 1s (remain 2m 49s) Loss: 0.0044(0.0151) Grad: 139483.1562  LR: 0.00000145  \n",
            "Epoch: [2][240/830] Elapsed 1m 6s (remain 2m 43s) Loss: 0.0077(0.0151) Grad: 177751.2969  LR: 0.00000145  \n",
            "Epoch: [2][260/830] Elapsed 1m 13s (remain 2m 39s) Loss: 0.0546(0.0154) Grad: 371673.9688  LR: 0.00000145  \n",
            "Epoch: [2][280/830] Elapsed 1m 18s (remain 2m 33s) Loss: 0.0221(0.0164) Grad: 314672.7188  LR: 0.00000145  \n",
            "Epoch: [2][300/830] Elapsed 1m 25s (remain 2m 29s) Loss: 0.0155(0.0159) Grad: 270524.1875  LR: 0.00000144  \n",
            "Epoch: [2][320/830] Elapsed 1m 30s (remain 2m 23s) Loss: 0.0239(0.0162) Grad: 326590.7188  LR: 0.00000144  \n",
            "Epoch: [2][340/830] Elapsed 1m 35s (remain 2m 17s) Loss: 0.0055(0.0159) Grad: 179559.3438  LR: 0.00000144  \n",
            "Epoch: [2][360/830] Elapsed 1m 41s (remain 2m 11s) Loss: 0.0047(0.0155) Grad: 129412.1328  LR: 0.00000144  \n",
            "Epoch: [2][380/830] Elapsed 1m 47s (remain 2m 6s) Loss: 0.0100(0.0156) Grad: 230466.7500  LR: 0.00000144  \n",
            "Epoch: [2][400/830] Elapsed 1m 52s (remain 2m 0s) Loss: 0.0151(0.0155) Grad: 319244.8750  LR: 0.00000143  \n",
            "Epoch: [2][420/830] Elapsed 1m 58s (remain 1m 54s) Loss: 0.0288(0.0157) Grad: 254832.9688  LR: 0.00000143  \n",
            "Epoch: [2][440/830] Elapsed 2m 3s (remain 1m 49s) Loss: 0.0435(0.0158) Grad: 426728.7812  LR: 0.00000143  \n",
            "Epoch: [2][460/830] Elapsed 2m 9s (remain 1m 43s) Loss: 0.0231(0.0155) Grad: 262116.7656  LR: 0.00000143  \n",
            "Epoch: [2][480/830] Elapsed 2m 14s (remain 1m 37s) Loss: 0.0063(0.0152) Grad: 158417.9062  LR: 0.00000142  \n",
            "Epoch: [2][500/830] Elapsed 2m 21s (remain 1m 32s) Loss: 0.0270(0.0150) Grad: 534145.9375  LR: 0.00000142  \n",
            "Epoch: [2][520/830] Elapsed 2m 26s (remain 1m 26s) Loss: 0.0071(0.0148) Grad: 169032.4219  LR: 0.00000142  \n",
            "Epoch: [2][540/830] Elapsed 2m 32s (remain 1m 21s) Loss: 0.0098(0.0147) Grad: 190222.2812  LR: 0.00000142  \n",
            "Epoch: [2][560/830] Elapsed 2m 37s (remain 1m 15s) Loss: 0.0138(0.0148) Grad: 281123.8125  LR: 0.00000142  \n",
            "Epoch: [2][580/830] Elapsed 2m 43s (remain 1m 9s) Loss: 0.0038(0.0145) Grad: 116842.7344  LR: 0.00000141  \n",
            "Epoch: [2][600/830] Elapsed 2m 48s (remain 1m 4s) Loss: 0.0085(0.0145) Grad: 148405.3750  LR: 0.00000141  \n",
            "Epoch: [2][620/830] Elapsed 2m 54s (remain 0m 58s) Loss: 0.0095(0.0148) Grad: 276939.7500  LR: 0.00000141  \n",
            "Epoch: [2][640/830] Elapsed 2m 59s (remain 0m 52s) Loss: 0.0045(0.0148) Grad: 157973.5469  LR: 0.00000141  \n",
            "Epoch: [2][660/830] Elapsed 3m 5s (remain 0m 47s) Loss: 0.0518(0.0148) Grad: 487409.9062  LR: 0.00000140  \n",
            "Epoch: [2][680/830] Elapsed 3m 10s (remain 0m 41s) Loss: 0.0072(0.0148) Grad: 195548.2969  LR: 0.00000140  \n",
            "Epoch: [2][700/830] Elapsed 3m 16s (remain 0m 36s) Loss: 0.0157(0.0150) Grad: 237828.5938  LR: 0.00000140  \n",
            "Epoch: [2][720/830] Elapsed 3m 21s (remain 0m 30s) Loss: 0.0128(0.0149) Grad: 284208.6875  LR: 0.00000140  \n",
            "Epoch: [2][740/830] Elapsed 3m 27s (remain 0m 24s) Loss: 0.0018(0.0149) Grad: 71031.5156  LR: 0.00000139  \n",
            "Epoch: [2][760/830] Elapsed 3m 32s (remain 0m 19s) Loss: 0.0040(0.0148) Grad: 139869.1719  LR: 0.00000139  \n",
            "Epoch: [2][780/830] Elapsed 3m 38s (remain 0m 13s) Loss: 0.0096(0.0147) Grad: 203787.9375  LR: 0.00000139  \n",
            "Epoch: [2][800/830] Elapsed 3m 44s (remain 0m 8s) Loss: 0.0082(0.0147) Grad: 216893.3594  LR: 0.00000139  \n",
            "Epoch: [2][820/830] Elapsed 3m 50s (remain 0m 2s) Loss: 0.0031(0.0146) Grad: 118929.8750  LR: 0.00000139  \n",
            "Epoch: [2][829/830] Elapsed 3m 52s (remain 0m 0s) Loss: 0.0028(0.0145) Grad: 113292.0547  LR: 0.00000138  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0887(0.0887) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0829(0.1251) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0473(0.1192) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0684(0.1172) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1045(0.1226) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0968(0.1223) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0706(0.1249) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0145  avg_val_loss: 0.1244  time: 248s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0145  avg_val_loss: 0.1244  time: 248s\n",
            "Epoch 2 - Score: 0.5015  Scores: [0.5405288055532975, 0.5122707434936085, 0.44103238142711404, 0.5106267202163524, 0.5228661339525466, 0.4819225544529323]\n",
            "INFO:__main__:Epoch 2 - Score: 0.5015  Scores: [0.5405288055532975, 0.5122707434936085, 0.44103238142711404, 0.5106267202163524, 0.5228661339525466, 0.4819225544529323]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0791(0.1244) \n",
            "Epoch: [3][0/830] Elapsed 0m 0s (remain 6m 55s) Loss: 0.0041(0.0041) Grad: 120996.1406  LR: 0.00000138  \n",
            "Epoch: [3][20/830] Elapsed 0m 6s (remain 4m 17s) Loss: 0.0053(0.0091) Grad: 163205.7500  LR: 0.00000138  \n",
            "Epoch: [3][40/830] Elapsed 0m 12s (remain 3m 54s) Loss: 0.0021(0.0098) Grad: 116178.4375  LR: 0.00000138  \n",
            "Epoch: [3][60/830] Elapsed 0m 17s (remain 3m 43s) Loss: 0.0020(0.0090) Grad: 84741.4375  LR: 0.00000138  \n",
            "Epoch: [3][80/830] Elapsed 0m 23s (remain 3m 35s) Loss: 0.0083(0.0096) Grad: 198866.6250  LR: 0.00000138  \n",
            "Epoch: [3][100/830] Elapsed 0m 28s (remain 3m 28s) Loss: 0.0089(0.0093) Grad: 217995.2500  LR: 0.00000137  \n",
            "Epoch: [3][120/830] Elapsed 0m 34s (remain 3m 21s) Loss: 0.0032(0.0095) Grad: 116828.8438  LR: 0.00000137  \n",
            "Epoch: [3][140/830] Elapsed 0m 39s (remain 3m 14s) Loss: 0.0057(0.0096) Grad: 160328.0156  LR: 0.00000137  \n",
            "Epoch: [3][160/830] Elapsed 0m 45s (remain 3m 8s) Loss: 0.0083(0.0091) Grad: 226758.0781  LR: 0.00000137  \n",
            "Epoch: [3][180/830] Elapsed 0m 50s (remain 3m 1s) Loss: 0.0316(0.0096) Grad: 300430.7812  LR: 0.00000136  \n",
            "Epoch: [3][200/830] Elapsed 0m 56s (remain 2m 55s) Loss: 0.0161(0.0094) Grad: 200418.0156  LR: 0.00000136  \n",
            "Epoch: [3][220/830] Elapsed 1m 1s (remain 2m 50s) Loss: 0.0052(0.0094) Grad: 167760.6719  LR: 0.00000136  \n",
            "Epoch: [3][240/830] Elapsed 1m 7s (remain 2m 44s) Loss: 0.0022(0.0090) Grad: 105373.8672  LR: 0.00000136  \n",
            "Epoch: [3][260/830] Elapsed 1m 12s (remain 2m 38s) Loss: 0.0227(0.0090) Grad: 255349.4531  LR: 0.00000135  \n",
            "Epoch: [3][280/830] Elapsed 1m 18s (remain 2m 32s) Loss: 0.0011(0.0089) Grad: 64576.5352  LR: 0.00000135  \n",
            "Epoch: [3][300/830] Elapsed 1m 23s (remain 2m 26s) Loss: 0.0134(0.0090) Grad: 241412.5781  LR: 0.00000135  \n",
            "Epoch: [3][320/830] Elapsed 1m 29s (remain 2m 21s) Loss: 0.0064(0.0088) Grad: 194268.2031  LR: 0.00000135  \n",
            "Epoch: [3][340/830] Elapsed 1m 34s (remain 2m 15s) Loss: 0.0026(0.0086) Grad: 90153.8438  LR: 0.00000134  \n",
            "Epoch: [3][360/830] Elapsed 1m 40s (remain 2m 11s) Loss: 0.0106(0.0085) Grad: 192744.6719  LR: 0.00000134  \n",
            "Epoch: [3][380/830] Elapsed 1m 46s (remain 2m 5s) Loss: 0.0121(0.0085) Grad: 424634.3125  LR: 0.00000134  \n",
            "Epoch: [3][400/830] Elapsed 1m 52s (remain 1m 59s) Loss: 0.0061(0.0086) Grad: 140757.9844  LR: 0.00000134  \n",
            "Epoch: [3][420/830] Elapsed 1m 57s (remain 1m 54s) Loss: 0.0150(0.0085) Grad: 253163.0469  LR: 0.00000134  \n",
            "Epoch: [3][440/830] Elapsed 2m 3s (remain 1m 48s) Loss: 0.0088(0.0088) Grad: 225711.5312  LR: 0.00000133  \n",
            "Epoch: [3][460/830] Elapsed 2m 8s (remain 1m 42s) Loss: 0.0015(0.0089) Grad: 78489.1875  LR: 0.00000133  \n",
            "Epoch: [3][480/830] Elapsed 2m 14s (remain 1m 37s) Loss: 0.0054(0.0092) Grad: 188501.2812  LR: 0.00000133  \n",
            "Epoch: [3][500/830] Elapsed 2m 20s (remain 1m 32s) Loss: 0.0437(0.0093) Grad: 409908.0938  LR: 0.00000133  \n",
            "Epoch: [3][520/830] Elapsed 2m 26s (remain 1m 26s) Loss: 0.0064(0.0092) Grad: 132583.7656  LR: 0.00000132  \n",
            "Epoch: [3][540/830] Elapsed 2m 31s (remain 1m 21s) Loss: 0.0145(0.0093) Grad: 175739.8438  LR: 0.00000132  \n",
            "Epoch: [3][560/830] Elapsed 2m 37s (remain 1m 15s) Loss: 0.0339(0.0093) Grad: 386475.2500  LR: 0.00000132  \n",
            "Epoch: [3][580/830] Elapsed 2m 42s (remain 1m 9s) Loss: 0.0039(0.0094) Grad: 113692.3594  LR: 0.00000132  \n",
            "Epoch: [3][600/830] Elapsed 2m 48s (remain 1m 4s) Loss: 0.0311(0.0096) Grad: 422842.6562  LR: 0.00000131  \n",
            "Epoch: [3][620/830] Elapsed 2m 53s (remain 0m 58s) Loss: 0.0028(0.0096) Grad: 117473.0781  LR: 0.00000131  \n",
            "Epoch: [3][640/830] Elapsed 2m 59s (remain 0m 52s) Loss: 0.0118(0.0097) Grad: 225552.3438  LR: 0.00000131  \n",
            "Epoch: [3][660/830] Elapsed 3m 5s (remain 0m 47s) Loss: 0.0024(0.0101) Grad: 73870.3125  LR: 0.00000131  \n",
            "Epoch: [3][680/830] Elapsed 3m 10s (remain 0m 41s) Loss: 0.0220(0.0104) Grad: 293464.6250  LR: 0.00000130  \n",
            "Epoch: [3][700/830] Elapsed 3m 16s (remain 0m 36s) Loss: 0.0046(0.0104) Grad: 165384.6406  LR: 0.00000130  \n",
            "Epoch: [3][720/830] Elapsed 3m 21s (remain 0m 30s) Loss: 0.0064(0.0104) Grad: 220858.3125  LR: 0.00000130  \n",
            "Epoch: [3][740/830] Elapsed 3m 27s (remain 0m 24s) Loss: 0.0135(0.0103) Grad: 203811.3906  LR: 0.00000130  \n",
            "Epoch: [3][760/830] Elapsed 3m 33s (remain 0m 19s) Loss: 0.0479(0.0104) Grad: 544886.0000  LR: 0.00000129  \n",
            "Epoch: [3][780/830] Elapsed 3m 38s (remain 0m 13s) Loss: 0.0030(0.0103) Grad: 124279.1250  LR: 0.00000129  \n",
            "Epoch: [3][800/830] Elapsed 3m 44s (remain 0m 8s) Loss: 0.0122(0.0103) Grad: 267233.9688  LR: 0.00000129  \n",
            "Epoch: [3][820/830] Elapsed 3m 50s (remain 0m 2s) Loss: 0.0421(0.0106) Grad: 413868.2500  LR: 0.00000129  \n",
            "Epoch: [3][829/830] Elapsed 3m 52s (remain 0m 0s) Loss: 0.0077(0.0106) Grad: 238219.4531  LR: 0.00000129  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0875(0.0875) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0889(0.1208) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0545(0.1183) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0696(0.1178) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1019(0.1225) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1070(0.1231) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0702(0.1257) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0106  avg_val_loss: 0.1254  time: 249s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0106  avg_val_loss: 0.1254  time: 249s\n",
            "Epoch 3 - Score: 0.5032  Scores: [0.5440513505449945, 0.5086684924989494, 0.4461054250000473, 0.5144297859872405, 0.5204570762846312, 0.4856065906960962]\n",
            "INFO:__main__:Epoch 3 - Score: 0.5032  Scores: [0.5440513505449945, 0.5086684924989494, 0.4461054250000473, 0.5144297859872405, 0.5204570762846312, 0.4856065906960962]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0918(0.1254) \n",
            "Epoch: [4][0/830] Elapsed 0m 0s (remain 6m 47s) Loss: 0.0072(0.0072) Grad: 179467.5781  LR: 0.00000129  \n",
            "Epoch: [4][20/830] Elapsed 0m 6s (remain 3m 59s) Loss: 0.0143(0.0061) Grad: 264512.6875  LR: 0.00000128  \n",
            "Epoch: [4][40/830] Elapsed 0m 11s (remain 3m 45s) Loss: 0.0049(0.0067) Grad: 158950.5625  LR: 0.00000128  \n",
            "Epoch: [4][60/830] Elapsed 0m 17s (remain 3m 37s) Loss: 0.0020(0.0071) Grad: 82554.3438  LR: 0.00000128  \n",
            "Epoch: [4][80/830] Elapsed 0m 22s (remain 3m 30s) Loss: 0.0016(0.0070) Grad: 82596.6094  LR: 0.00000128  \n",
            "Epoch: [4][100/830] Elapsed 0m 28s (remain 3m 23s) Loss: 0.0022(0.0064) Grad: 106168.0312  LR: 0.00000127  \n",
            "Epoch: [4][120/830] Elapsed 0m 33s (remain 3m 17s) Loss: 0.0128(0.0071) Grad: 193454.2188  LR: 0.00000127  \n",
            "Epoch: [4][140/830] Elapsed 0m 39s (remain 3m 11s) Loss: 0.0025(0.0079) Grad: 131843.6094  LR: 0.00000127  \n",
            "Epoch: [4][160/830] Elapsed 0m 44s (remain 3m 6s) Loss: 0.0164(0.0078) Grad: 216480.3125  LR: 0.00000127  \n",
            "Epoch: [4][180/830] Elapsed 0m 50s (remain 3m 0s) Loss: 0.0026(0.0087) Grad: 104699.4688  LR: 0.00000126  \n",
            "Epoch: [4][200/830] Elapsed 0m 56s (remain 2m 55s) Loss: 0.1899(0.0098) Grad: 1106730.1250  LR: 0.00000126  \n",
            "Epoch: [4][220/830] Elapsed 1m 1s (remain 2m 49s) Loss: 0.0034(0.0096) Grad: 138508.2188  LR: 0.00000126  \n",
            "Epoch: [4][240/830] Elapsed 1m 7s (remain 2m 43s) Loss: 0.0357(0.0097) Grad: 791537.4375  LR: 0.00000126  \n",
            "Epoch: [4][260/830] Elapsed 1m 13s (remain 2m 40s) Loss: 0.0207(0.0098) Grad: 231490.7969  LR: 0.00000125  \n",
            "Epoch: [4][280/830] Elapsed 1m 18s (remain 2m 34s) Loss: 0.0012(0.0096) Grad: 59174.3867  LR: 0.00000125  \n",
            "Epoch: [4][300/830] Elapsed 1m 24s (remain 2m 28s) Loss: 0.0005(0.0093) Grad: 40745.9570  LR: 0.00000125  \n",
            "Epoch: [4][320/830] Elapsed 1m 29s (remain 2m 22s) Loss: 0.0010(0.0091) Grad: 73105.1094  LR: 0.00000125  \n",
            "Epoch: [4][340/830] Elapsed 1m 35s (remain 2m 16s) Loss: 0.0202(0.0090) Grad: 359555.7500  LR: 0.00000124  \n",
            "Epoch: [4][360/830] Elapsed 1m 41s (remain 2m 12s) Loss: 0.0040(0.0089) Grad: 146238.9375  LR: 0.00000124  \n",
            "Epoch: [4][380/830] Elapsed 1m 47s (remain 2m 6s) Loss: 0.0005(0.0088) Grad: 39763.3242  LR: 0.00000124  \n",
            "Epoch: [4][400/830] Elapsed 1m 52s (remain 2m 0s) Loss: 0.0018(0.0086) Grad: 104000.2344  LR: 0.00000124  \n",
            "Epoch: [4][420/830] Elapsed 1m 58s (remain 1m 55s) Loss: 0.0128(0.0085) Grad: 176601.9688  LR: 0.00000124  \n",
            "Epoch: [4][440/830] Elapsed 2m 4s (remain 1m 49s) Loss: 0.0069(0.0085) Grad: 171727.4688  LR: 0.00000123  \n",
            "Epoch: [4][460/830] Elapsed 2m 9s (remain 1m 43s) Loss: 0.0023(0.0085) Grad: 99899.8047  LR: 0.00000123  \n",
            "Epoch: [4][480/830] Elapsed 2m 15s (remain 1m 38s) Loss: 0.0088(0.0084) Grad: 241375.8750  LR: 0.00000123  \n",
            "Epoch: [4][500/830] Elapsed 2m 20s (remain 1m 32s) Loss: 0.0033(0.0084) Grad: 138888.5938  LR: 0.00000123  \n",
            "Epoch: [4][520/830] Elapsed 2m 26s (remain 1m 26s) Loss: 0.0009(0.0083) Grad: 54469.7422  LR: 0.00000122  \n",
            "Epoch: [4][540/830] Elapsed 2m 32s (remain 1m 21s) Loss: 0.0016(0.0084) Grad: 67980.0938  LR: 0.00000122  \n",
            "Epoch: [4][560/830] Elapsed 2m 37s (remain 1m 15s) Loss: 0.0013(0.0083) Grad: 67580.5312  LR: 0.00000122  \n",
            "Epoch: [4][580/830] Elapsed 2m 43s (remain 1m 9s) Loss: 0.0009(0.0083) Grad: 49531.0938  LR: 0.00000122  \n",
            "Epoch: [4][600/830] Elapsed 2m 48s (remain 1m 4s) Loss: 0.0194(0.0083) Grad: 270039.0625  LR: 0.00000121  \n",
            "Epoch: [4][620/830] Elapsed 2m 54s (remain 0m 58s) Loss: 0.0028(0.0083) Grad: 110255.0625  LR: 0.00000121  \n",
            "Epoch: [4][640/830] Elapsed 3m 0s (remain 0m 53s) Loss: 0.0026(0.0083) Grad: 120656.7656  LR: 0.00000121  \n",
            "Epoch: [4][660/830] Elapsed 3m 5s (remain 0m 47s) Loss: 0.0113(0.0083) Grad: 225781.9062  LR: 0.00000121  \n",
            "Epoch: [4][680/830] Elapsed 3m 11s (remain 0m 41s) Loss: 0.0090(0.0082) Grad: 237454.2656  LR: 0.00000120  \n",
            "Epoch: [4][700/830] Elapsed 3m 17s (remain 0m 36s) Loss: 0.0009(0.0082) Grad: 73926.6719  LR: 0.00000120  \n",
            "Epoch: [4][720/830] Elapsed 3m 23s (remain 0m 30s) Loss: 0.0035(0.0082) Grad: 127333.6406  LR: 0.00000120  \n",
            "Epoch: [4][740/830] Elapsed 3m 28s (remain 0m 25s) Loss: 0.0034(0.0082) Grad: 119414.7969  LR: 0.00000120  \n",
            "Epoch: [4][760/830] Elapsed 3m 34s (remain 0m 19s) Loss: 0.0371(0.0082) Grad: 392661.0938  LR: 0.00000119  \n",
            "Epoch: [4][780/830] Elapsed 3m 39s (remain 0m 13s) Loss: 0.0042(0.0081) Grad: 144258.2969  LR: 0.00000119  \n",
            "Epoch: [4][800/830] Elapsed 3m 45s (remain 0m 8s) Loss: 0.0053(0.0082) Grad: 276161.3125  LR: 0.00000119  \n",
            "Epoch: [4][820/830] Elapsed 3m 50s (remain 0m 2s) Loss: 0.0126(0.0082) Grad: 223130.0781  LR: 0.00000119  \n",
            "Epoch: [4][829/830] Elapsed 3m 52s (remain 0m 0s) Loss: 0.0030(0.0082) Grad: 94726.7656  LR: 0.00000118  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0980(0.0980) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1101(0.1294) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0644(0.1253) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0697(0.1226) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1033(0.1285) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0979(0.1276) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0766(0.1303) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0082  avg_val_loss: 0.1295  time: 249s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0082  avg_val_loss: 0.1295  time: 249s\n",
            "Epoch 4 - Score: 0.5123  Scores: [0.5458458398234279, 0.5188138364242358, 0.449310656377602, 0.5248912749558405, 0.5247608029556297, 0.5103194569819957]\n",
            "INFO:__main__:Epoch 4 - Score: 0.5123  Scores: [0.5458458398234279, 0.5188138364242358, 0.449310656377602, 0.5248912749558405, 0.5247608029556297, 0.5103194569819957]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0780(0.1295) \n",
            "Epoch: [5][0/830] Elapsed 0m 0s (remain 6m 55s) Loss: 0.0055(0.0055) Grad: 172255.5625  LR: 0.00000118  \n",
            "Epoch: [5][20/830] Elapsed 0m 5s (remain 3m 50s) Loss: 0.0008(0.0068) Grad: 84917.8828  LR: 0.00000118  \n",
            "Epoch: [5][40/830] Elapsed 0m 11s (remain 3m 39s) Loss: 0.0162(0.0061) Grad: 287006.2500  LR: 0.00000118  \n",
            "Epoch: [5][60/830] Elapsed 0m 16s (remain 3m 32s) Loss: 0.0006(0.0059) Grad: 53052.1172  LR: 0.00000118  \n",
            "Epoch: [5][80/830] Elapsed 0m 22s (remain 3m 26s) Loss: 0.0010(0.0053) Grad: 63884.1875  LR: 0.00000117  \n",
            "Epoch: [5][100/830] Elapsed 0m 27s (remain 3m 21s) Loss: 0.0004(0.0051) Grad: 44869.6289  LR: 0.00000117  \n",
            "Epoch: [5][120/830] Elapsed 0m 33s (remain 3m 15s) Loss: 0.0015(0.0056) Grad: 71140.9141  LR: 0.00000117  \n",
            "Epoch: [5][140/830] Elapsed 0m 38s (remain 3m 10s) Loss: 0.0081(0.0056) Grad: 252484.3594  LR: 0.00000117  \n",
            "Epoch: [5][160/830] Elapsed 0m 44s (remain 3m 4s) Loss: 0.0086(0.0056) Grad: 231056.9062  LR: 0.00000116  \n",
            "Epoch: [5][180/830] Elapsed 0m 49s (remain 2m 58s) Loss: 0.0042(0.0055) Grad: 156071.7656  LR: 0.00000116  \n",
            "Epoch: [5][200/830] Elapsed 0m 56s (remain 2m 55s) Loss: 0.0094(0.0056) Grad: 209877.1406  LR: 0.00000116  \n",
            "Epoch: [5][220/830] Elapsed 1m 2s (remain 2m 53s) Loss: 0.0067(0.0056) Grad: 223728.0000  LR: 0.00000116  \n",
            "Epoch: [5][240/830] Elapsed 1m 8s (remain 2m 46s) Loss: 0.0685(0.0062) Grad: 456921.8750  LR: 0.00000116  \n",
            "Epoch: [5][260/830] Elapsed 1m 13s (remain 2m 40s) Loss: 0.0067(0.0062) Grad: 170547.3438  LR: 0.00000115  \n",
            "Epoch: [5][280/830] Elapsed 1m 19s (remain 2m 35s) Loss: 0.0065(0.0061) Grad: 238073.1719  LR: 0.00000115  \n",
            "Epoch: [5][300/830] Elapsed 1m 24s (remain 2m 29s) Loss: 0.0093(0.0060) Grad: 215056.2500  LR: 0.00000115  \n",
            "Epoch: [5][320/830] Elapsed 1m 30s (remain 2m 23s) Loss: 0.0046(0.0060) Grad: 156747.0469  LR: 0.00000115  \n",
            "Epoch: [5][340/830] Elapsed 1m 35s (remain 2m 17s) Loss: 0.0029(0.0061) Grad: 116886.5547  LR: 0.00000114  \n",
            "Epoch: [5][360/830] Elapsed 1m 41s (remain 2m 11s) Loss: 0.0034(0.0060) Grad: 128197.5000  LR: 0.00000114  \n",
            "Epoch: [5][380/830] Elapsed 1m 48s (remain 2m 7s) Loss: 0.0102(0.0059) Grad: 220733.6719  LR: 0.00000114  \n",
            "Epoch: [5][400/830] Elapsed 1m 53s (remain 2m 1s) Loss: 0.0007(0.0059) Grad: 50393.1211  LR: 0.00000114  \n",
            "Epoch: [5][420/830] Elapsed 1m 59s (remain 1m 56s) Loss: 0.0010(0.0059) Grad: 66909.5547  LR: 0.00000113  \n",
            "Epoch: [5][440/830] Elapsed 2m 4s (remain 1m 50s) Loss: 0.0044(0.0062) Grad: 138766.3906  LR: 0.00000113  \n",
            "Epoch: [5][460/830] Elapsed 2m 10s (remain 1m 44s) Loss: 0.0009(0.0062) Grad: 70308.0547  LR: 0.00000113  \n",
            "Epoch: [5][480/830] Elapsed 2m 15s (remain 1m 38s) Loss: 0.0236(0.0062) Grad: 300827.9062  LR: 0.00000113  \n",
            "Epoch: [5][500/830] Elapsed 2m 21s (remain 1m 32s) Loss: 0.0006(0.0062) Grad: 55731.5625  LR: 0.00000112  \n",
            "Epoch: [5][520/830] Elapsed 2m 27s (remain 1m 27s) Loss: 0.0082(0.0063) Grad: 214975.6406  LR: 0.00000112  \n",
            "Epoch: [5][540/830] Elapsed 2m 32s (remain 1m 21s) Loss: 0.0214(0.0063) Grad: 296713.6875  LR: 0.00000112  \n",
            "Epoch: [5][560/830] Elapsed 2m 38s (remain 1m 15s) Loss: 0.0023(0.0062) Grad: 125678.0000  LR: 0.00000112  \n",
            "Epoch: [5][580/830] Elapsed 2m 43s (remain 1m 10s) Loss: 0.0072(0.0062) Grad: 267650.8438  LR: 0.00000111  \n",
            "Epoch: [5][600/830] Elapsed 2m 49s (remain 1m 4s) Loss: 0.0032(0.0063) Grad: 123145.4844  LR: 0.00000111  \n",
            "Epoch: [5][620/830] Elapsed 2m 54s (remain 0m 58s) Loss: 0.0094(0.0062) Grad: 209831.9844  LR: 0.00000111  \n",
            "Epoch: [5][640/830] Elapsed 3m 0s (remain 0m 53s) Loss: 0.0029(0.0062) Grad: 131414.4219  LR: 0.00000111  \n",
            "Epoch: [5][660/830] Elapsed 3m 5s (remain 0m 47s) Loss: 0.0037(0.0061) Grad: 106860.4766  LR: 0.00000110  \n",
            "Epoch: [5][680/830] Elapsed 3m 11s (remain 0m 41s) Loss: 0.0229(0.0062) Grad: 286033.2500  LR: 0.00000110  \n",
            "Epoch: [5][700/830] Elapsed 3m 16s (remain 0m 36s) Loss: 0.0019(0.0063) Grad: 106339.2422  LR: 0.00000110  \n",
            "Epoch: [5][720/830] Elapsed 3m 22s (remain 0m 30s) Loss: 0.0056(0.0063) Grad: 169917.4688  LR: 0.00000110  \n",
            "Epoch: [5][740/830] Elapsed 3m 27s (remain 0m 24s) Loss: 0.0046(0.0065) Grad: 196625.2344  LR: 0.00000109  \n",
            "Epoch: [5][760/830] Elapsed 3m 33s (remain 0m 19s) Loss: 0.0028(0.0064) Grad: 129421.1328  LR: 0.00000109  \n",
            "Epoch: [5][780/830] Elapsed 3m 39s (remain 0m 13s) Loss: 0.0011(0.0064) Grad: 68891.1484  LR: 0.00000109  \n",
            "Epoch: [5][800/830] Elapsed 3m 44s (remain 0m 8s) Loss: 0.0080(0.0064) Grad: 915267.1875  LR: 0.00000109  \n",
            "Epoch: [5][820/830] Elapsed 3m 50s (remain 0m 2s) Loss: 0.0048(0.0064) Grad: 158100.2969  LR: 0.00000108  \n",
            "Epoch: [5][829/830] Elapsed 3m 53s (remain 0m 0s) Loss: 0.0018(0.0064) Grad: 76609.7656  LR: 0.00000108  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0911(0.0911) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1109(0.1259) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0585(0.1223) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0554(0.1208) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1088(0.1258) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0989(0.1253) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0724(0.1283) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0064  avg_val_loss: 0.1276  time: 249s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0064  avg_val_loss: 0.1276  time: 249s\n",
            "Epoch 5 - Score: 0.5077  Scores: [0.5405835698503, 0.50903030982796, 0.45089933905107754, 0.5320451149251493, 0.5240654117679651, 0.48930470742032456]\n",
            "INFO:__main__:Epoch 5 - Score: 0.5077  Scores: [0.5405835698503, 0.50903030982796, 0.45089933905107754, 0.5320451149251493, 0.5240654117679651, 0.48930470742032456]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0918(0.1276) \n",
            "(40, 6) Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Index(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold'], dtype='object')\n",
            "Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Epoch: [1][0/870] Elapsed 0m 0s (remain 7m 18s) Loss: 0.0035(0.0035) Grad: 154182.3750  LR: 0.00000108  \n",
            "Epoch: [1][20/870] Elapsed 0m 6s (remain 4m 3s) Loss: 0.0012(0.0072) Grad: 83013.0469  LR: 0.00000108  \n",
            "Epoch: [1][40/870] Elapsed 0m 11s (remain 3m 52s) Loss: 0.0136(0.0057) Grad: 183863.2344  LR: 0.00000108  \n",
            "Epoch: [1][60/870] Elapsed 0m 17s (remain 3m 51s) Loss: 0.0008(0.0053) Grad: 63988.9648  LR: 0.00000107  \n",
            "Epoch: [1][80/870] Elapsed 0m 23s (remain 3m 49s) Loss: 0.0019(0.0059) Grad: 95078.8984  LR: 0.00000107  \n",
            "Epoch: [1][100/870] Elapsed 0m 29s (remain 3m 41s) Loss: 0.0057(0.0053) Grad: 176050.2031  LR: 0.00000107  \n",
            "Epoch: [1][120/870] Elapsed 0m 34s (remain 3m 34s) Loss: 0.0015(0.0054) Grad: 83248.6641  LR: 0.00000107  \n",
            "Epoch: [1][140/870] Elapsed 0m 40s (remain 3m 27s) Loss: 0.0018(0.0051) Grad: 114454.1719  LR: 0.00000106  \n",
            "Epoch: [1][160/870] Elapsed 0m 45s (remain 3m 20s) Loss: 0.0061(0.0052) Grad: nan  LR: 0.00000106  \n",
            "Epoch: [1][180/870] Elapsed 0m 50s (remain 3m 13s) Loss: 0.0032(0.0050) Grad: 50613.4961  LR: 0.00000106  \n",
            "Epoch: [1][200/870] Elapsed 0m 56s (remain 3m 7s) Loss: 0.0015(0.0049) Grad: 43368.7070  LR: 0.00000106  \n",
            "Epoch: [1][220/870] Elapsed 1m 1s (remain 3m 1s) Loss: 0.0026(0.0048) Grad: 47937.8086  LR: 0.00000105  \n",
            "Epoch: [1][240/870] Elapsed 1m 8s (remain 2m 58s) Loss: 0.0012(0.0048) Grad: 26310.7168  LR: 0.00000105  \n",
            "Epoch: [1][260/870] Elapsed 1m 14s (remain 2m 54s) Loss: 0.0060(0.0047) Grad: 96248.1094  LR: 0.00000105  \n",
            "Epoch: [1][280/870] Elapsed 1m 20s (remain 2m 48s) Loss: 0.0005(0.0047) Grad: 23065.4648  LR: 0.00000105  \n",
            "Epoch: [1][300/870] Elapsed 1m 26s (remain 2m 42s) Loss: 0.0268(0.0050) Grad: 257505.4688  LR: 0.00000104  \n",
            "Epoch: [1][320/870] Elapsed 1m 31s (remain 2m 36s) Loss: 0.0563(0.0051) Grad: 190318.5938  LR: 0.00000104  \n",
            "Epoch: [1][340/870] Elapsed 1m 37s (remain 2m 30s) Loss: 0.0005(0.0052) Grad: 24552.9102  LR: 0.00000104  \n",
            "Epoch: [1][360/870] Elapsed 1m 42s (remain 2m 24s) Loss: 0.0009(0.0051) Grad: 35966.2383  LR: 0.00000104  \n",
            "Epoch: [1][380/870] Elapsed 1m 47s (remain 2m 18s) Loss: 0.0019(0.0051) Grad: 56211.7695  LR: 0.00000103  \n",
            "Epoch: [1][400/870] Elapsed 1m 53s (remain 2m 12s) Loss: 0.0012(0.0052) Grad: 30600.5586  LR: 0.00000103  \n",
            "Epoch: [1][420/870] Elapsed 1m 58s (remain 2m 6s) Loss: 0.0052(0.0052) Grad: 165262.8750  LR: 0.00000103  \n",
            "Epoch: [1][440/870] Elapsed 2m 4s (remain 2m 0s) Loss: 0.0013(0.0052) Grad: 41431.3555  LR: 0.00000103  \n",
            "Epoch: [1][460/870] Elapsed 2m 9s (remain 1m 54s) Loss: 0.0047(0.0052) Grad: 38789.9531  LR: 0.00000102  \n",
            "Epoch: [1][480/870] Elapsed 2m 15s (remain 1m 49s) Loss: 0.0128(0.0054) Grad: 73351.3906  LR: 0.00000102  \n",
            "Epoch: [1][500/870] Elapsed 2m 20s (remain 1m 43s) Loss: 0.0013(0.0055) Grad: 13522.6426  LR: 0.00000102  \n",
            "Epoch: [1][520/870] Elapsed 2m 26s (remain 1m 38s) Loss: 0.0129(0.0057) Grad: 72032.4375  LR: 0.00000102  \n",
            "Epoch: [1][540/870] Elapsed 2m 31s (remain 1m 32s) Loss: 0.0029(0.0059) Grad: 33505.3086  LR: 0.00000101  \n",
            "Epoch: [1][560/870] Elapsed 2m 37s (remain 1m 26s) Loss: 0.0027(0.0060) Grad: 32848.8516  LR: 0.00000101  \n",
            "Epoch: [1][580/870] Elapsed 2m 42s (remain 1m 20s) Loss: 0.0063(0.0063) Grad: 53253.8203  LR: 0.00000101  \n",
            "Epoch: [1][600/870] Elapsed 2m 48s (remain 1m 15s) Loss: 0.0148(0.0064) Grad: 73234.0000  LR: 0.00000101  \n",
            "Epoch: [1][620/870] Elapsed 2m 53s (remain 1m 9s) Loss: 0.0032(0.0063) Grad: 29106.2656  LR: 0.00000100  \n",
            "Epoch: [1][640/870] Elapsed 2m 59s (remain 1m 4s) Loss: 0.0025(0.0063) Grad: 32555.8047  LR: 0.00000100  \n",
            "Epoch: [1][660/870] Elapsed 3m 4s (remain 0m 58s) Loss: 0.0014(0.0064) Grad: 21081.2871  LR: 0.00000100  \n",
            "Epoch: [1][680/870] Elapsed 3m 10s (remain 0m 52s) Loss: 0.0030(0.0064) Grad: 21174.6426  LR: 0.00000100  \n",
            "Epoch: [1][700/870] Elapsed 3m 15s (remain 0m 47s) Loss: 0.0439(0.0065) Grad: 94442.8984  LR: 0.00000099  \n",
            "Epoch: [1][720/870] Elapsed 3m 21s (remain 0m 41s) Loss: 0.0052(0.0066) Grad: 48883.4180  LR: 0.00000099  \n",
            "Epoch: [1][740/870] Elapsed 3m 26s (remain 0m 36s) Loss: 0.0095(0.0066) Grad: 64453.4492  LR: 0.00000099  \n",
            "Epoch: [1][760/870] Elapsed 3m 32s (remain 0m 30s) Loss: 0.0024(0.0066) Grad: 25214.2871  LR: 0.00000099  \n",
            "Epoch: [1][780/870] Elapsed 3m 38s (remain 0m 24s) Loss: 0.0028(0.0066) Grad: 23734.3477  LR: 0.00000098  \n",
            "Epoch: [1][800/870] Elapsed 3m 44s (remain 0m 19s) Loss: 0.0028(0.0067) Grad: 35845.6992  LR: 0.00000098  \n",
            "Epoch: [1][820/870] Elapsed 3m 49s (remain 0m 13s) Loss: 0.0069(0.0067) Grad: 59710.9180  LR: 0.00000098  \n",
            "Epoch: [1][840/870] Elapsed 3m 55s (remain 0m 8s) Loss: 0.0014(0.0068) Grad: 17306.6680  LR: 0.00000098  \n",
            "Epoch: [1][860/870] Elapsed 4m 0s (remain 0m 2s) Loss: 0.0023(0.0068) Grad: 19266.1660  LR: 0.00000097  \n",
            "Epoch: [1][869/870] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0049(0.0067) Grad: 49973.5195  LR: 0.00000097  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0961(0.0961) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0979(0.1252) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0448(0.1221) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0500(0.1197) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1092(0.1239) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1063(0.1230) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0833(0.1261) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0067  avg_val_loss: 0.1256  time: 259s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0067  avg_val_loss: 0.1256  time: 259s\n",
            "Epoch 1 - Score: 0.5045  Scores: [0.5416996852553712, 0.5152315925740998, 0.449879012841027, 0.5182643394648494, 0.5156217064648022, 0.4865996393209579]\n",
            "INFO:__main__:Epoch 1 - Score: 0.5045  Scores: [0.5416996852553712, 0.5152315925740998, 0.449879012841027, 0.5182643394648494, 0.5156217064648022, 0.4865996393209579]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0871(0.1256) \n",
            "Epoch: [2][0/870] Elapsed 0m 0s (remain 7m 5s) Loss: 0.0020(0.0020) Grad: 117312.1562  LR: 0.00000097  \n",
            "Epoch: [2][20/870] Elapsed 0m 5s (remain 4m 1s) Loss: 0.0057(0.0067) Grad: 190357.9219  LR: 0.00000097  \n",
            "Epoch: [2][40/870] Elapsed 0m 11s (remain 3m 53s) Loss: 0.0091(0.0078) Grad: 351188.5312  LR: 0.00000097  \n",
            "Epoch: [2][60/870] Elapsed 0m 17s (remain 3m 46s) Loss: 0.0069(0.0074) Grad: 257613.4375  LR: 0.00000096  \n",
            "Epoch: [2][80/870] Elapsed 0m 22s (remain 3m 38s) Loss: 0.0053(0.0077) Grad: 216814.7344  LR: 0.00000096  \n",
            "Epoch: [2][100/870] Elapsed 0m 27s (remain 3m 32s) Loss: 0.0012(0.0078) Grad: 70369.9219  LR: 0.00000096  \n",
            "Epoch: [2][120/870] Elapsed 0m 33s (remain 3m 28s) Loss: 0.0154(0.0076) Grad: 352721.8438  LR: 0.00000096  \n",
            "Epoch: [2][140/870] Elapsed 0m 39s (remain 3m 23s) Loss: 0.0111(0.0085) Grad: 256304.7656  LR: 0.00000095  \n",
            "Epoch: [2][160/870] Elapsed 0m 44s (remain 3m 17s) Loss: 0.0011(0.0084) Grad: 67321.3047  LR: 0.00000095  \n",
            "Epoch: [2][180/870] Elapsed 0m 50s (remain 3m 12s) Loss: 0.0022(0.0090) Grad: 110777.8516  LR: 0.00000095  \n",
            "Epoch: [2][200/870] Elapsed 0m 56s (remain 3m 7s) Loss: 0.0100(0.0089) Grad: 197046.3750  LR: 0.00000095  \n",
            "Epoch: [2][220/870] Elapsed 1m 1s (remain 3m 1s) Loss: 0.0007(0.0085) Grad: 65464.0156  LR: 0.00000094  \n",
            "Epoch: [2][240/870] Elapsed 1m 7s (remain 2m 55s) Loss: 0.0051(0.0084) Grad: 180099.7500  LR: 0.00000094  \n",
            "Epoch: [2][260/870] Elapsed 1m 12s (remain 2m 49s) Loss: 0.0023(0.0082) Grad: 103946.3281  LR: 0.00000094  \n",
            "Epoch: [2][280/870] Elapsed 1m 18s (remain 2m 45s) Loss: 0.0086(0.0081) Grad: 221590.5000  LR: 0.00000094  \n",
            "Epoch: [2][300/870] Elapsed 1m 24s (remain 2m 39s) Loss: 0.0086(0.0081) Grad: 196316.0938  LR: 0.00000093  \n",
            "Epoch: [2][320/870] Elapsed 1m 29s (remain 2m 33s) Loss: 0.0098(0.0083) Grad: 272859.1875  LR: 0.00000093  \n",
            "Epoch: [2][340/870] Elapsed 1m 36s (remain 2m 29s) Loss: 0.0073(0.0082) Grad: 146486.7969  LR: 0.00000093  \n",
            "Epoch: [2][360/870] Elapsed 1m 41s (remain 2m 23s) Loss: 0.0100(0.0081) Grad: 259762.6250  LR: 0.00000093  \n",
            "Epoch: [2][380/870] Elapsed 1m 47s (remain 2m 17s) Loss: 0.0003(0.0079) Grad: 37591.8477  LR: 0.00000092  \n",
            "Epoch: [2][400/870] Elapsed 1m 52s (remain 2m 11s) Loss: 0.0076(0.0078) Grad: 300949.4062  LR: 0.00000092  \n",
            "Epoch: [2][420/870] Elapsed 1m 58s (remain 2m 6s) Loss: 0.0040(0.0077) Grad: 158183.7812  LR: 0.00000092  \n",
            "Epoch: [2][440/870] Elapsed 2m 3s (remain 2m 0s) Loss: 0.0021(0.0078) Grad: 81672.2422  LR: 0.00000092  \n",
            "Epoch: [2][460/870] Elapsed 2m 9s (remain 1m 54s) Loss: 0.0036(0.0078) Grad: 142525.3438  LR: 0.00000091  \n",
            "Epoch: [2][480/870] Elapsed 2m 14s (remain 1m 49s) Loss: 0.0046(0.0078) Grad: 196351.3438  LR: 0.00000091  \n",
            "Epoch: [2][500/870] Elapsed 2m 20s (remain 1m 43s) Loss: 0.0044(0.0076) Grad: 186280.3750  LR: 0.00000091  \n",
            "Epoch: [2][520/870] Elapsed 2m 25s (remain 1m 37s) Loss: 0.0028(0.0076) Grad: 149239.5469  LR: 0.00000091  \n",
            "Epoch: [2][540/870] Elapsed 2m 31s (remain 1m 32s) Loss: 0.0165(0.0075) Grad: 376542.5625  LR: 0.00000090  \n",
            "Epoch: [2][560/870] Elapsed 2m 36s (remain 1m 26s) Loss: 0.0040(0.0074) Grad: 66980.7891  LR: 0.00000090  \n",
            "Epoch: [2][580/870] Elapsed 2m 42s (remain 1m 20s) Loss: 0.0142(0.0074) Grad: 148911.6875  LR: 0.00000090  \n",
            "Epoch: [2][600/870] Elapsed 2m 48s (remain 1m 15s) Loss: 0.0007(0.0074) Grad: 23422.6152  LR: 0.00000090  \n",
            "Epoch: [2][620/870] Elapsed 2m 53s (remain 1m 9s) Loss: 0.0055(0.0074) Grad: 89031.2344  LR: 0.00000089  \n",
            "Epoch: [2][640/870] Elapsed 2m 59s (remain 1m 4s) Loss: 0.0037(0.0073) Grad: 68145.1016  LR: 0.00000089  \n",
            "Epoch: [2][660/870] Elapsed 3m 4s (remain 0m 58s) Loss: 0.0013(0.0073) Grad: 36897.6094  LR: 0.00000089  \n",
            "Epoch: [2][680/870] Elapsed 3m 11s (remain 0m 53s) Loss: 0.0172(0.0075) Grad: 133168.9531  LR: 0.00000089  \n",
            "Epoch: [2][700/870] Elapsed 3m 17s (remain 0m 47s) Loss: 0.0076(0.0075) Grad: 126831.4141  LR: 0.00000088  \n",
            "Epoch: [2][720/870] Elapsed 3m 22s (remain 0m 41s) Loss: 0.0094(0.0076) Grad: 127376.9062  LR: 0.00000088  \n",
            "Epoch: [2][740/870] Elapsed 3m 28s (remain 0m 36s) Loss: 0.0122(0.0075) Grad: 143224.4844  LR: 0.00000088  \n",
            "Epoch: [2][760/870] Elapsed 3m 33s (remain 0m 30s) Loss: 0.0101(0.0075) Grad: 123182.4688  LR: 0.00000088  \n",
            "Epoch: [2][780/870] Elapsed 3m 39s (remain 0m 24s) Loss: 0.0059(0.0075) Grad: 83896.2656  LR: 0.00000087  \n",
            "Epoch: [2][800/870] Elapsed 3m 44s (remain 0m 19s) Loss: 0.0153(0.0075) Grad: 191512.7500  LR: 0.00000087  \n",
            "Epoch: [2][820/870] Elapsed 3m 50s (remain 0m 13s) Loss: 0.0024(0.0074) Grad: 61668.8281  LR: 0.00000087  \n",
            "Epoch: [2][840/870] Elapsed 3m 55s (remain 0m 8s) Loss: 0.0039(0.0074) Grad: 82725.2891  LR: 0.00000087  \n",
            "Epoch: [2][860/870] Elapsed 4m 1s (remain 0m 2s) Loss: 0.0040(0.0073) Grad: 80549.7266  LR: 0.00000086  \n",
            "Epoch: [2][869/870] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0032(0.0074) Grad: 61064.8594  LR: 0.00000086  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0929(0.0929) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0976(0.1302) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0450(0.1285) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0703(0.1258) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.0995(0.1290) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0997(0.1291) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0945(0.1314) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0074  avg_val_loss: 0.1309  time: 260s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0074  avg_val_loss: 0.1309  time: 260s\n",
            "Epoch 2 - Score: 0.5149  Scores: [0.5676493168127109, 0.5226570933333974, 0.46255814454633354, 0.5250135811602556, 0.5241595172410125, 0.4873440593325937]\n",
            "INFO:__main__:Epoch 2 - Score: 0.5149  Scores: [0.5676493168127109, 0.5226570933333974, 0.46255814454633354, 0.5250135811602556, 0.5241595172410125, 0.4873440593325937]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0767(0.1309) \n",
            "Epoch: [3][0/870] Elapsed 0m 0s (remain 7m 21s) Loss: 0.0004(0.0004) Grad: 29786.0195  LR: 0.00000086  \n",
            "Epoch: [3][20/870] Elapsed 0m 5s (remain 4m 2s) Loss: 0.0017(0.0029) Grad: 83332.1875  LR: 0.00000086  \n",
            "Epoch: [3][40/870] Elapsed 0m 11s (remain 3m 50s) Loss: 0.0021(0.0039) Grad: 107891.8828  LR: 0.00000086  \n",
            "Epoch: [3][60/870] Elapsed 0m 16s (remain 3m 43s) Loss: 0.0035(0.0039) Grad: 152217.2031  LR: 0.00000086  \n",
            "Epoch: [3][80/870] Elapsed 0m 22s (remain 3m 37s) Loss: 0.0016(0.0036) Grad: 112780.4375  LR: 0.00000085  \n",
            "Epoch: [3][100/870] Elapsed 0m 27s (remain 3m 31s) Loss: 0.0020(0.0040) Grad: 102437.9297  LR: 0.00000085  \n",
            "Epoch: [3][120/870] Elapsed 0m 33s (remain 3m 25s) Loss: 0.0012(0.0040) Grad: 81695.5547  LR: 0.00000085  \n",
            "Epoch: [3][140/870] Elapsed 0m 38s (remain 3m 19s) Loss: 0.0051(0.0038) Grad: 184711.9531  LR: 0.00000085  \n",
            "Epoch: [3][160/870] Elapsed 0m 44s (remain 3m 14s) Loss: 0.0099(0.0039) Grad: 264390.1562  LR: 0.00000084  \n",
            "Epoch: [3][180/870] Elapsed 0m 49s (remain 3m 9s) Loss: 0.0019(0.0037) Grad: 105604.3516  LR: 0.00000084  \n",
            "Epoch: [3][200/870] Elapsed 0m 55s (remain 3m 3s) Loss: 0.0012(0.0037) Grad: 72005.2109  LR: 0.00000084  \n",
            "Epoch: [3][220/870] Elapsed 1m 1s (remain 2m 59s) Loss: 0.0034(0.0037) Grad: 155537.5312  LR: 0.00000084  \n",
            "Epoch: [3][240/870] Elapsed 1m 6s (remain 2m 53s) Loss: 0.0003(0.0036) Grad: 30301.4512  LR: 0.00000083  \n",
            "Epoch: [3][260/870] Elapsed 1m 12s (remain 2m 48s) Loss: 0.0015(0.0036) Grad: 91240.4219  LR: 0.00000083  \n",
            "Epoch: [3][280/870] Elapsed 1m 17s (remain 2m 42s) Loss: 0.0108(0.0035) Grad: 403478.3438  LR: 0.00000083  \n",
            "Epoch: [3][300/870] Elapsed 1m 23s (remain 2m 37s) Loss: 0.0028(0.0035) Grad: 132210.2344  LR: 0.00000083  \n",
            "Epoch: [3][320/870] Elapsed 1m 29s (remain 2m 32s) Loss: 0.0009(0.0036) Grad: 62741.2578  LR: 0.00000082  \n",
            "Epoch: [3][340/870] Elapsed 1m 34s (remain 2m 26s) Loss: 0.0109(0.0036) Grad: 257239.2656  LR: 0.00000082  \n",
            "Epoch: [3][360/870] Elapsed 1m 40s (remain 2m 21s) Loss: 0.0045(0.0035) Grad: 156526.5938  LR: 0.00000082  \n",
            "Epoch: [3][380/870] Elapsed 1m 45s (remain 2m 15s) Loss: 0.0013(0.0035) Grad: 86906.9219  LR: 0.00000082  \n",
            "Epoch: [3][400/870] Elapsed 1m 51s (remain 2m 10s) Loss: 0.0014(0.0036) Grad: 98011.1094  LR: 0.00000081  \n",
            "Epoch: [3][420/870] Elapsed 1m 56s (remain 2m 4s) Loss: 0.0085(0.0036) Grad: 231998.4688  LR: 0.00000081  \n",
            "Epoch: [3][440/870] Elapsed 2m 2s (remain 1m 58s) Loss: 0.0015(0.0035) Grad: 91973.5234  LR: 0.00000081  \n",
            "Epoch: [3][460/870] Elapsed 2m 7s (remain 1m 53s) Loss: 0.0016(0.0035) Grad: 101864.4609  LR: 0.00000081  \n",
            "Epoch: [3][480/870] Elapsed 2m 13s (remain 1m 47s) Loss: 0.0025(0.0036) Grad: 100163.3516  LR: 0.00000080  \n",
            "Epoch: [3][500/870] Elapsed 2m 18s (remain 1m 42s) Loss: 0.0013(0.0037) Grad: 95209.6172  LR: 0.00000080  \n",
            "Epoch: [3][520/870] Elapsed 2m 24s (remain 1m 36s) Loss: 0.0013(0.0036) Grad: 87327.3984  LR: 0.00000080  \n",
            "Epoch: [3][540/870] Elapsed 2m 30s (remain 1m 31s) Loss: 0.0014(0.0038) Grad: 74766.4688  LR: 0.00000080  \n",
            "Epoch: [3][560/870] Elapsed 2m 35s (remain 1m 25s) Loss: 0.0069(0.0038) Grad: 248239.6562  LR: 0.00000079  \n",
            "Epoch: [3][580/870] Elapsed 2m 41s (remain 1m 20s) Loss: 0.0006(0.0038) Grad: 43657.7070  LR: 0.00000079  \n",
            "Epoch: [3][600/870] Elapsed 2m 46s (remain 1m 14s) Loss: 0.0015(0.0037) Grad: 82389.7500  LR: 0.00000079  \n",
            "Epoch: [3][620/870] Elapsed 2m 52s (remain 1m 9s) Loss: 0.0010(0.0037) Grad: 65353.2617  LR: 0.00000079  \n",
            "Epoch: [3][640/870] Elapsed 2m 58s (remain 1m 3s) Loss: 0.0066(0.0038) Grad: 183626.1250  LR: 0.00000078  \n",
            "Epoch: [3][660/870] Elapsed 3m 3s (remain 0m 58s) Loss: 0.0008(0.0037) Grad: 54531.1484  LR: 0.00000078  \n",
            "Epoch: [3][680/870] Elapsed 3m 9s (remain 0m 52s) Loss: 0.0079(0.0037) Grad: 218498.7500  LR: 0.00000078  \n",
            "Epoch: [3][700/870] Elapsed 3m 14s (remain 0m 46s) Loss: 0.0085(0.0037) Grad: 265380.2812  LR: 0.00000078  \n",
            "Epoch: [3][720/870] Elapsed 3m 21s (remain 0m 41s) Loss: 0.0014(0.0037) Grad: 70396.2812  LR: 0.00000077  \n",
            "Epoch: [3][740/870] Elapsed 3m 27s (remain 0m 36s) Loss: 0.0044(0.0037) Grad: 173311.0781  LR: 0.00000077  \n",
            "Epoch: [3][760/870] Elapsed 3m 33s (remain 0m 30s) Loss: 0.0054(0.0037) Grad: 175580.3594  LR: 0.00000077  \n",
            "Epoch: [3][780/870] Elapsed 3m 38s (remain 0m 24s) Loss: 0.0024(0.0037) Grad: 118511.5703  LR: 0.00000077  \n",
            "Epoch: [3][800/870] Elapsed 3m 44s (remain 0m 19s) Loss: 0.0032(0.0037) Grad: 147235.0938  LR: 0.00000076  \n",
            "Epoch: [3][820/870] Elapsed 3m 50s (remain 0m 13s) Loss: 0.0014(0.0037) Grad: 66959.9219  LR: 0.00000076  \n",
            "Epoch: [3][840/870] Elapsed 3m 56s (remain 0m 8s) Loss: 0.0067(0.0037) Grad: 186162.0625  LR: 0.00000076  \n",
            "Epoch: [3][860/870] Elapsed 4m 1s (remain 0m 2s) Loss: 0.0068(0.0038) Grad: 77987.7656  LR: 0.00000076  \n",
            "Epoch: [3][869/870] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0048(0.0038) Grad: 60653.2852  LR: 0.00000076  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0852(0.0852) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.1011(0.1244) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0486(0.1221) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0647(0.1210) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1166(0.1258) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.0965(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0837(0.1280) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0038  avg_val_loss: 0.1276  time: 260s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0038  avg_val_loss: 0.1276  time: 260s\n",
            "Epoch 3 - Score: 0.5079  Scores: [0.5414237515807696, 0.5221531679129477, 0.45038705710745847, 0.5216062271828302, 0.5257409569170493, 0.48623820453147953]\n",
            "INFO:__main__:Epoch 3 - Score: 0.5079  Scores: [0.5414237515807696, 0.5221531679129477, 0.45038705710745847, 0.5216062271828302, 0.5257409569170493, 0.48623820453147953]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0892(0.1276) \n",
            "Epoch: [4][0/870] Elapsed 0m 0s (remain 7m 15s) Loss: 0.0022(0.0022) Grad: 119678.6562  LR: 0.00000076  \n",
            "Epoch: [4][20/870] Elapsed 0m 6s (remain 4m 5s) Loss: 0.0017(0.0037) Grad: 93839.4609  LR: 0.00000075  \n",
            "Epoch: [4][40/870] Elapsed 0m 12s (remain 4m 13s) Loss: 0.0027(0.0030) Grad: 112342.6641  LR: 0.00000075  \n",
            "Epoch: [4][60/870] Elapsed 0m 18s (remain 3m 58s) Loss: 0.0004(0.0025) Grad: 42743.2188  LR: 0.00000075  \n",
            "Epoch: [4][80/870] Elapsed 0m 23s (remain 3m 50s) Loss: 0.0002(0.0037) Grad: 31591.1367  LR: 0.00000075  \n",
            "Epoch: [4][100/870] Elapsed 0m 29s (remain 3m 41s) Loss: 0.0009(0.0037) Grad: 53099.1523  LR: 0.00000074  \n",
            "Epoch: [4][120/870] Elapsed 0m 34s (remain 3m 33s) Loss: 0.0028(0.0033) Grad: 119325.5391  LR: 0.00000074  \n",
            "Epoch: [4][140/870] Elapsed 0m 40s (remain 3m 27s) Loss: 0.0010(0.0035) Grad: 82046.3281  LR: 0.00000074  \n",
            "Epoch: [4][160/870] Elapsed 0m 45s (remain 3m 21s) Loss: 0.0007(0.0035) Grad: 60281.0859  LR: 0.00000074  \n",
            "Epoch: [4][180/870] Elapsed 0m 51s (remain 3m 14s) Loss: 0.0018(0.0033) Grad: 90335.8125  LR: 0.00000073  \n",
            "Epoch: [4][200/870] Elapsed 0m 56s (remain 3m 9s) Loss: 0.0011(0.0033) Grad: 72806.5703  LR: 0.00000073  \n",
            "Epoch: [4][220/870] Elapsed 1m 3s (remain 3m 5s) Loss: 0.0040(0.0033) Grad: 178965.6719  LR: 0.00000073  \n",
            "Epoch: [4][240/870] Elapsed 1m 8s (remain 2m 59s) Loss: 0.0014(0.0032) Grad: 88620.7656  LR: 0.00000073  \n",
            "Epoch: [4][260/870] Elapsed 1m 14s (remain 2m 53s) Loss: 0.0001(0.0031) Grad: 17516.2695  LR: 0.00000072  \n",
            "Epoch: [4][280/870] Elapsed 1m 19s (remain 2m 47s) Loss: 0.0028(0.0031) Grad: 109746.9766  LR: 0.00000072  \n",
            "Epoch: [4][300/870] Elapsed 1m 25s (remain 2m 41s) Loss: 0.0005(0.0031) Grad: 63947.1289  LR: 0.00000072  \n",
            "Epoch: [4][320/870] Elapsed 1m 31s (remain 2m 35s) Loss: 0.0004(0.0031) Grad: 52857.7773  LR: 0.00000072  \n",
            "Epoch: [4][340/870] Elapsed 1m 36s (remain 2m 30s) Loss: 0.0012(0.0030) Grad: 73810.2812  LR: 0.00000071  \n",
            "Epoch: [4][360/870] Elapsed 1m 42s (remain 2m 24s) Loss: 0.0022(0.0029) Grad: 119118.3906  LR: 0.00000071  \n",
            "Epoch: [4][380/870] Elapsed 1m 48s (remain 2m 18s) Loss: 0.0013(0.0029) Grad: 79746.3125  LR: 0.00000071  \n",
            "Epoch: [4][400/870] Elapsed 1m 53s (remain 2m 12s) Loss: 0.0005(0.0028) Grad: 50503.9180  LR: 0.00000071  \n",
            "Epoch: [4][420/870] Elapsed 1m 59s (remain 2m 6s) Loss: 0.0003(0.0028) Grad: 36516.8242  LR: 0.00000071  \n",
            "Epoch: [4][440/870] Elapsed 2m 4s (remain 2m 1s) Loss: 0.0026(0.0028) Grad: 113851.2031  LR: 0.00000070  \n",
            "Epoch: [4][460/870] Elapsed 2m 10s (remain 1m 55s) Loss: 0.0040(0.0028) Grad: 200931.7344  LR: 0.00000070  \n",
            "Epoch: [4][480/870] Elapsed 2m 15s (remain 1m 49s) Loss: 0.0006(0.0027) Grad: 47933.3828  LR: 0.00000070  \n",
            "Epoch: [4][500/870] Elapsed 2m 21s (remain 1m 44s) Loss: 0.0018(0.0027) Grad: 100497.8203  LR: 0.00000070  \n",
            "Epoch: [4][520/870] Elapsed 2m 26s (remain 1m 38s) Loss: 0.0010(0.0027) Grad: 84257.2656  LR: 0.00000069  \n",
            "Epoch: [4][540/870] Elapsed 2m 32s (remain 1m 32s) Loss: 0.0022(0.0027) Grad: 84686.0781  LR: 0.00000069  \n",
            "Epoch: [4][560/870] Elapsed 2m 38s (remain 1m 27s) Loss: 0.0007(0.0028) Grad: 67038.8438  LR: 0.00000069  \n",
            "Epoch: [4][580/870] Elapsed 2m 43s (remain 1m 21s) Loss: 0.0030(0.0028) Grad: 130132.3750  LR: 0.00000069  \n",
            "Epoch: [4][600/870] Elapsed 2m 49s (remain 1m 15s) Loss: 0.0013(0.0027) Grad: 77366.3828  LR: 0.00000068  \n",
            "Epoch: [4][620/870] Elapsed 2m 54s (remain 1m 10s) Loss: 0.0018(0.0028) Grad: 92475.2031  LR: 0.00000068  \n",
            "Epoch: [4][640/870] Elapsed 3m 0s (remain 1m 4s) Loss: 0.0012(0.0027) Grad: 75842.3281  LR: 0.00000068  \n",
            "Epoch: [4][660/870] Elapsed 3m 5s (remain 0m 58s) Loss: 0.0014(0.0027) Grad: 120699.5312  LR: 0.00000068  \n",
            "Epoch: [4][680/870] Elapsed 3m 11s (remain 0m 53s) Loss: 0.0005(0.0027) Grad: 42388.8867  LR: 0.00000067  \n",
            "Epoch: [4][700/870] Elapsed 3m 16s (remain 0m 47s) Loss: 0.0033(0.0027) Grad: 117989.0703  LR: 0.00000067  \n",
            "Epoch: [4][720/870] Elapsed 3m 22s (remain 0m 41s) Loss: 0.0005(0.0027) Grad: 32616.6465  LR: 0.00000067  \n",
            "Epoch: [4][740/870] Elapsed 3m 27s (remain 0m 36s) Loss: 0.0018(0.0027) Grad: 93181.4219  LR: 0.00000067  \n",
            "Epoch: [4][760/870] Elapsed 3m 33s (remain 0m 30s) Loss: 0.0006(0.0027) Grad: 54514.9844  LR: 0.00000066  \n",
            "Epoch: [4][780/870] Elapsed 3m 39s (remain 0m 25s) Loss: 0.0017(0.0027) Grad: 85005.6172  LR: 0.00000066  \n",
            "Epoch: [4][800/870] Elapsed 3m 45s (remain 0m 19s) Loss: 0.0005(0.0026) Grad: 35901.7148  LR: 0.00000066  \n",
            "Epoch: [4][820/870] Elapsed 3m 50s (remain 0m 13s) Loss: 0.0002(0.0026) Grad: 23477.8340  LR: 0.00000066  \n",
            "Epoch: [4][840/870] Elapsed 3m 56s (remain 0m 8s) Loss: 0.0006(0.0026) Grad: 53387.2070  LR: 0.00000066  \n",
            "Epoch: [4][860/870] Elapsed 4m 1s (remain 0m 2s) Loss: 0.0008(0.0026) Grad: 60915.5078  LR: 0.00000065  \n",
            "Epoch: [4][869/870] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0011(0.0026) Grad: 64922.3672  LR: 0.00000065  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0875(0.0875) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0964(0.1254) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0468(0.1217) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0587(0.1196) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1129(0.1235) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1011(0.1235) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0755(0.1262) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0026  avg_val_loss: 0.1258  time: 260s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0026  avg_val_loss: 0.1258  time: 260s\n",
            "Epoch 4 - Score: 0.5041  Scores: [0.5375375129190014, 0.5081259083778679, 0.4478713765583822, 0.5212334560678149, 0.5241735553694846, 0.4853737466576682]\n",
            "INFO:__main__:Epoch 4 - Score: 0.5041  Scores: [0.5375375129190014, 0.5081259083778679, 0.4478713765583822, 0.5212334560678149, 0.5241735553694846, 0.4853737466576682]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0846(0.1258) \n",
            "Epoch: [5][0/870] Elapsed 0m 0s (remain 7m 15s) Loss: 0.0008(0.0008) Grad: 68584.8594  LR: 0.00000065  \n",
            "Epoch: [5][20/870] Elapsed 0m 7s (remain 4m 45s) Loss: 0.0002(0.0020) Grad: 28033.0156  LR: 0.00000065  \n",
            "Epoch: [5][40/870] Elapsed 0m 13s (remain 4m 27s) Loss: 0.0015(0.0020) Grad: 93670.9062  LR: 0.00000065  \n",
            "Epoch: [5][60/870] Elapsed 0m 18s (remain 4m 7s) Loss: 0.0007(0.0017) Grad: 50740.9180  LR: 0.00000064  \n",
            "Epoch: [5][80/870] Elapsed 0m 24s (remain 3m 57s) Loss: 0.0004(0.0015) Grad: 47135.3594  LR: 0.00000064  \n",
            "Epoch: [5][100/870] Elapsed 0m 29s (remain 3m 47s) Loss: 0.0001(0.0013) Grad: 18340.6113  LR: 0.00000064  \n",
            "Epoch: [5][120/870] Elapsed 0m 35s (remain 3m 39s) Loss: 0.0005(0.0013) Grad: 48082.6719  LR: 0.00000064  \n",
            "Epoch: [5][140/870] Elapsed 0m 40s (remain 3m 31s) Loss: 0.0019(0.0013) Grad: 96416.2891  LR: 0.00000064  \n",
            "Epoch: [5][160/870] Elapsed 0m 46s (remain 3m 24s) Loss: 0.0053(0.0013) Grad: 187162.2812  LR: 0.00000063  \n",
            "Epoch: [5][180/870] Elapsed 0m 51s (remain 3m 17s) Loss: 0.0003(0.0012) Grad: 30402.0781  LR: 0.00000063  \n",
            "Epoch: [5][200/870] Elapsed 0m 57s (remain 3m 11s) Loss: 0.0007(0.0012) Grad: 58465.4688  LR: 0.00000063  \n",
            "Epoch: [5][220/870] Elapsed 1m 2s (remain 3m 4s) Loss: 0.0006(0.0012) Grad: 54910.8242  LR: 0.00000063  \n",
            "Epoch: [5][240/870] Elapsed 1m 8s (remain 2m 58s) Loss: 0.0005(0.0013) Grad: 42746.2695  LR: 0.00000062  \n",
            "Epoch: [5][260/870] Elapsed 1m 13s (remain 2m 52s) Loss: 0.0003(0.0015) Grad: 32137.5938  LR: 0.00000062  \n",
            "Epoch: [5][280/870] Elapsed 1m 19s (remain 2m 46s) Loss: 0.0007(0.0015) Grad: 63969.2227  LR: 0.00000062  \n",
            "Epoch: [5][300/870] Elapsed 1m 25s (remain 2m 41s) Loss: 0.0003(0.0015) Grad: 41360.7109  LR: 0.00000062  \n",
            "Epoch: [5][320/870] Elapsed 1m 31s (remain 2m 36s) Loss: 0.0002(0.0015) Grad: 35798.7227  LR: 0.00000061  \n",
            "Epoch: [5][340/870] Elapsed 1m 36s (remain 2m 30s) Loss: 0.0005(0.0017) Grad: 40210.7344  LR: 0.00000061  \n",
            "Epoch: [5][360/870] Elapsed 1m 42s (remain 2m 24s) Loss: 0.0006(0.0018) Grad: 55594.1758  LR: 0.00000061  \n",
            "Epoch: [5][380/870] Elapsed 1m 47s (remain 2m 18s) Loss: 0.0024(0.0018) Grad: 108957.0234  LR: 0.00000061  \n",
            "Epoch: [5][400/870] Elapsed 1m 53s (remain 2m 12s) Loss: 0.0046(0.0018) Grad: 173859.7812  LR: 0.00000061  \n",
            "Epoch: [5][420/870] Elapsed 1m 58s (remain 2m 6s) Loss: 0.0011(0.0018) Grad: 85477.0156  LR: 0.00000060  \n",
            "Epoch: [5][440/870] Elapsed 2m 4s (remain 2m 0s) Loss: 0.0001(0.0019) Grad: 15733.6152  LR: 0.00000060  \n",
            "Epoch: [5][460/870] Elapsed 2m 10s (remain 1m 56s) Loss: 0.0016(0.0020) Grad: 91431.2500  LR: 0.00000060  \n",
            "Epoch: [5][480/870] Elapsed 2m 16s (remain 1m 50s) Loss: 0.0001(0.0020) Grad: 15767.2666  LR: 0.00000060  \n",
            "Epoch: [5][500/870] Elapsed 2m 21s (remain 1m 44s) Loss: 0.0009(0.0019) Grad: 66024.7109  LR: 0.00000059  \n",
            "Epoch: [5][520/870] Elapsed 2m 27s (remain 1m 38s) Loss: 0.0011(0.0020) Grad: 34166.4453  LR: 0.00000059  \n",
            "Epoch: [5][540/870] Elapsed 2m 32s (remain 1m 33s) Loss: 0.0018(0.0020) Grad: 45103.6562  LR: 0.00000059  \n",
            "Epoch: [5][560/870] Elapsed 2m 38s (remain 1m 27s) Loss: 0.0003(0.0020) Grad: 16758.1875  LR: 0.00000059  \n",
            "Epoch: [5][580/870] Elapsed 2m 44s (remain 1m 21s) Loss: 0.0020(0.0020) Grad: 55864.7773  LR: 0.00000058  \n",
            "Epoch: [5][600/870] Elapsed 2m 49s (remain 1m 15s) Loss: 0.0001(0.0019) Grad: 14752.6191  LR: 0.00000058  \n",
            "Epoch: [5][620/870] Elapsed 2m 55s (remain 1m 10s) Loss: 0.0014(0.0019) Grad: 46613.6484  LR: 0.00000058  \n",
            "Epoch: [5][640/870] Elapsed 3m 0s (remain 1m 4s) Loss: 0.0051(0.0020) Grad: 83808.4609  LR: 0.00000058  \n",
            "Epoch: [5][660/870] Elapsed 3m 6s (remain 0m 58s) Loss: 0.0029(0.0020) Grad: 59336.5547  LR: 0.00000058  \n",
            "Epoch: [5][680/870] Elapsed 3m 11s (remain 0m 53s) Loss: 0.0003(0.0020) Grad: 16862.9473  LR: 0.00000057  \n",
            "Epoch: [5][700/870] Elapsed 3m 17s (remain 0m 47s) Loss: 0.0043(0.0020) Grad: 87538.0547  LR: 0.00000057  \n",
            "Epoch: [5][720/870] Elapsed 3m 22s (remain 0m 41s) Loss: 0.0010(0.0019) Grad: 36053.7031  LR: 0.00000057  \n",
            "Epoch: [5][740/870] Elapsed 3m 27s (remain 0m 36s) Loss: 0.0007(0.0019) Grad: 22609.3535  LR: 0.00000057  \n",
            "Epoch: [5][760/870] Elapsed 3m 33s (remain 0m 30s) Loss: 0.0006(0.0019) Grad: 27996.1191  LR: 0.00000056  \n",
            "Epoch: [5][780/870] Elapsed 3m 38s (remain 0m 24s) Loss: 0.0013(0.0019) Grad: 42819.2070  LR: 0.00000056  \n",
            "Epoch: [5][800/870] Elapsed 3m 44s (remain 0m 19s) Loss: 0.0009(0.0019) Grad: 35955.6133  LR: 0.00000056  \n",
            "Epoch: [5][820/870] Elapsed 3m 49s (remain 0m 13s) Loss: 0.0013(0.0019) Grad: 50551.6055  LR: 0.00000056  \n",
            "Epoch: [5][840/870] Elapsed 3m 55s (remain 0m 8s) Loss: 0.0043(0.0019) Grad: 77018.0938  LR: 0.00000055  \n",
            "Epoch: [5][860/870] Elapsed 4m 1s (remain 0m 2s) Loss: 0.0004(0.0019) Grad: 21261.9551  LR: 0.00000055  \n",
            "Epoch: [5][869/870] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0046(0.0019) Grad: 88863.6875  LR: 0.00000055  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 38s) Loss: 0.0915(0.0915) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0912(0.1251) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0503(0.1217) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0585(0.1195) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1164(0.1247) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1069(0.1248) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0805(0.1274) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0019  avg_val_loss: 0.1271  time: 259s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0019  avg_val_loss: 0.1271  time: 259s\n",
            "Epoch 5 - Score: 0.5069  Scores: [0.5415299644681293, 0.5104307310000584, 0.4511421515327618, 0.5245109830272339, 0.5277811496239864, 0.48630310752536343]\n",
            "INFO:__main__:Epoch 5 - Score: 0.5069  Scores: [0.5415299644681293, 0.5104307310000584, 0.4511421515327618, 0.5245109830272339, 0.5277811496239864, 0.48630310752536343]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0893(0.1271) \n",
            "(40, 6) Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Index(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold'], dtype='object')\n",
            "Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Epoch: [1][0/910] Elapsed 0m 0s (remain 7m 42s) Loss: 0.0013(0.0013) Grad: 87811.2578  LR: 0.00000055  \n",
            "Epoch: [1][20/910] Elapsed 0m 6s (remain 4m 47s) Loss: 0.0007(0.0019) Grad: 50958.0625  LR: 0.00000055  \n",
            "Epoch: [1][40/910] Elapsed 0m 12s (remain 4m 22s) Loss: 0.0007(0.0029) Grad: 62810.7617  LR: 0.00000055  \n",
            "Epoch: [1][60/910] Elapsed 0m 17s (remain 4m 8s) Loss: 0.0004(0.0023) Grad: 35490.8320  LR: 0.00000054  \n",
            "Epoch: [1][80/910] Elapsed 0m 23s (remain 3m 59s) Loss: 0.0007(0.0020) Grad: 54789.0586  LR: 0.00000054  \n",
            "Epoch: [1][100/910] Elapsed 0m 28s (remain 3m 51s) Loss: 0.0004(0.0018) Grad: 44680.7969  LR: 0.00000054  \n",
            "Epoch: [1][120/910] Elapsed 0m 34s (remain 3m 43s) Loss: 0.0014(0.0016) Grad: 97529.8906  LR: 0.00000054  \n",
            "Epoch: [1][140/910] Elapsed 0m 39s (remain 3m 37s) Loss: 0.0004(0.0015) Grad: 44438.3477  LR: 0.00000054  \n",
            "Epoch: [1][160/910] Elapsed 0m 45s (remain 3m 30s) Loss: 0.0124(0.0015) Grad: 220589.9688  LR: 0.00000053  \n",
            "Epoch: [1][180/910] Elapsed 0m 50s (remain 3m 24s) Loss: 0.0006(0.0014) Grad: 54786.5664  LR: 0.00000053  \n",
            "Epoch: [1][200/910] Elapsed 0m 56s (remain 3m 17s) Loss: 0.0014(0.0014) Grad: 96520.4844  LR: 0.00000053  \n",
            "Epoch: [1][220/910] Elapsed 1m 1s (remain 3m 11s) Loss: 0.0014(0.0014) Grad: 88394.4766  LR: 0.00000053  \n",
            "Epoch: [1][240/910] Elapsed 1m 7s (remain 3m 7s) Loss: 0.0003(0.0015) Grad: 35315.0156  LR: 0.00000052  \n",
            "Epoch: [1][260/910] Elapsed 1m 12s (remain 3m 1s) Loss: 0.0015(0.0014) Grad: 82235.6016  LR: 0.00000052  \n",
            "Epoch: [1][280/910] Elapsed 1m 18s (remain 2m 56s) Loss: 0.0026(0.0014) Grad: 144576.2031  LR: 0.00000052  \n",
            "Epoch: [1][300/910] Elapsed 1m 24s (remain 2m 50s) Loss: 0.0020(0.0014) Grad: 107634.2891  LR: 0.00000052  \n",
            "Epoch: [1][320/910] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0015(0.0014) Grad: 85478.8203  LR: 0.00000052  \n",
            "Epoch: [1][340/910] Elapsed 1m 34s (remain 2m 38s) Loss: 0.0006(0.0013) Grad: 65415.8789  LR: 0.00000051  \n",
            "Epoch: [1][360/910] Elapsed 1m 40s (remain 2m 32s) Loss: 0.0002(0.0013) Grad: 20488.3594  LR: 0.00000051  \n",
            "Epoch: [1][380/910] Elapsed 1m 45s (remain 2m 26s) Loss: 0.0011(0.0013) Grad: 88481.8047  LR: 0.00000051  \n",
            "Epoch: [1][400/910] Elapsed 1m 51s (remain 2m 21s) Loss: 0.0020(0.0013) Grad: 127680.0938  LR: 0.00000051  \n",
            "Epoch: [1][420/910] Elapsed 1m 56s (remain 2m 15s) Loss: 0.0011(0.0015) Grad: 74557.0625  LR: 0.00000050  \n",
            "Epoch: [1][440/910] Elapsed 2m 2s (remain 2m 10s) Loss: 0.0010(0.0014) Grad: 82494.4609  LR: 0.00000050  \n",
            "Epoch: [1][460/910] Elapsed 2m 9s (remain 2m 6s) Loss: 0.0019(0.0014) Grad: 107297.0078  LR: 0.00000050  \n",
            "Epoch: [1][480/910] Elapsed 2m 15s (remain 2m 0s) Loss: 0.0001(0.0014) Grad: 16617.4434  LR: 0.00000050  \n",
            "Epoch: [1][500/910] Elapsed 2m 20s (remain 1m 54s) Loss: 0.0011(0.0014) Grad: 65821.8984  LR: 0.00000050  \n",
            "Epoch: [1][520/910] Elapsed 2m 26s (remain 1m 49s) Loss: 0.0002(0.0014) Grad: 32501.7227  LR: 0.00000049  \n",
            "Epoch: [1][540/910] Elapsed 2m 31s (remain 1m 43s) Loss: 0.0013(0.0014) Grad: 74361.3047  LR: 0.00000049  \n",
            "Epoch: [1][560/910] Elapsed 2m 37s (remain 1m 37s) Loss: 0.0017(0.0014) Grad: 85074.8516  LR: 0.00000049  \n",
            "Epoch: [1][580/910] Elapsed 2m 42s (remain 1m 32s) Loss: 0.0003(0.0014) Grad: 40104.3945  LR: 0.00000049  \n",
            "Epoch: [1][600/910] Elapsed 2m 47s (remain 1m 26s) Loss: 0.0013(0.0014) Grad: 101737.5234  LR: 0.00000049  \n",
            "Epoch: [1][620/910] Elapsed 2m 53s (remain 1m 20s) Loss: 0.0007(0.0014) Grad: 39318.9961  LR: 0.00000048  \n",
            "Epoch: [1][640/910] Elapsed 2m 59s (remain 1m 15s) Loss: 0.0006(0.0014) Grad: 54759.4766  LR: 0.00000048  \n",
            "Epoch: [1][660/910] Elapsed 3m 5s (remain 1m 9s) Loss: 0.0003(0.0014) Grad: 38765.7266  LR: 0.00000048  \n",
            "Epoch: [1][680/910] Elapsed 3m 10s (remain 1m 4s) Loss: 0.0003(0.0015) Grad: 42855.7695  LR: 0.00000048  \n",
            "Epoch: [1][700/910] Elapsed 3m 15s (remain 0m 58s) Loss: 0.0015(0.0015) Grad: 100971.1797  LR: 0.00000047  \n",
            "Epoch: [1][720/910] Elapsed 3m 21s (remain 0m 52s) Loss: 0.0006(0.0015) Grad: 59125.0586  LR: 0.00000047  \n",
            "Epoch: [1][740/910] Elapsed 3m 27s (remain 0m 47s) Loss: 0.0040(0.0015) Grad: 179072.0469  LR: 0.00000047  \n",
            "Epoch: [1][760/910] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0001(0.0014) Grad: 22533.7480  LR: 0.00000047  \n",
            "Epoch: [1][780/910] Elapsed 3m 38s (remain 0m 36s) Loss: 0.0017(0.0014) Grad: 117950.3359  LR: 0.00000047  \n",
            "Epoch: [1][800/910] Elapsed 3m 44s (remain 0m 30s) Loss: 0.0006(0.0014) Grad: 52617.3438  LR: 0.00000046  \n",
            "Epoch: [1][820/910] Elapsed 3m 49s (remain 0m 24s) Loss: 0.0007(0.0014) Grad: 63138.9023  LR: 0.00000046  \n",
            "Epoch: [1][840/910] Elapsed 3m 55s (remain 0m 19s) Loss: 0.0009(0.0014) Grad: 71258.5156  LR: 0.00000046  \n",
            "Epoch: [1][860/910] Elapsed 4m 0s (remain 0m 13s) Loss: 0.0013(0.0014) Grad: 75369.6250  LR: 0.00000046  \n",
            "Epoch: [1][880/910] Elapsed 4m 6s (remain 0m 8s) Loss: 0.0003(0.0014) Grad: 45176.0547  LR: 0.00000046  \n",
            "Epoch: [1][900/910] Elapsed 4m 11s (remain 0m 2s) Loss: 0.0008(0.0014) Grad: 66920.0469  LR: 0.00000045  \n",
            "Epoch: [1][909/910] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0009(0.0014) Grad: 65664.3281  LR: 0.00000045  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0916(0.0916) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0966(0.1255) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0502(0.1221) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0628(0.1208) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1134(0.1253) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1005(0.1256) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0840(0.1278) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0014  avg_val_loss: 0.1274  time: 270s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0014  avg_val_loss: 0.1274  time: 270s\n",
            "Epoch 1 - Score: 0.5074  Scores: [0.5420384559929684, 0.5138704330863841, 0.4505294083432854, 0.524291415824992, 0.5259609232019423, 0.4878848291923165]\n",
            "INFO:__main__:Epoch 1 - Score: 0.5074  Scores: [0.5420384559929684, 0.5138704330863841, 0.4505294083432854, 0.524291415824992, 0.5259609232019423, 0.4878848291923165]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0860(0.1274) \n",
            "Epoch: [2][0/910] Elapsed 0m 0s (remain 7m 24s) Loss: 0.0001(0.0001) Grad: 22560.4336  LR: 0.00000045  \n",
            "Epoch: [2][20/910] Elapsed 0m 5s (remain 4m 13s) Loss: 0.0002(0.0005) Grad: 39990.4805  LR: 0.00000045  \n",
            "Epoch: [2][40/910] Elapsed 0m 11s (remain 4m 1s) Loss: 0.0004(0.0006) Grad: 47840.4531  LR: 0.00000045  \n",
            "Epoch: [2][60/910] Elapsed 0m 17s (remain 3m 57s) Loss: 0.0003(0.0006) Grad: 32876.3594  LR: 0.00000045  \n",
            "Epoch: [2][80/910] Elapsed 0m 22s (remain 3m 51s) Loss: 0.0006(0.0009) Grad: 62708.9844  LR: 0.00000044  \n",
            "Epoch: [2][100/910] Elapsed 0m 28s (remain 3m 45s) Loss: 0.0002(0.0008) Grad: 26697.3770  LR: 0.00000044  \n",
            "Epoch: [2][120/910] Elapsed 0m 33s (remain 3m 39s) Loss: 0.0004(0.0007) Grad: 41745.3672  LR: 0.00000044  \n",
            "Epoch: [2][140/910] Elapsed 0m 39s (remain 3m 33s) Loss: 0.0002(0.0007) Grad: 23953.1191  LR: 0.00000044  \n",
            "Epoch: [2][160/910] Elapsed 0m 45s (remain 3m 29s) Loss: 0.0022(0.0007) Grad: 131051.0469  LR: 0.00000044  \n",
            "Epoch: [2][180/910] Elapsed 0m 50s (remain 3m 24s) Loss: 0.0001(0.0007) Grad: 16082.6113  LR: 0.00000043  \n",
            "Epoch: [2][200/910] Elapsed 0m 56s (remain 3m 18s) Loss: 0.0008(0.0007) Grad: 63886.0469  LR: 0.00000043  \n",
            "Epoch: [2][220/910] Elapsed 1m 1s (remain 3m 12s) Loss: 0.0002(0.0008) Grad: 24442.4629  LR: 0.00000043  \n",
            "Epoch: [2][240/910] Elapsed 1m 7s (remain 3m 7s) Loss: 0.0002(0.0008) Grad: 31065.9199  LR: 0.00000043  \n",
            "Epoch: [2][260/910] Elapsed 1m 12s (remain 3m 1s) Loss: 0.0007(0.0008) Grad: 55994.2031  LR: 0.00000043  \n",
            "Epoch: [2][280/910] Elapsed 1m 18s (remain 2m 55s) Loss: 0.0003(0.0008) Grad: 45699.1445  LR: 0.00000042  \n",
            "Epoch: [2][300/910] Elapsed 1m 24s (remain 2m 49s) Loss: 0.0013(0.0008) Grad: 104631.0078  LR: 0.00000042  \n",
            "Epoch: [2][320/910] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0002(0.0008) Grad: 31013.9785  LR: 0.00000042  \n",
            "Epoch: [2][340/910] Elapsed 1m 35s (remain 2m 38s) Loss: 0.0012(0.0008) Grad: 91377.7578  LR: 0.00000042  \n",
            "Epoch: [2][360/910] Elapsed 1m 41s (remain 2m 34s) Loss: 0.0003(0.0008) Grad: 36498.4023  LR: 0.00000042  \n",
            "Epoch: [2][380/910] Elapsed 1m 47s (remain 2m 28s) Loss: 0.0503(0.0009) Grad: 528824.1250  LR: 0.00000041  \n",
            "Epoch: [2][400/910] Elapsed 1m 52s (remain 2m 23s) Loss: 0.0002(0.0009) Grad: 33548.3281  LR: 0.00000041  \n",
            "Epoch: [2][420/910] Elapsed 1m 58s (remain 2m 17s) Loss: 0.0003(0.0009) Grad: 34175.5352  LR: 0.00000041  \n",
            "Epoch: [2][440/910] Elapsed 2m 3s (remain 2m 11s) Loss: 0.0003(0.0008) Grad: 39782.7188  LR: 0.00000041  \n",
            "Epoch: [2][460/910] Elapsed 2m 9s (remain 2m 5s) Loss: 0.0002(0.0009) Grad: 28640.0254  LR: 0.00000041  \n",
            "Epoch: [2][480/910] Elapsed 2m 14s (remain 2m 0s) Loss: 0.0006(0.0008) Grad: 49647.3008  LR: 0.00000040  \n",
            "Epoch: [2][500/910] Elapsed 2m 20s (remain 1m 54s) Loss: 0.0002(0.0008) Grad: 30582.2441  LR: 0.00000040  \n",
            "Epoch: [2][520/910] Elapsed 2m 25s (remain 1m 48s) Loss: 0.0003(0.0008) Grad: 42882.6016  LR: 0.00000040  \n",
            "Epoch: [2][540/910] Elapsed 2m 31s (remain 1m 43s) Loss: 0.0007(0.0008) Grad: 93908.9297  LR: 0.00000040  \n",
            "Epoch: [2][560/910] Elapsed 2m 37s (remain 1m 38s) Loss: 0.0029(0.0008) Grad: 207022.5469  LR: 0.00000039  \n",
            "Epoch: [2][580/910] Elapsed 2m 43s (remain 1m 32s) Loss: 0.0005(0.0008) Grad: 53431.7148  LR: 0.00000039  \n",
            "Epoch: [2][600/910] Elapsed 2m 49s (remain 1m 26s) Loss: 0.0012(0.0008) Grad: 77074.5391  LR: 0.00000039  \n",
            "Epoch: [2][620/910] Elapsed 2m 54s (remain 1m 21s) Loss: 0.0025(0.0008) Grad: 129639.8281  LR: 0.00000039  \n",
            "Epoch: [2][640/910] Elapsed 3m 0s (remain 1m 15s) Loss: 0.0001(0.0008) Grad: 18721.3770  LR: 0.00000039  \n",
            "Epoch: [2][660/910] Elapsed 3m 5s (remain 1m 9s) Loss: 0.0002(0.0008) Grad: 26181.0879  LR: 0.00000039  \n",
            "Epoch: [2][680/910] Elapsed 3m 11s (remain 1m 4s) Loss: 0.0002(0.0009) Grad: 36568.1641  LR: 0.00000038  \n",
            "Epoch: [2][700/910] Elapsed 3m 16s (remain 0m 58s) Loss: 0.0002(0.0009) Grad: 34638.7773  LR: 0.00000038  \n",
            "Epoch: [2][720/910] Elapsed 3m 22s (remain 0m 53s) Loss: 0.0080(0.0009) Grad: 137417.9062  LR: 0.00000038  \n",
            "Epoch: [2][740/910] Elapsed 3m 27s (remain 0m 47s) Loss: 0.0203(0.0009) Grad: 339972.7500  LR: 0.00000038  \n",
            "Epoch: [2][760/910] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0080(0.0010) Grad: 202067.3750  LR: 0.00000038  \n",
            "Epoch: [2][780/910] Elapsed 3m 38s (remain 0m 36s) Loss: 0.0012(0.0010) Grad: 79690.4688  LR: 0.00000037  \n",
            "Epoch: [2][800/910] Elapsed 3m 44s (remain 0m 30s) Loss: 0.0008(0.0010) Grad: 60371.2617  LR: 0.00000037  \n",
            "Epoch: [2][820/910] Elapsed 3m 50s (remain 0m 24s) Loss: 0.0004(0.0010) Grad: 36756.8320  LR: 0.00000037  \n",
            "Epoch: [2][840/910] Elapsed 3m 55s (remain 0m 19s) Loss: 0.0006(0.0010) Grad: 74425.3359  LR: 0.00000037  \n",
            "Epoch: [2][860/910] Elapsed 4m 1s (remain 0m 13s) Loss: 0.0005(0.0009) Grad: 49131.3555  LR: 0.00000037  \n",
            "Epoch: [2][880/910] Elapsed 4m 7s (remain 0m 8s) Loss: 0.0040(0.0010) Grad: 126222.7891  LR: 0.00000036  \n",
            "Epoch: [2][900/910] Elapsed 4m 12s (remain 0m 2s) Loss: 0.0013(0.0010) Grad: 102174.7812  LR: 0.00000036  \n",
            "Epoch: [2][909/910] Elapsed 4m 15s (remain 0m 0s) Loss: 0.0004(0.0009) Grad: 43758.9023  LR: 0.00000036  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0916(0.0916) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0968(0.1258) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0539(0.1230) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0620(0.1211) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1168(0.1257) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1067(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0913(0.1282) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0009  avg_val_loss: 0.1277  time: 271s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0009  avg_val_loss: 0.1277  time: 271s\n",
            "Epoch 2 - Score: 0.5081  Scores: [0.54608867919998, 0.5102387692945188, 0.45202881649346777, 0.5247050837830798, 0.5249763636699456, 0.4905076972501754]\n",
            "INFO:__main__:Epoch 2 - Score: 0.5081  Scores: [0.54608867919998, 0.5102387692945188, 0.45202881649346777, 0.5247050837830798, 0.5249763636699456, 0.4905076972501754]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0843(0.1277) \n",
            "Epoch: [3][0/910] Elapsed 0m 0s (remain 7m 32s) Loss: 0.0005(0.0005) Grad: 46040.3125  LR: 0.00000036  \n",
            "Epoch: [3][20/910] Elapsed 0m 5s (remain 4m 12s) Loss: 0.0002(0.0004) Grad: 34228.4062  LR: 0.00000036  \n",
            "Epoch: [3][40/910] Elapsed 0m 11s (remain 4m 3s) Loss: 0.0007(0.0004) Grad: 73464.1719  LR: 0.00000036  \n",
            "Epoch: [3][60/910] Elapsed 0m 16s (remain 3m 56s) Loss: 0.0002(0.0009) Grad: 24574.7871  LR: 0.00000035  \n",
            "Epoch: [3][80/910] Elapsed 0m 22s (remain 3m 52s) Loss: 0.0001(0.0009) Grad: 21368.6406  LR: 0.00000035  \n",
            "Epoch: [3][100/910] Elapsed 0m 28s (remain 3m 45s) Loss: 0.0002(0.0008) Grad: 39123.8594  LR: 0.00000035  \n",
            "Epoch: [3][120/910] Elapsed 0m 33s (remain 3m 39s) Loss: 0.0002(0.0007) Grad: 32097.6992  LR: 0.00000035  \n",
            "Epoch: [3][140/910] Elapsed 0m 39s (remain 3m 32s) Loss: 0.0007(0.0006) Grad: 73597.9453  LR: 0.00000035  \n",
            "Epoch: [3][160/910] Elapsed 0m 44s (remain 3m 27s) Loss: 0.0000(0.0006) Grad: 5948.8438  LR: 0.00000035  \n",
            "Epoch: [3][180/910] Elapsed 0m 49s (remain 3m 21s) Loss: 0.0008(0.0006) Grad: 68899.3672  LR: 0.00000034  \n",
            "Epoch: [3][200/910] Elapsed 0m 55s (remain 3m 15s) Loss: 0.0001(0.0008) Grad: 20721.4258  LR: 0.00000034  \n",
            "Epoch: [3][220/910] Elapsed 1m 1s (remain 3m 10s) Loss: 0.0003(0.0007) Grad: 46598.3945  LR: 0.00000034  \n",
            "Epoch: [3][240/910] Elapsed 1m 6s (remain 3m 5s) Loss: 0.0001(0.0007) Grad: 16540.0156  LR: 0.00000034  \n",
            "Epoch: [3][260/910] Elapsed 1m 12s (remain 2m 59s) Loss: 0.0002(0.0007) Grad: 26532.7422  LR: 0.00000034  \n",
            "Epoch: [3][280/910] Elapsed 1m 18s (remain 2m 55s) Loss: 0.0003(0.0007) Grad: 46234.6445  LR: 0.00000033  \n",
            "Epoch: [3][300/910] Elapsed 1m 24s (remain 2m 50s) Loss: 0.0007(0.0006) Grad: 54009.7461  LR: 0.00000033  \n",
            "Epoch: [3][320/910] Elapsed 1m 29s (remain 2m 44s) Loss: 0.0001(0.0007) Grad: 18919.3359  LR: 0.00000033  \n",
            "Epoch: [3][340/910] Elapsed 1m 34s (remain 2m 38s) Loss: 0.0002(0.0007) Grad: 22930.5059  LR: 0.00000033  \n",
            "Epoch: [3][360/910] Elapsed 1m 40s (remain 2m 32s) Loss: 0.0032(0.0007) Grad: 121127.6328  LR: 0.00000033  \n",
            "Epoch: [3][380/910] Elapsed 1m 46s (remain 2m 27s) Loss: 0.0002(0.0007) Grad: 30822.2324  LR: 0.00000032  \n",
            "Epoch: [3][400/910] Elapsed 1m 51s (remain 2m 21s) Loss: 0.0005(0.0008) Grad: 55268.2852  LR: 0.00000032  \n",
            "Epoch: [3][420/910] Elapsed 1m 56s (remain 2m 15s) Loss: 0.0001(0.0008) Grad: 24768.2754  LR: 0.00000032  \n",
            "Epoch: [3][440/910] Elapsed 2m 2s (remain 2m 10s) Loss: 0.0001(0.0008) Grad: 17623.6699  LR: 0.00000032  \n",
            "Epoch: [3][460/910] Elapsed 2m 8s (remain 2m 4s) Loss: 0.0001(0.0007) Grad: 12692.2881  LR: 0.00000032  \n",
            "Epoch: [3][480/910] Elapsed 2m 13s (remain 1m 59s) Loss: 0.0003(0.0007) Grad: 27025.8984  LR: 0.00000032  \n",
            "Epoch: [3][500/910] Elapsed 2m 19s (remain 1m 53s) Loss: 0.0002(0.0008) Grad: 35383.3633  LR: 0.00000031  \n",
            "Epoch: [3][520/910] Elapsed 2m 24s (remain 1m 48s) Loss: 0.0002(0.0008) Grad: 32759.3535  LR: 0.00000031  \n",
            "Epoch: [3][540/910] Elapsed 2m 30s (remain 1m 42s) Loss: 0.0016(0.0008) Grad: 118569.8984  LR: 0.00000031  \n",
            "Epoch: [3][560/910] Elapsed 2m 35s (remain 1m 36s) Loss: 0.0002(0.0007) Grad: 35445.1875  LR: 0.00000031  \n",
            "Epoch: [3][580/910] Elapsed 2m 41s (remain 1m 31s) Loss: 0.0013(0.0008) Grad: 72201.1641  LR: 0.00000031  \n",
            "Epoch: [3][600/910] Elapsed 2m 47s (remain 1m 26s) Loss: 0.0002(0.0008) Grad: 39726.1641  LR: 0.00000030  \n",
            "Epoch: [3][620/910] Elapsed 2m 52s (remain 1m 20s) Loss: 0.0007(0.0008) Grad: 57949.8477  LR: 0.00000030  \n",
            "Epoch: [3][640/910] Elapsed 2m 58s (remain 1m 14s) Loss: 0.0002(0.0008) Grad: 37159.7109  LR: 0.00000030  \n",
            "Epoch: [3][660/910] Elapsed 3m 3s (remain 1m 9s) Loss: 0.0008(0.0008) Grad: 74679.4766  LR: 0.00000030  \n",
            "Epoch: [3][680/910] Elapsed 3m 9s (remain 1m 3s) Loss: 0.0002(0.0008) Grad: 39709.8320  LR: 0.00000030  \n",
            "Epoch: [3][700/910] Elapsed 3m 14s (remain 0m 58s) Loss: 0.0024(0.0007) Grad: 87102.1641  LR: 0.00000030  \n",
            "Epoch: [3][720/910] Elapsed 3m 20s (remain 0m 52s) Loss: 0.0001(0.0007) Grad: 33469.2812  LR: 0.00000029  \n",
            "Epoch: [3][740/910] Elapsed 3m 25s (remain 0m 46s) Loss: 0.0024(0.0008) Grad: 124154.1328  LR: 0.00000029  \n",
            "Epoch: [3][760/910] Elapsed 3m 31s (remain 0m 41s) Loss: 0.0002(0.0008) Grad: 30263.2227  LR: 0.00000029  \n",
            "Epoch: [3][780/910] Elapsed 3m 37s (remain 0m 35s) Loss: 0.0000(0.0007) Grad: 13952.6172  LR: 0.00000029  \n",
            "Epoch: [3][800/910] Elapsed 3m 43s (remain 0m 30s) Loss: 0.0001(0.0007) Grad: 25368.7637  LR: 0.00000029  \n",
            "Epoch: [3][820/910] Elapsed 3m 48s (remain 0m 24s) Loss: 0.0004(0.0007) Grad: 48746.7656  LR: 0.00000028  \n",
            "Epoch: [3][840/910] Elapsed 3m 54s (remain 0m 19s) Loss: 0.0007(0.0007) Grad: 67342.5859  LR: 0.00000028  \n",
            "Epoch: [3][860/910] Elapsed 4m 0s (remain 0m 13s) Loss: 0.0004(0.0007) Grad: 52826.1328  LR: 0.00000028  \n",
            "Epoch: [3][880/910] Elapsed 4m 6s (remain 0m 8s) Loss: 0.0001(0.0007) Grad: 18996.1426  LR: 0.00000028  \n",
            "Epoch: [3][900/910] Elapsed 4m 11s (remain 0m 2s) Loss: 0.0003(0.0007) Grad: 79337.9531  LR: 0.00000028  \n",
            "Epoch: [3][909/910] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0002(0.0007) Grad: 29424.2559  LR: 0.00000028  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0909(0.0909) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0931(0.1251) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0519(0.1224) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0621(0.1209) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1146(0.1256) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1058(0.1254) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0842(0.1279) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0007  avg_val_loss: 0.1274  time: 270s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0007  avg_val_loss: 0.1274  time: 270s\n",
            "Epoch 3 - Score: 0.5075  Scores: [0.546833676917089, 0.5123772051460811, 0.45126070479923225, 0.5222465266406981, 0.5251932741202482, 0.48717624189361297]\n",
            "INFO:__main__:Epoch 3 - Score: 0.5075  Scores: [0.546833676917089, 0.5123772051460811, 0.45126070479923225, 0.5222465266406981, 0.5251932741202482, 0.48717624189361297]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0855(0.1274) \n",
            "Epoch: [4][0/910] Elapsed 0m 0s (remain 7m 37s) Loss: 0.0001(0.0001) Grad: 16260.3154  LR: 0.00000028  \n",
            "Epoch: [4][20/910] Elapsed 0m 6s (remain 4m 15s) Loss: 0.0001(0.0002) Grad: 35658.7188  LR: 0.00000028  \n",
            "Epoch: [4][40/910] Elapsed 0m 11s (remain 4m 4s) Loss: 0.0002(0.0002) Grad: 41280.6406  LR: 0.00000027  \n",
            "Epoch: [4][60/910] Elapsed 0m 16s (remain 3m 56s) Loss: 0.0002(0.0006) Grad: 26864.6992  LR: 0.00000027  \n",
            "Epoch: [4][80/910] Elapsed 0m 22s (remain 3m 49s) Loss: 0.0003(0.0005) Grad: 49586.3633  LR: 0.00000027  \n",
            "Epoch: [4][100/910] Elapsed 0m 27s (remain 3m 43s) Loss: 0.0003(0.0004) Grad: 40512.7031  LR: 0.00000027  \n",
            "Epoch: [4][120/910] Elapsed 0m 33s (remain 3m 40s) Loss: 0.0002(0.0005) Grad: 43296.9492  LR: 0.00000027  \n",
            "Epoch: [4][140/910] Elapsed 0m 39s (remain 3m 34s) Loss: 0.0001(0.0008) Grad: 11539.4443  LR: 0.00000026  \n",
            "Epoch: [4][160/910] Elapsed 0m 44s (remain 3m 28s) Loss: 0.0001(0.0007) Grad: 26953.1875  LR: 0.00000026  \n",
            "Epoch: [4][180/910] Elapsed 0m 50s (remain 3m 22s) Loss: 0.0004(0.0007) Grad: 48856.9102  LR: 0.00000026  \n",
            "Epoch: [4][200/910] Elapsed 0m 56s (remain 3m 18s) Loss: 0.0004(0.0006) Grad: 54334.7461  LR: 0.00000026  \n",
            "Epoch: [4][220/910] Elapsed 1m 1s (remain 3m 12s) Loss: 0.0002(0.0006) Grad: 34199.9375  LR: 0.00000026  \n",
            "Epoch: [4][240/910] Elapsed 1m 7s (remain 3m 6s) Loss: 0.0008(0.0006) Grad: 55333.8633  LR: 0.00000026  \n",
            "Epoch: [4][260/910] Elapsed 1m 12s (remain 3m 0s) Loss: 0.0001(0.0005) Grad: 27247.2949  LR: 0.00000025  \n",
            "Epoch: [4][280/910] Elapsed 1m 18s (remain 2m 55s) Loss: 0.0004(0.0005) Grad: 39056.6328  LR: 0.00000025  \n",
            "Epoch: [4][300/910] Elapsed 1m 23s (remain 2m 49s) Loss: 0.0002(0.0005) Grad: 28826.0391  LR: 0.00000025  \n",
            "Epoch: [4][320/910] Elapsed 1m 29s (remain 2m 43s) Loss: 0.0004(0.0005) Grad: 51530.1641  LR: 0.00000025  \n",
            "Epoch: [4][340/910] Elapsed 1m 34s (remain 2m 37s) Loss: 0.0003(0.0005) Grad: 37222.9141  LR: 0.00000025  \n",
            "Epoch: [4][360/910] Elapsed 1m 40s (remain 2m 32s) Loss: 0.0001(0.0005) Grad: 15815.4346  LR: 0.00000025  \n",
            "Epoch: [4][380/910] Elapsed 1m 45s (remain 2m 26s) Loss: 0.0002(0.0005) Grad: 30618.3945  LR: 0.00000024  \n",
            "Epoch: [4][400/910] Elapsed 1m 51s (remain 2m 20s) Loss: 0.0002(0.0005) Grad: 36309.7734  LR: 0.00000024  \n",
            "Epoch: [4][420/910] Elapsed 1m 56s (remain 2m 15s) Loss: 0.0003(0.0005) Grad: 40457.8867  LR: 0.00000024  \n",
            "Epoch: [4][440/910] Elapsed 2m 2s (remain 2m 9s) Loss: 0.0001(0.0005) Grad: 29091.7754  LR: 0.00000024  \n",
            "Epoch: [4][460/910] Elapsed 2m 7s (remain 2m 4s) Loss: 0.0001(0.0004) Grad: 13328.5234  LR: 0.00000024  \n",
            "Epoch: [4][480/910] Elapsed 2m 13s (remain 1m 58s) Loss: 0.0002(0.0004) Grad: 37252.6016  LR: 0.00000024  \n",
            "Epoch: [4][500/910] Elapsed 2m 18s (remain 1m 53s) Loss: 0.0008(0.0005) Grad: 62770.9375  LR: 0.00000024  \n",
            "Epoch: [4][520/910] Elapsed 2m 24s (remain 1m 47s) Loss: 0.0001(0.0005) Grad: 19352.3848  LR: 0.00000023  \n",
            "Epoch: [4][540/910] Elapsed 2m 29s (remain 1m 42s) Loss: 0.0001(0.0005) Grad: 22475.5449  LR: 0.00000023  \n",
            "Epoch: [4][560/910] Elapsed 2m 35s (remain 1m 36s) Loss: 0.0002(0.0005) Grad: 34818.7148  LR: 0.00000023  \n",
            "Epoch: [4][580/910] Elapsed 2m 41s (remain 1m 31s) Loss: 0.0003(0.0005) Grad: 55020.2617  LR: 0.00000023  \n",
            "Epoch: [4][600/910] Elapsed 2m 46s (remain 1m 25s) Loss: 0.0001(0.0005) Grad: 18425.1992  LR: 0.00000023  \n",
            "Epoch: [4][620/910] Elapsed 2m 52s (remain 1m 20s) Loss: 0.0001(0.0005) Grad: 30125.8398  LR: 0.00000023  \n",
            "Epoch: [4][640/910] Elapsed 2m 57s (remain 1m 14s) Loss: 0.0002(0.0005) Grad: 33650.5586  LR: 0.00000022  \n",
            "Epoch: [4][660/910] Elapsed 3m 4s (remain 1m 9s) Loss: 0.0000(0.0005) Grad: 16870.7402  LR: 0.00000022  \n",
            "Epoch: [4][680/910] Elapsed 3m 9s (remain 1m 3s) Loss: 0.0005(0.0006) Grad: 37794.4648  LR: 0.00000022  \n",
            "Epoch: [4][700/910] Elapsed 3m 15s (remain 0m 58s) Loss: 0.0001(0.0006) Grad: 26135.1699  LR: 0.00000022  \n",
            "Epoch: [4][720/910] Elapsed 3m 21s (remain 0m 52s) Loss: 0.0001(0.0006) Grad: 25604.2129  LR: 0.00000022  \n",
            "Epoch: [4][740/910] Elapsed 3m 27s (remain 0m 47s) Loss: 0.0001(0.0006) Grad: 27822.8398  LR: 0.00000022  \n",
            "Epoch: [4][760/910] Elapsed 3m 33s (remain 0m 41s) Loss: 0.0002(0.0006) Grad: 32526.5156  LR: 0.00000021  \n",
            "Epoch: [4][780/910] Elapsed 3m 38s (remain 0m 36s) Loss: 0.0001(0.0006) Grad: 21003.9043  LR: 0.00000021  \n",
            "Epoch: [4][800/910] Elapsed 3m 44s (remain 0m 30s) Loss: 0.0001(0.0006) Grad: 26816.4609  LR: 0.00000021  \n",
            "Epoch: [4][820/910] Elapsed 3m 49s (remain 0m 24s) Loss: 0.0001(0.0006) Grad: 13780.0391  LR: 0.00000021  \n",
            "Epoch: [4][840/910] Elapsed 3m 55s (remain 0m 19s) Loss: 0.0001(0.0006) Grad: 19281.9746  LR: 0.00000021  \n",
            "Epoch: [4][860/910] Elapsed 4m 1s (remain 0m 13s) Loss: 0.0001(0.0006) Grad: 14729.7246  LR: 0.00000021  \n",
            "Epoch: [4][880/910] Elapsed 4m 6s (remain 0m 8s) Loss: 0.0001(0.0006) Grad: 18337.5371  LR: 0.00000021  \n",
            "Epoch: [4][900/910] Elapsed 4m 12s (remain 0m 2s) Loss: 0.0001(0.0006) Grad: 16157.0234  LR: 0.00000020  \n",
            "Epoch: [4][909/910] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0013(0.0006) Grad: 77190.7891  LR: 0.00000020  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0926(0.0926) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0949(0.1257) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0520(0.1232) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0615(0.1212) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1145(0.1258) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1052(0.1258) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0839(0.1283) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0006  avg_val_loss: 0.1279  time: 270s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0006  avg_val_loss: 0.1279  time: 270s\n",
            "Epoch 4 - Score: 0.5084  Scores: [0.5468098012109956, 0.5134951764189142, 0.4522369496209764, 0.5246193387755153, 0.5261855164375867, 0.4870069684605603]\n",
            "INFO:__main__:Epoch 4 - Score: 0.5084  Scores: [0.5468098012109956, 0.5134951764189142, 0.4522369496209764, 0.5246193387755153, 0.5261855164375867, 0.4870069684605603]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0854(0.1279) \n",
            "Epoch: [5][0/910] Elapsed 0m 0s (remain 7m 49s) Loss: 0.0002(0.0002) Grad: 31090.4297  LR: 0.00000020  \n",
            "Epoch: [5][20/910] Elapsed 0m 5s (remain 4m 11s) Loss: 0.0001(0.0002) Grad: 26570.3281  LR: 0.00000020  \n",
            "Epoch: [5][40/910] Elapsed 0m 11s (remain 4m 0s) Loss: 0.0002(0.0002) Grad: 24003.5176  LR: 0.00000020  \n",
            "Epoch: [5][60/910] Elapsed 0m 16s (remain 3m 53s) Loss: 0.0000(0.0001) Grad: 5505.4473  LR: 0.00000020  \n",
            "Epoch: [5][80/910] Elapsed 0m 22s (remain 3m 47s) Loss: 0.0002(0.0001) Grad: 42828.9023  LR: 0.00000020  \n",
            "Epoch: [5][100/910] Elapsed 0m 27s (remain 3m 41s) Loss: 0.0001(0.0003) Grad: 29836.9160  LR: 0.00000020  \n",
            "Epoch: [5][120/910] Elapsed 0m 33s (remain 3m 36s) Loss: 0.0001(0.0002) Grad: 19945.4453  LR: 0.00000019  \n",
            "Epoch: [5][140/910] Elapsed 0m 38s (remain 3m 30s) Loss: 0.0001(0.0002) Grad: 26407.9277  LR: 0.00000019  \n",
            "Epoch: [5][160/910] Elapsed 0m 44s (remain 3m 25s) Loss: 0.0000(0.0003) Grad: 8400.1465  LR: 0.00000019  \n",
            "Epoch: [5][180/910] Elapsed 0m 49s (remain 3m 19s) Loss: 0.0001(0.0003) Grad: 26553.1914  LR: 0.00000019  \n",
            "Epoch: [5][200/910] Elapsed 0m 55s (remain 3m 14s) Loss: 0.0001(0.0002) Grad: 26546.6211  LR: 0.00000019  \n",
            "Epoch: [5][220/910] Elapsed 1m 0s (remain 3m 8s) Loss: 0.0001(0.0003) Grad: 15794.9590  LR: 0.00000019  \n",
            "Epoch: [5][240/910] Elapsed 1m 6s (remain 3m 3s) Loss: 0.0002(0.0003) Grad: 40258.0391  LR: 0.00000019  \n",
            "Epoch: [5][260/910] Elapsed 1m 11s (remain 2m 57s) Loss: 0.0001(0.0003) Grad: 15238.3398  LR: 0.00000018  \n",
            "Epoch: [5][280/910] Elapsed 1m 17s (remain 2m 54s) Loss: 0.0001(0.0003) Grad: 35996.0859  LR: 0.00000018  \n",
            "Epoch: [5][300/910] Elapsed 1m 23s (remain 2m 49s) Loss: 0.0000(0.0003) Grad: 7374.2603  LR: 0.00000018  \n",
            "Epoch: [5][320/910] Elapsed 1m 29s (remain 2m 43s) Loss: 0.0000(0.0003) Grad: 12368.7051  LR: 0.00000018  \n",
            "Epoch: [5][340/910] Elapsed 1m 34s (remain 2m 38s) Loss: 0.0002(0.0004) Grad: 29730.7227  LR: 0.00000018  \n",
            "Epoch: [5][360/910] Elapsed 1m 40s (remain 2m 32s) Loss: 0.0000(0.0004) Grad: 6841.2871  LR: 0.00000018  \n",
            "Epoch: [5][380/910] Elapsed 1m 45s (remain 2m 27s) Loss: 0.0001(0.0004) Grad: 25577.0625  LR: 0.00000017  \n",
            "Epoch: [5][400/910] Elapsed 1m 51s (remain 2m 21s) Loss: 0.0001(0.0004) Grad: 28508.1484  LR: 0.00000017  \n",
            "Epoch: [5][420/910] Elapsed 1m 56s (remain 2m 15s) Loss: 0.0000(0.0005) Grad: 17064.0547  LR: 0.00000017  \n",
            "Epoch: [5][440/910] Elapsed 2m 3s (remain 2m 10s) Loss: 0.0002(0.0005) Grad: 34743.6602  LR: 0.00000017  \n",
            "Epoch: [5][460/910] Elapsed 2m 8s (remain 2m 5s) Loss: 0.0000(0.0005) Grad: 8452.7754  LR: 0.00000017  \n",
            "Epoch: [5][480/910] Elapsed 2m 14s (remain 1m 59s) Loss: 0.0001(0.0004) Grad: 20142.5605  LR: 0.00000017  \n",
            "Epoch: [5][500/910] Elapsed 2m 20s (remain 1m 54s) Loss: 0.0000(0.0005) Grad: 14634.0469  LR: 0.00000017  \n",
            "Epoch: [5][520/910] Elapsed 2m 25s (remain 1m 48s) Loss: 0.0004(0.0005) Grad: 57482.2812  LR: 0.00000017  \n",
            "Epoch: [5][540/910] Elapsed 2m 31s (remain 1m 43s) Loss: 0.0005(0.0005) Grad: 53027.2031  LR: 0.00000016  \n",
            "Epoch: [5][560/910] Elapsed 2m 36s (remain 1m 37s) Loss: 0.0000(0.0005) Grad: 10106.7451  LR: 0.00000016  \n",
            "Epoch: [5][580/910] Elapsed 2m 42s (remain 1m 31s) Loss: 0.0000(0.0005) Grad: 11488.7334  LR: 0.00000016  \n",
            "Epoch: [5][600/910] Elapsed 2m 47s (remain 1m 26s) Loss: 0.0001(0.0005) Grad: 18881.4375  LR: 0.00000016  \n",
            "Epoch: [5][620/910] Elapsed 2m 53s (remain 1m 20s) Loss: 0.0001(0.0005) Grad: 25458.9961  LR: 0.00000016  \n",
            "Epoch: [5][640/910] Elapsed 2m 59s (remain 1m 15s) Loss: 0.0001(0.0005) Grad: 18836.5801  LR: 0.00000016  \n",
            "Epoch: [5][660/910] Elapsed 3m 4s (remain 1m 9s) Loss: 0.0000(0.0005) Grad: 11781.3584  LR: 0.00000016  \n",
            "Epoch: [5][680/910] Elapsed 3m 10s (remain 1m 3s) Loss: 0.0001(0.0005) Grad: 15613.8428  LR: 0.00000015  \n",
            "Epoch: [5][700/910] Elapsed 3m 15s (remain 0m 58s) Loss: 0.0001(0.0005) Grad: 14187.8936  LR: 0.00000015  \n",
            "Epoch: [5][720/910] Elapsed 3m 21s (remain 0m 52s) Loss: 0.0001(0.0004) Grad: 24108.5273  LR: 0.00000015  \n",
            "Epoch: [5][740/910] Elapsed 3m 26s (remain 0m 47s) Loss: 0.0001(0.0005) Grad: 22713.6914  LR: 0.00000015  \n",
            "Epoch: [5][760/910] Elapsed 3m 32s (remain 0m 41s) Loss: 0.0001(0.0004) Grad: 19718.9395  LR: 0.00000015  \n",
            "Epoch: [5][780/910] Elapsed 3m 37s (remain 0m 35s) Loss: 0.0001(0.0005) Grad: 12918.0889  LR: 0.00000015  \n",
            "Epoch: [5][800/910] Elapsed 3m 43s (remain 0m 30s) Loss: 0.0001(0.0005) Grad: 15462.4678  LR: 0.00000015  \n",
            "Epoch: [5][820/910] Elapsed 3m 48s (remain 0m 24s) Loss: 0.0001(0.0005) Grad: 17545.9121  LR: 0.00000015  \n",
            "Epoch: [5][840/910] Elapsed 3m 54s (remain 0m 19s) Loss: 0.0000(0.0005) Grad: 12661.2910  LR: 0.00000014  \n",
            "Epoch: [5][860/910] Elapsed 3m 59s (remain 0m 13s) Loss: 0.0000(0.0005) Grad: 17169.9805  LR: 0.00000014  \n",
            "Epoch: [5][880/910] Elapsed 4m 5s (remain 0m 8s) Loss: 0.0001(0.0005) Grad: 23555.2871  LR: 0.00000014  \n",
            "Epoch: [5][900/910] Elapsed 4m 11s (remain 0m 2s) Loss: 0.0002(0.0004) Grad: 33855.2852  LR: 0.00000014  \n",
            "Epoch: [5][909/910] Elapsed 4m 14s (remain 0m 0s) Loss: 0.0001(0.0004) Grad: 18507.7441  LR: 0.00000014  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0909(0.0909) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0942(0.1256) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0504(0.1228) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0613(0.1210) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1163(0.1257) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1066(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0843(0.1283) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0004  avg_val_loss: 0.1278  time: 270s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0004  avg_val_loss: 0.1278  time: 270s\n",
            "Epoch 5 - Score: 0.5083  Scores: [0.5469370568510489, 0.5133744262266249, 0.4526195373043647, 0.5242260109150003, 0.5254554281601501, 0.48735972540309075]\n",
            "INFO:__main__:Epoch 5 - Score: 0.5083  Scores: [0.5469370568510489, 0.5133744262266249, 0.4526195373043647, 0.5242260109150003, 0.5254554281601501, 0.48735972540309075]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0869(0.1278) \n",
            "(40, 6) Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Index(['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold'], dtype='object')\n",
            "Index(['full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], dtype='object')\n",
            "Epoch: [1][0/950] Elapsed 0m 0s (remain 7m 54s) Loss: 0.0000(0.0000) Grad: 11589.6377  LR: 0.00000014  \n",
            "Epoch: [1][20/950] Elapsed 0m 5s (remain 4m 23s) Loss: 0.0000(0.0001) Grad: 16388.7637  LR: 0.00000014  \n",
            "Epoch: [1][40/950] Elapsed 0m 11s (remain 4m 11s) Loss: 0.0001(0.0002) Grad: 16258.2168  LR: 0.00000014  \n",
            "Epoch: [1][60/950] Elapsed 0m 16s (remain 4m 4s) Loss: 0.0002(0.0001) Grad: 39569.8789  LR: 0.00000014  \n",
            "Epoch: [1][80/950] Elapsed 0m 22s (remain 3m 58s) Loss: 0.0001(0.0002) Grad: 16611.9570  LR: 0.00000013  \n",
            "Epoch: [1][100/950] Elapsed 0m 27s (remain 3m 52s) Loss: 0.0002(0.0002) Grad: 31914.1934  LR: 0.00000013  \n",
            "Epoch: [1][120/950] Elapsed 0m 33s (remain 3m 46s) Loss: 0.0000(0.0004) Grad: 15916.5586  LR: 0.00000013  \n",
            "Epoch: [1][140/950] Elapsed 0m 38s (remain 3m 40s) Loss: 0.0000(0.0004) Grad: 8665.7236  LR: 0.00000013  \n",
            "Epoch: [1][160/950] Elapsed 0m 44s (remain 3m 36s) Loss: 0.0001(0.0004) Grad: 12394.3975  LR: 0.00000013  \n",
            "Epoch: [1][180/950] Elapsed 0m 49s (remain 3m 30s) Loss: 0.0000(0.0004) Grad: 9262.8350  LR: 0.00000013  \n",
            "Epoch: [1][200/950] Elapsed 0m 55s (remain 3m 25s) Loss: 0.0000(0.0004) Grad: 21150.5195  LR: 0.00000013  \n",
            "Epoch: [1][220/950] Elapsed 1m 0s (remain 3m 19s) Loss: 0.0000(0.0004) Grad: 15302.1377  LR: 0.00000013  \n",
            "Epoch: [1][240/950] Elapsed 1m 6s (remain 3m 14s) Loss: 0.0000(0.0003) Grad: 11534.5557  LR: 0.00000012  \n",
            "Epoch: [1][260/950] Elapsed 1m 11s (remain 3m 9s) Loss: 0.0004(0.0004) Grad: 45743.8555  LR: 0.00000012  \n",
            "Epoch: [1][280/950] Elapsed 1m 17s (remain 3m 3s) Loss: 0.0000(0.0004) Grad: 12666.8926  LR: 0.00000012  \n",
            "Epoch: [1][300/950] Elapsed 1m 22s (remain 2m 57s) Loss: 0.0001(0.0004) Grad: 18684.6250  LR: 0.00000012  \n",
            "Epoch: [1][320/950] Elapsed 1m 28s (remain 2m 52s) Loss: 0.0001(0.0004) Grad: 18403.4336  LR: 0.00000012  \n",
            "Epoch: [1][340/950] Elapsed 1m 33s (remain 2m 47s) Loss: 0.0001(0.0004) Grad: 21686.2090  LR: 0.00000012  \n",
            "Epoch: [1][360/950] Elapsed 1m 39s (remain 2m 41s) Loss: 0.0001(0.0004) Grad: 28729.6133  LR: 0.00000012  \n",
            "Epoch: [1][380/950] Elapsed 1m 44s (remain 2m 36s) Loss: 0.0002(0.0004) Grad: 30803.9375  LR: 0.00000012  \n",
            "Epoch: [1][400/950] Elapsed 1m 50s (remain 2m 31s) Loss: 0.0001(0.0004) Grad: 25105.3379  LR: 0.00000011  \n",
            "Epoch: [1][420/950] Elapsed 1m 56s (remain 2m 26s) Loss: 0.0001(0.0004) Grad: 25179.9473  LR: 0.00000011  \n",
            "Epoch: [1][440/950] Elapsed 2m 1s (remain 2m 20s) Loss: 0.0002(0.0004) Grad: 42456.8828  LR: 0.00000011  \n",
            "Epoch: [1][460/950] Elapsed 2m 7s (remain 2m 15s) Loss: 0.0000(0.0003) Grad: 11593.5430  LR: 0.00000011  \n",
            "Epoch: [1][480/950] Elapsed 2m 12s (remain 2m 9s) Loss: 0.0001(0.0003) Grad: 26577.9609  LR: 0.00000011  \n",
            "Epoch: [1][500/950] Elapsed 2m 18s (remain 2m 4s) Loss: 0.0002(0.0003) Grad: 37852.7930  LR: 0.00000011  \n",
            "Epoch: [1][520/950] Elapsed 2m 24s (remain 1m 58s) Loss: 0.0000(0.0003) Grad: 15352.8760  LR: 0.00000011  \n",
            "Epoch: [1][540/950] Elapsed 2m 30s (remain 1m 53s) Loss: 0.0000(0.0003) Grad: 14231.9961  LR: 0.00000011  \n",
            "Epoch: [1][560/950] Elapsed 2m 35s (remain 1m 48s) Loss: 0.0000(0.0003) Grad: 9076.3369  LR: 0.00000011  \n",
            "Epoch: [1][580/950] Elapsed 2m 42s (remain 1m 42s) Loss: 0.0000(0.0004) Grad: 15440.7002  LR: 0.00000010  \n",
            "Epoch: [1][600/950] Elapsed 2m 47s (remain 1m 37s) Loss: 0.0001(0.0004) Grad: 19022.6777  LR: 0.00000010  \n",
            "Epoch: [1][620/950] Elapsed 2m 53s (remain 1m 31s) Loss: 0.0000(0.0004) Grad: 10845.1855  LR: 0.00000010  \n",
            "Epoch: [1][640/950] Elapsed 2m 58s (remain 1m 26s) Loss: 0.0000(0.0004) Grad: 16512.3906  LR: 0.00000010  \n",
            "Epoch: [1][660/950] Elapsed 3m 5s (remain 1m 21s) Loss: 0.0000(0.0004) Grad: 2937.8918  LR: 0.00000010  \n",
            "Epoch: [1][680/950] Elapsed 3m 10s (remain 1m 15s) Loss: 0.0001(0.0004) Grad: 23365.0059  LR: 0.00000010  \n",
            "Epoch: [1][700/950] Elapsed 3m 16s (remain 1m 9s) Loss: 0.0010(0.0004) Grad: 67320.2344  LR: 0.00000010  \n",
            "Epoch: [1][720/950] Elapsed 3m 21s (remain 1m 4s) Loss: 0.0000(0.0003) Grad: 2808.7720  LR: 0.00000010  \n",
            "Epoch: [1][740/950] Elapsed 3m 27s (remain 0m 58s) Loss: 0.0000(0.0003) Grad: 10629.6709  LR: 0.00000010  \n",
            "Epoch: [1][760/950] Elapsed 3m 32s (remain 0m 52s) Loss: 0.0000(0.0003) Grad: 13854.1123  LR: 0.00000009  \n",
            "Epoch: [1][780/950] Elapsed 3m 38s (remain 0m 47s) Loss: 0.0005(0.0003) Grad: 44832.4414  LR: 0.00000009  \n",
            "Epoch: [1][800/950] Elapsed 3m 43s (remain 0m 41s) Loss: 0.0002(0.0003) Grad: 34833.9688  LR: 0.00000009  \n",
            "Epoch: [1][820/950] Elapsed 3m 49s (remain 0m 35s) Loss: 0.0000(0.0003) Grad: 7730.7812  LR: 0.00000009  \n",
            "Epoch: [1][840/950] Elapsed 3m 54s (remain 0m 30s) Loss: 0.0000(0.0003) Grad: 11661.8779  LR: 0.00000009  \n",
            "Epoch: [1][860/950] Elapsed 4m 0s (remain 0m 24s) Loss: 0.0002(0.0003) Grad: 42881.3867  LR: 0.00000009  \n",
            "Epoch: [1][880/950] Elapsed 4m 5s (remain 0m 19s) Loss: 0.0000(0.0003) Grad: 12152.6797  LR: 0.00000009  \n",
            "Epoch: [1][900/950] Elapsed 4m 11s (remain 0m 13s) Loss: 0.0002(0.0003) Grad: 36742.6914  LR: 0.00000009  \n",
            "Epoch: [1][920/950] Elapsed 4m 16s (remain 0m 8s) Loss: 0.0003(0.0004) Grad: 31133.2637  LR: 0.00000009  \n",
            "Epoch: [1][940/950] Elapsed 4m 22s (remain 0m 2s) Loss: 0.0000(0.0004) Grad: 12319.2588  LR: 0.00000009  \n",
            "Epoch: [1][949/950] Elapsed 4m 24s (remain 0m 0s) Loss: 0.0000(0.0004) Grad: 6785.4355  LR: 0.00000008  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 32s) Loss: 0.0911(0.0911) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0969(0.1258) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0495(0.1225) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0605(0.1206) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1158(0.1253) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1042(0.1255) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0819(0.1279) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0004  avg_val_loss: 0.1275  time: 281s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0004  avg_val_loss: 0.1275  time: 281s\n",
            "Epoch 1 - Score: 0.5076  Scores: [0.5441275580099116, 0.5128041569721333, 0.4528085695799629, 0.5241331449514001, 0.5252269230423061, 0.48678123550670743]\n",
            "INFO:__main__:Epoch 1 - Score: 0.5076  Scores: [0.5441275580099116, 0.5128041569721333, 0.4528085695799629, 0.5241331449514001, 0.5252269230423061, 0.48678123550670743]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0852(0.1275) \n",
            "Epoch: [2][0/950] Elapsed 0m 0s (remain 7m 51s) Loss: 0.0000(0.0000) Grad: 18776.9531  LR: 0.00000008  \n",
            "Epoch: [2][20/950] Elapsed 0m 5s (remain 4m 22s) Loss: 0.0000(0.0002) Grad: 11325.8643  LR: 0.00000008  \n",
            "Epoch: [2][40/950] Elapsed 0m 11s (remain 4m 13s) Loss: 0.0000(0.0001) Grad: 4861.7363  LR: 0.00000008  \n",
            "Epoch: [2][60/950] Elapsed 0m 16s (remain 4m 5s) Loss: 0.0000(0.0002) Grad: 8296.9170  LR: 0.00000008  \n",
            "Epoch: [2][80/950] Elapsed 0m 22s (remain 4m 3s) Loss: 0.0000(0.0004) Grad: 17876.0215  LR: 0.00000008  \n",
            "Epoch: [2][100/950] Elapsed 0m 28s (remain 3m 56s) Loss: 0.0001(0.0003) Grad: 15713.5781  LR: 0.00000008  \n",
            "Epoch: [2][120/950] Elapsed 0m 33s (remain 3m 52s) Loss: 0.0377(0.0006) Grad: 448744.5000  LR: 0.00000008  \n",
            "Epoch: [2][140/950] Elapsed 0m 39s (remain 3m 45s) Loss: 0.0000(0.0005) Grad: 9783.7588  LR: 0.00000008  \n",
            "Epoch: [2][160/950] Elapsed 0m 44s (remain 3m 40s) Loss: 0.0000(0.0005) Grad: 6908.3989  LR: 0.00000008  \n",
            "Epoch: [2][180/950] Elapsed 0m 50s (remain 3m 34s) Loss: 0.0000(0.0004) Grad: 3846.2505  LR: 0.00000008  \n",
            "Epoch: [2][200/950] Elapsed 0m 56s (remain 3m 29s) Loss: 0.0000(0.0004) Grad: 7356.8325  LR: 0.00000007  \n",
            "Epoch: [2][220/950] Elapsed 1m 1s (remain 3m 24s) Loss: 0.0000(0.0004) Grad: 5347.4336  LR: 0.00000007  \n",
            "Epoch: [2][240/950] Elapsed 1m 7s (remain 3m 18s) Loss: 0.0000(0.0004) Grad: 13546.5645  LR: 0.00000007  \n",
            "Epoch: [2][260/950] Elapsed 1m 13s (remain 3m 13s) Loss: 0.0000(0.0005) Grad: 14330.5254  LR: 0.00000007  \n",
            "Epoch: [2][280/950] Elapsed 1m 18s (remain 3m 7s) Loss: 0.0000(0.0005) Grad: 10233.8076  LR: 0.00000007  \n",
            "Epoch: [2][300/950] Elapsed 1m 24s (remain 3m 1s) Loss: 0.0000(0.0004) Grad: 6642.1797  LR: 0.00000007  \n",
            "Epoch: [2][320/950] Elapsed 1m 29s (remain 2m 55s) Loss: 0.0000(0.0004) Grad: 8061.9043  LR: 0.00000007  \n",
            "Epoch: [2][340/950] Elapsed 1m 35s (remain 2m 49s) Loss: 0.0000(0.0004) Grad: 8674.8018  LR: 0.00000007  \n",
            "Epoch: [2][360/950] Elapsed 1m 40s (remain 2m 43s) Loss: 0.0000(0.0004) Grad: 11544.4414  LR: 0.00000007  \n",
            "Epoch: [2][380/950] Elapsed 1m 45s (remain 2m 38s) Loss: 0.0000(0.0004) Grad: 8506.5498  LR: 0.00000007  \n",
            "Epoch: [2][400/950] Elapsed 1m 51s (remain 2m 32s) Loss: 0.0000(0.0003) Grad: 10041.0635  LR: 0.00000007  \n",
            "Epoch: [2][420/950] Elapsed 1m 56s (remain 2m 26s) Loss: 0.0001(0.0003) Grad: 17540.2852  LR: 0.00000006  \n",
            "Epoch: [2][440/950] Elapsed 2m 2s (remain 2m 21s) Loss: 0.0000(0.0003) Grad: 3565.6675  LR: 0.00000006  \n",
            "Epoch: [2][460/950] Elapsed 2m 7s (remain 2m 15s) Loss: 0.0000(0.0003) Grad: 9007.9258  LR: 0.00000006  \n",
            "Epoch: [2][480/950] Elapsed 2m 14s (remain 2m 11s) Loss: 0.0000(0.0003) Grad: 9850.0674  LR: 0.00000006  \n",
            "Epoch: [2][500/950] Elapsed 2m 19s (remain 2m 5s) Loss: 0.0000(0.0003) Grad: 12942.5059  LR: 0.00000006  \n",
            "Epoch: [2][520/950] Elapsed 2m 25s (remain 1m 59s) Loss: 0.0000(0.0003) Grad: 3395.3120  LR: 0.00000006  \n",
            "Epoch: [2][540/950] Elapsed 2m 30s (remain 1m 53s) Loss: 0.0000(0.0003) Grad: 7753.4468  LR: 0.00000006  \n",
            "Epoch: [2][560/950] Elapsed 2m 36s (remain 1m 48s) Loss: 0.0000(0.0003) Grad: 4932.4570  LR: 0.00000006  \n",
            "Epoch: [2][580/950] Elapsed 2m 41s (remain 1m 42s) Loss: 0.0000(0.0004) Grad: 5455.3887  LR: 0.00000006  \n",
            "Epoch: [2][600/950] Elapsed 2m 46s (remain 1m 36s) Loss: 0.0000(0.0004) Grad: 5852.0654  LR: 0.00000006  \n",
            "Epoch: [2][620/950] Elapsed 2m 52s (remain 1m 31s) Loss: 0.0001(0.0003) Grad: 15238.3203  LR: 0.00000006  \n",
            "Epoch: [2][640/950] Elapsed 2m 58s (remain 1m 25s) Loss: 0.0000(0.0003) Grad: 13814.5928  LR: 0.00000006  \n",
            "Epoch: [2][660/950] Elapsed 3m 3s (remain 1m 20s) Loss: 0.0000(0.0003) Grad: 6218.1104  LR: 0.00000005  \n",
            "Epoch: [2][680/950] Elapsed 3m 9s (remain 1m 14s) Loss: 0.0001(0.0003) Grad: 15132.3545  LR: 0.00000005  \n",
            "Epoch: [2][700/950] Elapsed 3m 14s (remain 1m 9s) Loss: 0.0000(0.0003) Grad: 9225.2998  LR: 0.00000005  \n",
            "Epoch: [2][720/950] Elapsed 3m 20s (remain 1m 3s) Loss: 0.0000(0.0003) Grad: 18158.3105  LR: 0.00000005  \n",
            "Epoch: [2][740/950] Elapsed 3m 25s (remain 0m 58s) Loss: 0.0000(0.0003) Grad: 15905.3965  LR: 0.00000005  \n",
            "Epoch: [2][760/950] Elapsed 3m 31s (remain 0m 52s) Loss: 0.0000(0.0003) Grad: 10809.0088  LR: 0.00000005  \n",
            "Epoch: [2][780/950] Elapsed 3m 36s (remain 0m 46s) Loss: 0.0000(0.0003) Grad: 10422.4365  LR: 0.00000005  \n",
            "Epoch: [2][800/950] Elapsed 3m 42s (remain 0m 41s) Loss: 0.0000(0.0004) Grad: 5513.9819  LR: 0.00000005  \n",
            "Epoch: [2][820/950] Elapsed 3m 48s (remain 0m 35s) Loss: 0.0001(0.0003) Grad: 23241.8711  LR: 0.00000005  \n",
            "Epoch: [2][840/950] Elapsed 3m 54s (remain 0m 30s) Loss: 0.0000(0.0003) Grad: 8137.4453  LR: 0.00000005  \n",
            "Epoch: [2][860/950] Elapsed 3m 59s (remain 0m 24s) Loss: 0.0000(0.0003) Grad: 6901.6851  LR: 0.00000005  \n",
            "Epoch: [2][880/950] Elapsed 4m 5s (remain 0m 19s) Loss: 0.0000(0.0003) Grad: 7888.5063  LR: 0.00000005  \n",
            "Epoch: [2][900/950] Elapsed 4m 10s (remain 0m 13s) Loss: 0.0000(0.0003) Grad: 3136.9321  LR: 0.00000005  \n",
            "Epoch: [2][920/950] Elapsed 4m 16s (remain 0m 8s) Loss: 0.0000(0.0003) Grad: 5161.2295  LR: 0.00000004  \n",
            "Epoch: [2][940/950] Elapsed 4m 22s (remain 0m 2s) Loss: 0.0001(0.0003) Grad: 20002.3379  LR: 0.00000004  \n",
            "Epoch: [2][949/950] Elapsed 4m 25s (remain 0m 0s) Loss: 0.0000(0.0003) Grad: 14693.6260  LR: 0.00000004  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0912(0.0912) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0970(0.1259) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0504(0.1228) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0613(0.1210) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1166(0.1256) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1053(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0833(0.1282) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0003  avg_val_loss: 0.1278  time: 281s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0003  avg_val_loss: 0.1278  time: 281s\n",
            "Epoch 2 - Score: 0.5082  Scores: [0.5457086792832536, 0.5135553957203323, 0.452676352973826, 0.5243406856264506, 0.525693103999385, 0.4869976638502538]\n",
            "INFO:__main__:Epoch 2 - Score: 0.5082  Scores: [0.5457086792832536, 0.5135553957203323, 0.452676352973826, 0.5243406856264506, 0.525693103999385, 0.4869976638502538]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0855(0.1278) \n",
            "Epoch: [3][0/950] Elapsed 0m 0s (remain 7m 42s) Loss: 0.0000(0.0000) Grad: 13873.0557  LR: 0.00000004  \n",
            "Epoch: [3][20/950] Elapsed 0m 5s (remain 4m 23s) Loss: 0.0000(0.0003) Grad: 6200.0557  LR: 0.00000004  \n",
            "Epoch: [3][40/950] Elapsed 0m 11s (remain 4m 13s) Loss: 0.0000(0.0002) Grad: 3269.6755  LR: 0.00000004  \n",
            "Epoch: [3][60/950] Elapsed 0m 16s (remain 4m 6s) Loss: 0.0000(0.0001) Grad: 12746.7578  LR: 0.00000004  \n",
            "Epoch: [3][80/950] Elapsed 0m 23s (remain 4m 9s) Loss: 0.0000(0.0001) Grad: 7666.3057  LR: 0.00000004  \n",
            "Epoch: [3][100/950] Elapsed 0m 28s (remain 4m 1s) Loss: 0.0002(0.0004) Grad: 25384.6035  LR: 0.00000004  \n",
            "Epoch: [3][120/950] Elapsed 0m 34s (remain 3m 54s) Loss: 0.0000(0.0003) Grad: 4596.6113  LR: 0.00000004  \n",
            "Epoch: [3][140/950] Elapsed 0m 39s (remain 3m 47s) Loss: 0.0000(0.0003) Grad: 5784.0640  LR: 0.00000004  \n",
            "Epoch: [3][160/950] Elapsed 0m 45s (remain 3m 45s) Loss: 0.0001(0.0003) Grad: 20475.6562  LR: 0.00000004  \n",
            "Epoch: [3][180/950] Elapsed 0m 51s (remain 3m 39s) Loss: 0.0000(0.0002) Grad: 8491.1006  LR: 0.00000004  \n",
            "Epoch: [3][200/950] Elapsed 0m 57s (remain 3m 32s) Loss: 0.0000(0.0002) Grad: 4926.7441  LR: 0.00000004  \n",
            "Epoch: [3][220/950] Elapsed 1m 2s (remain 3m 26s) Loss: 0.0000(0.0002) Grad: 5686.6846  LR: 0.00000004  \n",
            "Epoch: [3][240/950] Elapsed 1m 8s (remain 3m 20s) Loss: 0.0000(0.0002) Grad: 7229.6479  LR: 0.00000003  \n",
            "Epoch: [3][260/950] Elapsed 1m 13s (remain 3m 14s) Loss: 0.0014(0.0002) Grad: 78505.6875  LR: 0.00000003  \n",
            "Epoch: [3][280/950] Elapsed 1m 19s (remain 3m 8s) Loss: 0.0000(0.0003) Grad: 5757.9478  LR: 0.00000003  \n",
            "Epoch: [3][300/950] Elapsed 1m 24s (remain 3m 2s) Loss: 0.0004(0.0003) Grad: 32439.9434  LR: 0.00000003  \n",
            "Epoch: [3][320/950] Elapsed 1m 29s (remain 2m 56s) Loss: 0.0000(0.0003) Grad: 7486.0010  LR: 0.00000003  \n",
            "Epoch: [3][340/950] Elapsed 1m 35s (remain 2m 50s) Loss: 0.0000(0.0002) Grad: 6269.9224  LR: 0.00000003  \n",
            "Epoch: [3][360/950] Elapsed 1m 40s (remain 2m 44s) Loss: 0.0000(0.0003) Grad: 7619.3164  LR: 0.00000003  \n",
            "Epoch: [3][380/950] Elapsed 1m 46s (remain 2m 38s) Loss: 0.0000(0.0003) Grad: 4129.2061  LR: 0.00000003  \n",
            "Epoch: [3][400/950] Elapsed 1m 51s (remain 2m 33s) Loss: 0.0000(0.0002) Grad: 4656.9551  LR: 0.00000003  \n",
            "Epoch: [3][420/950] Elapsed 1m 57s (remain 2m 27s) Loss: 0.0000(0.0002) Grad: 3259.9827  LR: 0.00000003  \n",
            "Epoch: [3][440/950] Elapsed 2m 2s (remain 2m 21s) Loss: 0.0000(0.0002) Grad: 5060.4390  LR: 0.00000003  \n",
            "Epoch: [3][460/950] Elapsed 2m 8s (remain 2m 16s) Loss: 0.0000(0.0003) Grad: 7285.3862  LR: 0.00000003  \n",
            "Epoch: [3][480/950] Elapsed 2m 14s (remain 2m 11s) Loss: 0.0000(0.0002) Grad: 5334.5581  LR: 0.00000003  \n",
            "Epoch: [3][500/950] Elapsed 2m 20s (remain 2m 5s) Loss: 0.0000(0.0002) Grad: 8395.2783  LR: 0.00000003  \n",
            "Epoch: [3][520/950] Elapsed 2m 26s (remain 2m 0s) Loss: 0.0000(0.0002) Grad: 3023.2305  LR: 0.00000003  \n",
            "Epoch: [3][540/950] Elapsed 2m 31s (remain 1m 54s) Loss: 0.0002(0.0002) Grad: 30422.4336  LR: 0.00000003  \n",
            "Epoch: [3][560/950] Elapsed 2m 37s (remain 1m 48s) Loss: 0.0000(0.0002) Grad: 6482.2417  LR: 0.00000003  \n",
            "Epoch: [3][580/950] Elapsed 2m 42s (remain 1m 43s) Loss: 0.0000(0.0002) Grad: 6057.8267  LR: 0.00000002  \n",
            "Epoch: [3][600/950] Elapsed 2m 47s (remain 1m 37s) Loss: 0.0000(0.0002) Grad: 3252.3318  LR: 0.00000002  \n",
            "Epoch: [3][620/950] Elapsed 2m 53s (remain 1m 31s) Loss: 0.0000(0.0002) Grad: 2932.9082  LR: 0.00000002  \n",
            "Epoch: [3][640/950] Elapsed 2m 59s (remain 1m 26s) Loss: 0.0000(0.0002) Grad: 5749.7827  LR: 0.00000002  \n",
            "Epoch: [3][660/950] Elapsed 3m 4s (remain 1m 20s) Loss: 0.0000(0.0002) Grad: 3759.8337  LR: 0.00000002  \n",
            "Epoch: [3][680/950] Elapsed 3m 10s (remain 1m 15s) Loss: 0.0000(0.0002) Grad: 3620.6199  LR: 0.00000002  \n",
            "Epoch: [3][700/950] Elapsed 3m 15s (remain 1m 9s) Loss: 0.0000(0.0002) Grad: 1698.4076  LR: 0.00000002  \n",
            "Epoch: [3][720/950] Elapsed 3m 21s (remain 1m 3s) Loss: 0.0000(0.0003) Grad: 4016.7498  LR: 0.00000002  \n",
            "Epoch: [3][740/950] Elapsed 3m 26s (remain 0m 58s) Loss: 0.0000(0.0003) Grad: 5110.2456  LR: 0.00000002  \n",
            "Epoch: [3][760/950] Elapsed 3m 32s (remain 0m 52s) Loss: 0.0000(0.0003) Grad: 10227.0244  LR: 0.00000002  \n",
            "Epoch: [3][780/950] Elapsed 3m 37s (remain 0m 47s) Loss: 0.0000(0.0003) Grad: 12726.9502  LR: 0.00000002  \n",
            "Epoch: [3][800/950] Elapsed 3m 43s (remain 0m 41s) Loss: 0.0000(0.0003) Grad: 6930.1782  LR: 0.00000002  \n",
            "Epoch: [3][820/950] Elapsed 3m 48s (remain 0m 35s) Loss: 0.0003(0.0003) Grad: 39582.7461  LR: 0.00000002  \n",
            "Epoch: [3][840/950] Elapsed 3m 54s (remain 0m 30s) Loss: 0.0000(0.0003) Grad: 8371.2988  LR: 0.00000002  \n",
            "Epoch: [3][860/950] Elapsed 3m 59s (remain 0m 24s) Loss: 0.0000(0.0003) Grad: 2913.9148  LR: 0.00000002  \n",
            "Epoch: [3][880/950] Elapsed 4m 5s (remain 0m 19s) Loss: 0.0000(0.0003) Grad: 4686.0439  LR: 0.00000002  \n",
            "Epoch: [3][900/950] Elapsed 4m 11s (remain 0m 13s) Loss: 0.0000(0.0003) Grad: 7780.8867  LR: 0.00000002  \n",
            "Epoch: [3][920/950] Elapsed 4m 17s (remain 0m 8s) Loss: 0.0000(0.0003) Grad: 5343.8384  LR: 0.00000002  \n",
            "Epoch: [3][940/950] Elapsed 4m 22s (remain 0m 2s) Loss: 0.0000(0.0003) Grad: 6118.6821  LR: 0.00000002  \n",
            "Epoch: [3][949/950] Elapsed 4m 24s (remain 0m 0s) Loss: 0.0000(0.0003) Grad: 15781.0654  LR: 0.00000002  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0912(0.0912) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0969(0.1259) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0503(0.1228) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0614(0.1210) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1163(0.1256) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1052(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0835(0.1282) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 281s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 281s\n",
            "Epoch 3 - Score: 0.5081  Scores: [0.545783683395385, 0.5135661860699281, 0.45244701871014714, 0.5241441451592121, 0.525618966052651, 0.4872545257944505]\n",
            "INFO:__main__:Epoch 3 - Score: 0.5081  Scores: [0.545783683395385, 0.5135661860699281, 0.45244701871014714, 0.5241441451592121, 0.525618966052651, 0.4872545257944505]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0853(0.1277) \n",
            "Epoch: [4][0/950] Elapsed 0m 0s (remain 7m 58s) Loss: 0.0000(0.0000) Grad: 3135.6699  LR: 0.00000002  \n",
            "Epoch: [4][20/950] Elapsed 0m 5s (remain 4m 24s) Loss: 0.0000(0.0001) Grad: 2968.6260  LR: 0.00000001  \n",
            "Epoch: [4][40/950] Elapsed 0m 11s (remain 4m 14s) Loss: 0.0000(0.0001) Grad: 1044.6471  LR: 0.00000001  \n",
            "Epoch: [4][60/950] Elapsed 0m 16s (remain 4m 7s) Loss: 0.0002(0.0001) Grad: 28665.4336  LR: 0.00000001  \n",
            "Epoch: [4][80/950] Elapsed 0m 22s (remain 4m 0s) Loss: 0.0000(0.0002) Grad: 10046.6025  LR: 0.00000001  \n",
            "Epoch: [4][100/950] Elapsed 0m 27s (remain 3m 53s) Loss: 0.0000(0.0003) Grad: 3579.3923  LR: 0.00000001  \n",
            "Epoch: [4][120/950] Elapsed 0m 33s (remain 3m 48s) Loss: 0.0000(0.0003) Grad: 7352.0317  LR: 0.00000001  \n",
            "Epoch: [4][140/950] Elapsed 0m 38s (remain 3m 43s) Loss: 0.0000(0.0002) Grad: 5639.7085  LR: 0.00000001  \n",
            "Epoch: [4][160/950] Elapsed 0m 44s (remain 3m 38s) Loss: 0.0000(0.0002) Grad: 5572.9072  LR: 0.00000001  \n",
            "Epoch: [4][180/950] Elapsed 0m 50s (remain 3m 33s) Loss: 0.0000(0.0002) Grad: 3261.4377  LR: 0.00000001  \n",
            "Epoch: [4][200/950] Elapsed 0m 56s (remain 3m 31s) Loss: 0.0025(0.0002) Grad: 98238.0938  LR: 0.00000001  \n",
            "Epoch: [4][220/950] Elapsed 1m 2s (remain 3m 25s) Loss: 0.0000(0.0003) Grad: 3342.0747  LR: 0.00000001  \n",
            "Epoch: [4][240/950] Elapsed 1m 7s (remain 3m 19s) Loss: 0.0003(0.0002) Grad: 30548.2422  LR: 0.00000001  \n",
            "Epoch: [4][260/950] Elapsed 1m 13s (remain 3m 13s) Loss: 0.0000(0.0002) Grad: 3684.6914  LR: 0.00000001  \n",
            "Epoch: [4][280/950] Elapsed 1m 18s (remain 3m 7s) Loss: 0.0000(0.0002) Grad: 3898.9053  LR: 0.00000001  \n",
            "Epoch: [4][300/950] Elapsed 1m 24s (remain 3m 2s) Loss: 0.0000(0.0002) Grad: 8359.1631  LR: 0.00000001  \n",
            "Epoch: [4][320/950] Elapsed 1m 29s (remain 2m 56s) Loss: 0.0000(0.0002) Grad: 8471.6553  LR: 0.00000001  \n",
            "Epoch: [4][340/950] Elapsed 1m 35s (remain 2m 50s) Loss: 0.0000(0.0002) Grad: 3897.2498  LR: 0.00000001  \n",
            "Epoch: [4][360/950] Elapsed 1m 40s (remain 2m 44s) Loss: 0.0001(0.0002) Grad: 11075.7207  LR: 0.00000001  \n",
            "Epoch: [4][380/950] Elapsed 1m 46s (remain 2m 38s) Loss: 0.0000(0.0002) Grad: 5450.7520  LR: 0.00000001  \n",
            "Epoch: [4][400/950] Elapsed 1m 52s (remain 2m 33s) Loss: 0.0000(0.0002) Grad: 9599.9785  LR: 0.00000001  \n",
            "Epoch: [4][420/950] Elapsed 1m 57s (remain 2m 27s) Loss: 0.0000(0.0001) Grad: 7318.0615  LR: 0.00000001  \n",
            "Epoch: [4][440/950] Elapsed 2m 2s (remain 2m 21s) Loss: 0.0005(0.0002) Grad: 45527.6211  LR: 0.00000001  \n",
            "Epoch: [4][460/950] Elapsed 2m 8s (remain 2m 16s) Loss: 0.0000(0.0002) Grad: 1945.0454  LR: 0.00000001  \n",
            "Epoch: [4][480/950] Elapsed 2m 14s (remain 2m 11s) Loss: 0.0000(0.0002) Grad: 4103.9165  LR: 0.00000001  \n",
            "Epoch: [4][500/950] Elapsed 2m 19s (remain 2m 5s) Loss: 0.0000(0.0002) Grad: 2959.3511  LR: 0.00000001  \n",
            "Epoch: [4][520/950] Elapsed 2m 26s (remain 2m 0s) Loss: 0.0000(0.0002) Grad: 3384.5510  LR: 0.00000001  \n",
            "Epoch: [4][540/950] Elapsed 2m 31s (remain 1m 54s) Loss: 0.0000(0.0002) Grad: 5627.2734  LR: 0.00000001  \n",
            "Epoch: [4][560/950] Elapsed 2m 37s (remain 1m 48s) Loss: 0.0000(0.0002) Grad: 2437.3232  LR: 0.00000001  \n",
            "Epoch: [4][580/950] Elapsed 2m 42s (remain 1m 43s) Loss: 0.0005(0.0003) Grad: 50797.0195  LR: 0.00000001  \n",
            "Epoch: [4][600/950] Elapsed 2m 48s (remain 1m 37s) Loss: 0.0000(0.0003) Grad: 9837.4863  LR: 0.00000001  \n",
            "Epoch: [4][620/950] Elapsed 2m 53s (remain 1m 32s) Loss: 0.0000(0.0003) Grad: 4962.2153  LR: 0.00000000  \n",
            "Epoch: [4][640/950] Elapsed 2m 59s (remain 1m 26s) Loss: 0.0003(0.0003) Grad: 29258.9961  LR: 0.00000000  \n",
            "Epoch: [4][660/950] Elapsed 3m 4s (remain 1m 20s) Loss: 0.0000(0.0003) Grad: 3977.0708  LR: 0.00000000  \n",
            "Epoch: [4][680/950] Elapsed 3m 10s (remain 1m 15s) Loss: 0.0000(0.0003) Grad: 3077.8284  LR: 0.00000000  \n",
            "Epoch: [4][700/950] Elapsed 3m 15s (remain 1m 9s) Loss: 0.0000(0.0002) Grad: 5355.5151  LR: 0.00000000  \n",
            "Epoch: [4][720/950] Elapsed 3m 21s (remain 1m 3s) Loss: 0.0000(0.0002) Grad: 8468.2773  LR: 0.00000000  \n",
            "Epoch: [4][740/950] Elapsed 3m 26s (remain 0m 58s) Loss: 0.0000(0.0002) Grad: 5437.4907  LR: 0.00000000  \n",
            "Epoch: [4][760/950] Elapsed 3m 32s (remain 0m 52s) Loss: 0.0000(0.0003) Grad: 3886.2246  LR: 0.00000000  \n",
            "Epoch: [4][780/950] Elapsed 3m 37s (remain 0m 47s) Loss: 0.0000(0.0003) Grad: 2117.8977  LR: 0.00000000  \n",
            "Epoch: [4][800/950] Elapsed 3m 43s (remain 0m 41s) Loss: 0.0000(0.0003) Grad: 5782.3564  LR: 0.00000000  \n",
            "Epoch: [4][820/950] Elapsed 3m 48s (remain 0m 35s) Loss: 0.0000(0.0003) Grad: 7249.0581  LR: 0.00000000  \n",
            "Epoch: [4][840/950] Elapsed 3m 55s (remain 0m 30s) Loss: 0.0000(0.0003) Grad: 6581.2695  LR: 0.00000000  \n",
            "Epoch: [4][860/950] Elapsed 4m 0s (remain 0m 24s) Loss: 0.0000(0.0003) Grad: 5679.9707  LR: 0.00000000  \n",
            "Epoch: [4][880/950] Elapsed 4m 6s (remain 0m 19s) Loss: 0.0001(0.0003) Grad: 22319.1309  LR: 0.00000000  \n",
            "Epoch: [4][900/950] Elapsed 4m 11s (remain 0m 13s) Loss: 0.0000(0.0003) Grad: 4554.9390  LR: 0.00000000  \n",
            "Epoch: [4][920/950] Elapsed 4m 17s (remain 0m 8s) Loss: 0.0000(0.0003) Grad: 4372.7852  LR: 0.00000000  \n",
            "Epoch: [4][940/950] Elapsed 4m 22s (remain 0m 2s) Loss: 0.0000(0.0003) Grad: 9393.1738  LR: 0.00000000  \n",
            "Epoch: [4][949/950] Elapsed 4m 25s (remain 0m 0s) Loss: 0.0000(0.0003) Grad: 5749.4429  LR: 0.00000000  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0913(0.0913) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0966(0.1259) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0502(0.1227) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0614(0.1209) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1166(0.1256) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1051(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0835(0.1282) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 281s\n",
            "INFO:__main__:Epoch 4 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 281s\n",
            "Epoch 4 - Score: 0.5081  Scores: [0.5455060628966164, 0.5135729220380986, 0.45249622172674825, 0.5241183565016886, 0.5256997901419759, 0.48718962437669183]\n",
            "INFO:__main__:Epoch 4 - Score: 0.5081  Scores: [0.5455060628966164, 0.5135729220380986, 0.45249622172674825, 0.5241183565016886, 0.5256997901419759, 0.48718962437669183]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0854(0.1277) \n",
            "Epoch: [5][0/950] Elapsed 0m 0s (remain 8m 4s) Loss: 0.0000(0.0000) Grad: 1990.8517  LR: 0.00000000  \n",
            "Epoch: [5][20/950] Elapsed 0m 6s (remain 4m 28s) Loss: 0.0000(0.0003) Grad: 6170.4355  LR: 0.00000000  \n",
            "Epoch: [5][40/950] Elapsed 0m 11s (remain 4m 16s) Loss: 0.0000(0.0002) Grad: 3946.4648  LR: 0.00000000  \n",
            "Epoch: [5][60/950] Elapsed 0m 17s (remain 4m 8s) Loss: 0.0000(0.0002) Grad: 5246.5679  LR: 0.00000000  \n",
            "Epoch: [5][80/950] Elapsed 0m 22s (remain 4m 1s) Loss: 0.0000(0.0002) Grad: 4534.0371  LR: 0.00000000  \n",
            "Epoch: [5][100/950] Elapsed 0m 28s (remain 3m 55s) Loss: 0.0025(0.0005) Grad: 89604.2422  LR: 0.00000000  \n",
            "Epoch: [5][120/950] Elapsed 0m 33s (remain 3m 49s) Loss: 0.0001(0.0005) Grad: 15866.7822  LR: 0.00000000  \n",
            "Epoch: [5][140/950] Elapsed 0m 39s (remain 3m 43s) Loss: 0.0002(0.0004) Grad: 24845.2949  LR: 0.00000000  \n",
            "Epoch: [5][160/950] Elapsed 0m 44s (remain 3m 37s) Loss: 0.0000(0.0004) Grad: 1944.1212  LR: 0.00000000  \n",
            "Epoch: [5][180/950] Elapsed 0m 49s (remain 3m 31s) Loss: 0.0000(0.0003) Grad: 4358.7383  LR: 0.00000000  \n",
            "Epoch: [5][200/950] Elapsed 0m 55s (remain 3m 26s) Loss: 0.0000(0.0003) Grad: 4540.4673  LR: 0.00000000  \n",
            "Epoch: [5][220/950] Elapsed 1m 0s (remain 3m 20s) Loss: 0.0000(0.0004) Grad: 5865.1230  LR: 0.00000000  \n",
            "Epoch: [5][240/950] Elapsed 1m 6s (remain 3m 15s) Loss: 0.0000(0.0003) Grad: 2819.7832  LR: 0.00000000  \n",
            "Epoch: [5][260/950] Elapsed 1m 12s (remain 3m 10s) Loss: 0.0000(0.0003) Grad: 3504.7578  LR: 0.00000000  \n",
            "Epoch: [5][280/950] Elapsed 1m 18s (remain 3m 5s) Loss: 0.0000(0.0004) Grad: 5043.9150  LR: 0.00000000  \n",
            "Epoch: [5][300/950] Elapsed 1m 23s (remain 3m 0s) Loss: 0.0000(0.0005) Grad: 1717.7775  LR: 0.00000000  \n",
            "Epoch: [5][320/950] Elapsed 1m 30s (remain 2m 58s) Loss: 0.0000(0.0005) Grad: 1537.0192  LR: 0.00000000  \n",
            "Epoch: [5][340/950] Elapsed 1m 36s (remain 2m 52s) Loss: 0.0000(0.0004) Grad: 3745.7292  LR: 0.00000000  \n",
            "Epoch: [5][360/950] Elapsed 1m 42s (remain 2m 46s) Loss: 0.0000(0.0004) Grad: 3572.4470  LR: 0.00000000  \n",
            "Epoch: [5][380/950] Elapsed 1m 47s (remain 2m 40s) Loss: 0.0000(0.0004) Grad: 4659.0488  LR: 0.00000000  \n",
            "Epoch: [5][400/950] Elapsed 1m 53s (remain 2m 34s) Loss: 0.0002(0.0004) Grad: 32634.9707  LR: 0.00000000  \n",
            "Epoch: [5][420/950] Elapsed 1m 58s (remain 2m 29s) Loss: 0.0000(0.0004) Grad: 1649.9421  LR: 0.00000000  \n",
            "Epoch: [5][440/950] Elapsed 2m 4s (remain 2m 23s) Loss: 0.0000(0.0004) Grad: 4527.6172  LR: 0.00000000  \n",
            "Epoch: [5][460/950] Elapsed 2m 9s (remain 2m 17s) Loss: 0.0000(0.0004) Grad: 5679.4165  LR: 0.00000000  \n",
            "Epoch: [5][480/950] Elapsed 2m 15s (remain 2m 12s) Loss: 0.0013(0.0004) Grad: 63215.3438  LR: 0.00000000  \n",
            "Epoch: [5][500/950] Elapsed 2m 20s (remain 2m 6s) Loss: 0.0000(0.0004) Grad: 4829.8857  LR: 0.00000000  \n",
            "Epoch: [5][520/950] Elapsed 2m 26s (remain 2m 0s) Loss: 0.0000(0.0004) Grad: 2397.4116  LR: 0.00000000  \n",
            "Epoch: [5][540/950] Elapsed 2m 31s (remain 1m 54s) Loss: 0.0000(0.0004) Grad: 2420.7678  LR: 0.00000000  \n",
            "Epoch: [5][560/950] Elapsed 2m 37s (remain 1m 49s) Loss: 0.0000(0.0003) Grad: 4255.4033  LR: 0.00000000  \n",
            "Epoch: [5][580/950] Elapsed 2m 43s (remain 1m 43s) Loss: 0.0000(0.0003) Grad: 5307.1187  LR: 0.00000000  \n",
            "Epoch: [5][600/950] Elapsed 2m 48s (remain 1m 37s) Loss: 0.0000(0.0004) Grad: 5833.9268  LR: 0.00000000  \n",
            "Epoch: [5][620/950] Elapsed 2m 53s (remain 1m 32s) Loss: 0.0000(0.0004) Grad: 3461.7642  LR: 0.00000000  \n",
            "Epoch: [5][640/950] Elapsed 2m 59s (remain 1m 26s) Loss: 0.0000(0.0004) Grad: 5697.0039  LR: 0.00000000  \n",
            "Epoch: [5][660/950] Elapsed 3m 5s (remain 1m 20s) Loss: 0.0000(0.0004) Grad: 2676.4160  LR: 0.00000000  \n",
            "Epoch: [5][680/950] Elapsed 3m 10s (remain 1m 15s) Loss: 0.0000(0.0004) Grad: 8864.7432  LR: 0.00000000  \n",
            "Epoch: [5][700/950] Elapsed 3m 16s (remain 1m 9s) Loss: 0.0000(0.0004) Grad: 2174.0408  LR: 0.00000000  \n",
            "Epoch: [5][720/950] Elapsed 3m 22s (remain 1m 4s) Loss: 0.0000(0.0003) Grad: 3832.7888  LR: 0.00000000  \n",
            "Epoch: [5][740/950] Elapsed 3m 27s (remain 0m 58s) Loss: 0.0000(0.0003) Grad: 3472.0286  LR: 0.00000000  \n",
            "Epoch: [5][760/950] Elapsed 3m 33s (remain 0m 52s) Loss: 0.0000(0.0003) Grad: 2466.2747  LR: 0.00000000  \n",
            "Epoch: [5][780/950] Elapsed 3m 39s (remain 0m 47s) Loss: 0.0000(0.0003) Grad: 4745.9546  LR: 0.00000000  \n",
            "Epoch: [5][800/950] Elapsed 3m 45s (remain 0m 41s) Loss: 0.0000(0.0003) Grad: 7514.5098  LR: 0.00000000  \n",
            "Epoch: [5][820/950] Elapsed 3m 51s (remain 0m 36s) Loss: 0.0000(0.0003) Grad: 3402.7795  LR: 0.00000000  \n",
            "Epoch: [5][840/950] Elapsed 3m 56s (remain 0m 30s) Loss: 0.0000(0.0003) Grad: 6167.2476  LR: 0.00000000  \n",
            "Epoch: [5][860/950] Elapsed 4m 2s (remain 0m 25s) Loss: 0.0000(0.0003) Grad: 2905.6125  LR: 0.00000000  \n",
            "Epoch: [5][880/950] Elapsed 4m 8s (remain 0m 19s) Loss: 0.0000(0.0003) Grad: 2355.5864  LR: 0.00000000  \n",
            "Epoch: [5][900/950] Elapsed 4m 13s (remain 0m 13s) Loss: 0.0000(0.0003) Grad: 6029.8652  LR: 0.00000000  \n",
            "Epoch: [5][920/950] Elapsed 4m 19s (remain 0m 8s) Loss: 0.0000(0.0003) Grad: 3523.6523  LR: 0.00000000  \n",
            "Epoch: [5][940/950] Elapsed 4m 25s (remain 0m 2s) Loss: 0.0000(0.0003) Grad: 3533.2905  LR: 0.00000000  \n",
            "Epoch: [5][949/950] Elapsed 4m 27s (remain 0m 0s) Loss: 0.0000(0.0003) Grad: 3085.6023  LR: 0.00000000  \n",
            "EVAL: [0/125] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0913(0.0913) \n",
            "EVAL: [20/125] Elapsed 0m 2s (remain 0m 13s) Loss: 0.0967(0.1259) \n",
            "EVAL: [40/125] Elapsed 0m 4s (remain 0m 10s) Loss: 0.0502(0.1227) \n",
            "EVAL: [60/125] Elapsed 0m 7s (remain 0m 7s) Loss: 0.0613(0.1209) \n",
            "EVAL: [80/125] Elapsed 0m 11s (remain 0m 6s) Loss: 0.1166(0.1256) \n",
            "EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.1051(0.1257) \n",
            "EVAL: [120/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0835(0.1282) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 283s\n",
            "INFO:__main__:Epoch 5 - avg_train_loss: 0.0003  avg_val_loss: 0.1277  time: 283s\n",
            "Epoch 5 - Score: 0.5081  Scores: [0.5456146725600978, 0.5135118831744238, 0.45250204006176176, 0.524141167812546, 0.5256965106765492, 0.4871989665140227]\n",
            "INFO:__main__:Epoch 5 - Score: 0.5081  Scores: [0.5456146725600978, 0.5135118831744238, 0.45250204006176176, 0.524141167812546, 0.5256965106765492, 0.4871989665140227]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL: [124/125] Elapsed 0m 15s (remain 0m 0s) Loss: 0.0854(0.1277) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 result ==========\n",
            "INFO:__main__:========== fold: 0 result ==========\n",
            "Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n",
            "INFO:__main__:Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n",
            "========== CV ==========\n",
            "INFO:__main__:========== CV ==========\n",
            "Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n",
            "INFO:__main__:Score: 0.4744  Scores: [0.4955053859707873, 0.48258737323618073, 0.43359696928816804, 0.4744697259056485, 0.4935083034115342, 0.4669559239043084]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>█▁▂▂▁▁▂▂▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>[fold0] epoch</td><td>▁▃▅▆█▁▃▅▆█▁▃▅▆█▁▃▅▆█▁▃▅▆█▁▃▅▆█</td></tr><tr><td>[fold0] loss</td><td>▅█▂▂▃▄▃▁▂▃▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>█▁▂▂▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.00029</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.12772</td></tr><tr><td>[fold0] epoch</td><td>5</td></tr><tr><td>[fold0] loss</td><td>0.0</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.50811</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">/content/gdrive/MyDrive/Colab_Notebooks/Deberta/microsoft_deberta-large</strong>: <a href=\"https://wandb.ai/anony-moose-448938/FB3-Public/runs/2naro4d0?apiKey=2691494e2d81554403eb49d43063353209b5c90c\" target=\"_blank\">https://wandb.ai/anony-moose-448938/FB3-Public/runs/2naro4d0?apiKey=2691494e2d81554403eb49d43063353209b5c90c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221125_102556-2naro4d0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#testing\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    def get_result(oof_df):\n",
        "        labels = oof_df[CFG.target_cols].values\n",
        "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
        "        score, scores = get_score(labels, preds)\n",
        "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
        "    \n",
        "    if CFG.train:\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(CFG.n_fold):\n",
        "            if fold in CFG.trn_fold:\n",
        "                _oof_df = train_loop(train, fold)\n",
        "                oof_df = pd.concat([oof_df, _oof_df])\n",
        "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "                get_result(_oof_df)\n",
        "                break\n",
        "        oof_df = oof_df.reset_index(drop=True)\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
        "        \n",
        "    if CFG.wandb:\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "07daa080",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-31T22:32:41.984286Z",
          "iopub.status.busy": "2022-08-31T22:32:41.983961Z",
          "iopub.status.idle": "2022-09-01T01:51:42.378681Z",
          "shell.execute_reply": "2022-09-01T01:51:42.377687Z"
        },
        "id": "07daa080",
        "papermill": {
          "duration": 11940.406964,
          "end_time": "2022-09-01T01:51:42.380833",
          "exception": false,
          "start_time": "2022-08-31T22:32:41.973869",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "    \n",
        "#     def get_result(oof_df):\n",
        "#         labels = oof_df[CFG.target_cols].values\n",
        "#         preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
        "#         score, scores = get_score(labels, preds)\n",
        "#         LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
        "    \n",
        "#     if CFG.train:\n",
        "#         oof_df = pd.DataFrame()\n",
        "#         for fold in range(CFG.n_fold):\n",
        "#             if fold in CFG.trn_fold:\n",
        "#                 _oof_df = train_loop(train, fold)\n",
        "#                 oof_df = pd.concat([oof_df, _oof_df])\n",
        "#                 LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "#                 get_result(_oof_df)\n",
        "#         oof_df = oof_df.reset_index(drop=True)\n",
        "#         LOGGER.info(f\"========== CV ==========\")\n",
        "#         get_result(oof_df)\n",
        "#         oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
        "        \n",
        "#     if CFG.wandb:\n",
        "#         wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('anaconda3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 12032.949787,
      "end_time": "2022-09-01T01:51:45.270554",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-08-31T22:31:12.320767",
      "version": "2.3.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16f57f81db6c45228e19dd35c4b7940f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a4d3d7abd14464abdd90484b2efa3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_309ca4ae760a4938b5da41b61634dae8",
              "IPY_MODEL_61791d4971284907a704a8e5df365dd1",
              "IPY_MODEL_27a82cd579d647e9932df00e88180976"
            ],
            "layout": "IPY_MODEL_a3991ef6c11045edbc87df951a2091eb"
          }
        },
        "27a82cd579d647e9932df00e88180976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d8c52f341f4ee49edf3193038d1a32",
            "placeholder": "​",
            "style": "IPY_MODEL_a4a6f40b653842dc9f1c8a75ad02b236",
            "value": " 1000/1000 [00:01&lt;00:00, 978.83it/s]"
          }
        },
        "2eea28252b524bf79272f13af0782bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309ca4ae760a4938b5da41b61634dae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69cb2d77b57d4d38960d61c06b49db75",
            "placeholder": "​",
            "style": "IPY_MODEL_ac523221f7af49af978945c12395dae1",
            "value": "100%"
          }
        },
        "61791d4971284907a704a8e5df365dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eea28252b524bf79272f13af0782bfd",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16f57f81db6c45228e19dd35c4b7940f",
            "value": 1000
          }
        },
        "65d8c52f341f4ee49edf3193038d1a32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cb2d77b57d4d38960d61c06b49db75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3991ef6c11045edbc87df951a2091eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4a6f40b653842dc9f1c8a75ad02b236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac523221f7af49af978945c12395dae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
